def _get_final_flattened_size(self):
with torch.no_grad():
x = torch.zeros((1, 1, self.input_channels,
self.patch_size, self.patch_size))
x = self.conv1(x)
x = self.conv2(x)
_, t, c, w, h = x.size()
return t * c * w * h
def forward(self, x):
x = F.relu(self.conv1(x))
x = F.relu(self.conv2(x))