from utils import open_file
import numpy as np
import cv2
CUSTOM_DATASETS_CONFIG = {
'DFC2018_HSI': {
'img': '2018_IEEE_GRSS_DFC_HSI_TR.HDR',
'gt': '2018_IEEE_GRSS_DFC_GT_TR.tif',
'download': False,
'loader': lambda folder: dfc2018_loader(folder)
}
}
def dfc2018_loader(folder):
img = open_file(folder + '2018_IEEE_GRSS_DFC_HSI_TR.HDR')[:,:,:-2]
gt = open_file(folder + '2018_IEEE_GRSS_DFC_GT_TR.tif')
gt = gt.astype('uint8')
# The original data img size(601, 2384, 50) gt size(1202, 4768)
# So you first need to downsample the img data or upsample the gt data
gt = cv2.resize(gt, dsize=(img.shape[0],img.shape[1]), interpolation=cv2.INTER_NEAREST)
# img  = cv2.resize(img, dsize=(gt.shape[0],gt.shape[1]), interpolation=cv2.INTER_CUBIC)
rgb_bands = (47, 31, 15)
label_values = ["Unclassified",
"Healthy grass",
"Stressed grass",
"Artificial turf",
"Evergreen trees",
"Deciduous trees",
"Bare earth",
"Water",
"Residential buildings",
"Non-residential buildings",
"Roads",
"Sidewalks",
"Crosswalks",
"Major thoroughfares",
"Highways",
"Railways",
"Paved parking lots",
"Unpaved parking lots",
"Cars",
"Trains",
"Stadium seats"]
ignored_labels = [0]
palette = None
return img, gt, rgb_bands, ignored_labels, label_values, palette
# -*- coding: utf-8 -*-
"""
This file contains the PyTorch dataset for hyperspectral images and
related helpers.
"""
import spectral
import numpy as np
import torch
import torch.utils
import torch.utils.data
import os
from tqdm import tqdm
from sklearn import preprocessing
try:
# Python 3
from urllib.request import urlretrieve
except ImportError:
# Python 2
from urllib import urlretrieve
from utils import open_file
DATASETS_CONFIG = {
'PaviaC': {
'urls': ['http://www.ehu.eus/ccwintco/uploads/e/e3/Pavia.mat',
'http://www.ehu.eus/ccwintco/uploads/5/53/Pavia_gt.mat'],
'img': 'Pavia.mat',
'gt': 'Pavia_gt.mat'
},
'PaviaU': {
'urls': ['http://www.ehu.eus/ccwintco/uploads/e/ee/PaviaU.mat',
'http://www.ehu.eus/ccwintco/uploads/5/50/PaviaU_gt.mat'],
'img': 'PaviaU.mat',
'gt': 'PaviaU_gt.mat'
},
'KSC': {
'urls': ['http://www.ehu.es/ccwintco/uploads/2/26/KSC.mat',
'http://www.ehu.es/ccwintco/uploads/a/a6/KSC_gt.mat'],
'img': 'KSC.mat',
'gt': 'KSC_gt.mat'
},
'IndianPines': {
'urls': ['http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat',
'http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat'],
'img': 'Indian_pines_corrected.mat',
'gt': 'Indian_pines_gt.mat'
},
'Botswana': {
'urls': ['http://www.ehu.es/ccwintco/uploads/7/72/Botswana.mat',
'http://www.ehu.es/ccwintco/uploads/5/58/Botswana_gt.mat'],
'img': 'Botswana.mat',
'gt': 'Botswana_gt.mat',
}
}
try:
from custom_datasets import CUSTOM_DATASETS_CONFIG
DATASETS_CONFIG.update(CUSTOM_DATASETS_CONFIG)
except ImportError:
pass
class TqdmUpTo(tqdm):
"""Provides `update_to(n)` which uses `tqdm.update(delta_n)`."""
def update_to(self, b=1, bsize=1, tsize=None):
"""
b  : int, optional
Number of blocks transferred so far [default: 1].
bsize  : int, optional
Size of each block (in tqdm units) [default: 1].
tsize  : int, optional
Total size (in tqdm units). If [default: None] remains unchanged.
"""
if tsize is not None:
self.total = tsize
self.update(b * bsize - self.n)  # will also set self.n = b * bsize
def get_dataset(dataset_name, target_folder="./", datasets=DATASETS_CONFIG):
""" Gets the dataset specified by name and return the related components.
Args:
dataset_name: string with the name of the dataset
target_folder (optional): folder to store the datasets, defaults to ./
datasets (optional): dataset configuration dictionary, defaults to prebuilt one
Returns:
img: 3D hyperspectral image (WxHxB)
gt: 2D int array of labels
label_values: list of class names
ignored_labels: list of int classes to ignore
rgb_bands: int tuple that correspond to red, green and blue bands
"""
palette = None
if dataset_name not in datasets.keys():
raise ValueError("{} dataset is unknown.".format(dataset_name))
dataset = datasets[dataset_name]
folder = target_folder + datasets[dataset_name].get('folder', dataset_name + '/')
if dataset.get('download', True):
# Download the dataset if is not present
if not os.path.isdir(folder):
os.mkdir(folder)
for url in datasets[dataset_name]['urls']:
# download the files
filename = url.split('/')[-1]
if not os.path.exists(folder + filename):
with TqdmUpTo(unit='B', unit_scale=True, miniters=1,
desc="Downloading {}".format(filename)) as t:
urlretrieve(url, filename=folder + filename,
reporthook=t.update_to)
elif not os.path.isdir(folder):
print("WARNING: {} is not downloadable.".format(dataset_name))
if dataset_name == 'PaviaC':
# Load the image
img = open_file(folder + 'Pavia.mat')['pavia']
rgb_bands = (55, 41, 12)
gt = open_file(folder + 'Pavia_gt.mat')['pavia_gt']
label_values = ["Undefined", "Water", "Trees", "Asphalt",
"Self-Blocking Bricks", "Bitumen", "Tiles", "Shadows",
"Meadows", "Bare Soil"]
ignored_labels = [0]
elif dataset_name == 'PaviaU':
# Load the image
img = open_file(folder + 'PaviaU.mat')['paviaU']
rgb_bands = (55, 41, 12)
gt = open_file(folder + 'PaviaU_gt.mat')['paviaU_gt']
label_values = ['Undefined', 'Asphalt', 'Meadows', 'Gravel', 'Trees',
'Painted metal sheets', 'Bare Soil', 'Bitumen',
'Self-Blocking Bricks', 'Shadows']
ignored_labels = [0]
elif dataset_name == 'IndianPines':
# Load the image
img = open_file(folder + 'Indian_pines_corrected.mat')
img = img['indian_pines_corrected']
rgb_bands = (43, 21, 11)  # AVIRIS sensor
gt = open_file(folder + 'Indian_pines_gt.mat')['indian_pines_gt']
label_values = ["Undefined", "Alfalfa", "Corn-notill", "Corn-mintill",
"Corn", "Grass-pasture", "Grass-trees",
"Grass-pasture-mowed", "Hay-windrowed", "Oats",
"Soybean-notill", "Soybean-mintill", "Soybean-clean",
"Wheat", "Woods", "Buildings-Grass-Trees-Drives",
"Stone-Steel-Towers"]
ignored_labels = [0]
elif dataset_name == 'Botswana':
# Load the image
img = open_file(folder + 'Botswana.mat')['Botswana']
rgb_bands = (75, 33, 15)
gt = open_file(folder + 'Botswana_gt.mat')['Botswana_gt']
label_values = ["Undefined", "Water", "Hippo grass",
"Floodplain grasses 1", "Floodplain grasses 2",
"Reeds", "Riparian", "Firescar", "Island interior",
"Acacia woodlands", "Acacia shrublands",
"Acacia grasslands", "Short mopane", "Mixed mopane",
"Exposed soils"]
ignored_labels = [0]
elif dataset_name == 'KSC':
# Load the image
img = open_file(folder + 'KSC.mat')['KSC']
rgb_bands = (43, 21, 11)  # AVIRIS sensor
gt = open_file(folder + 'KSC_gt.mat')['KSC_gt']
label_values = ["Undefined", "Scrub", "Willow swamp",
"Cabbage palm hammock", "Cabbage palm/oak hammock",
"Slash pine", "Oak/broadleaf hammock",
"Hardwood swamp", "Graminoid marsh", "Spartina marsh",
"Cattail marsh", "Salt marsh", "Mud flats", "Wate"]
ignored_labels = [0]
else:
# Custom dataset
img, gt, rgb_bands, ignored_labels, label_values, palette = CUSTOM_DATASETS_CONFIG[dataset_name]['loader'](folder)
# Filter NaN out
nan_mask = np.isnan(img.sum(axis=-1))
if np.count_nonzero(nan_mask) > 0:
print("Warning: NaN have been found in the data. It is preferable to remove them beforehand. Learning on NaN data is disabled.")
img[nan_mask] = 0
gt[nan_mask] = 0
ignored_labels.append(0)
ignored_labels = list(set(ignored_labels))
# Normalization
img = np.asarray(img, dtype='float32')
#img = (img - np.min(img)) / (np.max(img) - np.min(img))
data = img.reshape(np.prod(img.shape[:2]), np.prod(img.shape[2:]))
#data = preprocessing.scale(data)
data  = preprocessing.minmax_scale(data)
img = data.reshape(img.shape)
return img, gt, label_values, ignored_labels, rgb_bands, palette
# class HyperX(torch.utils.data.Dataset):
#     """ Generic class for a hyperspectral scene """
#     def __init__(self, data, gt, **hyperparams):
#         """
#         Args:
#             data: 3D hyperspectral image
#             gt: 2D array of labels
#             patch_size: int, size of the spatial neighbourhood
#             center_pixel: bool, set to True to consider only the label of the
#                           center pixel
#             data_augmentation: bool, set to True to perform random flips
#             supervision: 'full' or 'semi' supervised algorithms
#         """
#         super(HyperX, self).__init__()
#         self.data = data
#         self.label = gt
#         self.name = hyperparams['dataset']
#         self.patch_size = hyperparams['patch_size']
#         self.ignored_labels = set(hyperparams['ignored_labels'])
#         self.flip_augmentation = hyperparams['flip_augmentation']
#         self.radiation_augmentation = hyperparams['radiation_augmentation']
#         self.mixture_augmentation = hyperparams['mixture_augmentation']
#         self.center_pixel = hyperparams['center_pixel']
#         mask = kwargs.setdefault('mask', np.zeros((data.shape[0], data.shape[1])))
#         supervision = hyperparams['supervision']
#         # Fully supervised : use all pixels with label not ignored
#         if supervision == 'full':
#             mask = np.ones_like(gt)
#             for l in self.ignored_labels:
#                 mask[gt == l] = 0
#         # Semi-supervised : use all pixels, except padding
#         elif supervision == 'semi':
#             mask = np.ones_like(gt)
#         x_pos, y_pos = np.nonzero(mask)
#         p = self.patch_size // 2
#         self.indices = np.array([(x,y) for x,y in zip(x_pos, y_pos) if x > p-1 and x < data.shape[0] - p and y > p-1 and y < data.shape[1] - p])
#         self.labels = [self.label[x,y] for x,y in self.indices]
#         np.random.shuffle(self.indices)
class HyperX(torch.utils.data.Dataset):
""" Generic class for a hyperspectral scene """
def __init__(self, data, gt, **hyperparams):
"""
Args:
data: 3D hyperspectral image
gt: 2D array of labels
patch_size: int, size of the spatial neighbourhood
center_pixel: bool, set to True to consider only the label of the
center pixel
data_augmentation: bool, set to True to perform random flips
supervision: 'full' or 'semi' supervised algorithms
"""
super(HyperX, self).__init__()
self.data = data
self.label = gt
self.name = hyperparams['dataset']
self.patch_size = hyperparams['patch_size']
self.ignored_labels = set(hyperparams['ignored_labels'])
self.flip_augmentation = hyperparams['flip_augmentation']
self.radiation_augmentation = hyperparams['radiation_augmentation']
self.mixture_augmentation = hyperparams['mixture_augmentation']
self.center_pixel = hyperparams['center_pixel']
mask = hyperparams.setdefault('mask', np.zeros((data.shape[0], data.shape[1])))
supervision = hyperparams['supervision']
# Fully supervised : use all pixels with label not ignored
if supervision == 'full':
mask = np.ones_like(gt)
for l in self.ignored_labels:
mask[gt == l] = 0
# Semi-supervised : use all pixels, except padding
elif supervision == 'semi':
mask = np.ones_like(gt)
x_pos, y_pos = np.nonzero(mask)
p = self.patch_size // 2
self.indices = np.array([(x,y) for x,y in zip(x_pos, y_pos) if x > p-1 and x < data.shape[0] - p and y > p-1 and y < data.shape[1] - p])
self.labels = [self.label[x,y] for x,y in self.indices]
np.random.shuffle(self.indices)
@staticmethod
def flip(*arrays):
horizontal = np.random.random() > 0.5
vertical = np.random.random() > 0.5
if horizontal:
arrays = [np.fliplr(arr) for arr in arrays]
if vertical:
arrays = [np.flipud(arr) for arr in arrays]
return arrays
@staticmethod
def radiation_noise(data, alpha_range=(0.9, 1.1), beta=1/25):
alpha = np.random.uniform(*alpha_range)
noise = np.random.normal(loc=0., scale=1.0, size=data.shape)
return alpha * data + beta * noise
def mixture_noise(self, data, label, beta=1/25):
alpha1, alpha2 = np.random.uniform(0.01, 1., size=2)
noise = np.random.normal(loc=0., scale=1.0, size=data.shape)
data2 = np.zeros_like(data)
for  idx, value in np.ndenumerate(label):
if value not in self.ignored_labels:
l_indices = np.nonzero(self.labels == value)[0]
l_indice = np.random.choice(l_indices)
assert(self.labels[l_indice] == value)
x, y = self.indices[l_indice]
data2[idx] = self.data[x,y]
return (alpha1 * data + alpha2 * data2) / (alpha1 + alpha2) + beta * noise
def __len__(self):
return len(self.indices)
def __getitem__(self, i):
x, y = self.indices[i]
x1, y1 = x - self.patch_size // 2, y - self.patch_size // 2
x2, y2 = x1 + self.patch_size, y1 + self.patch_size
data = self.data[x1:x2, y1:y2]
label = self.label[x1:x2, y1:y2]
if self.flip_augmentation and self.patch_size > 1:
# Perform data augmentation (only on 2D patches)
data, label = self.flip(data, label)
if self.radiation_augmentation and np.random.random() < 0.1:
data = self.radiation_noise(data)
if self.mixture_augmentation and np.random.random() < 0.2:
data = self.mixture_noise(data, label)
# Copy the data into numpy arrays (PyTorch doesn't like numpy views)
data = np.asarray(np.copy(data).transpose((2, 0, 1)), dtype='float32')
label = np.asarray(np.copy(label), dtype='int64')
# Load the data into PyTorch tensors
data = torch.from_numpy(data)
label = torch.from_numpy(label)
# Extract the center label if needed
if self.center_pixel and self.patch_size > 1:
label = label[self.patch_size // 2, self.patch_size // 2]
# Remove unused dimensions when we work with invidual spectrums
elif self.patch_size == 1:
data = data[:, 0, 0]
label = label[0, 0]
# Add a fourth dimension for 3D CNN
if self.patch_size > 1:
# Make 4D data ((Batch x) Planes x Channels x Width x Height)
data = data.unsqueeze(0)
return data, label
# Python 2/3 compatiblity
from __future__ import print_function
from __future__ import division
import os
from utils import convert_to_color_, convert_from_color_, get_device
from datasets import open_file
from models import get_model, test
import numpy as np
import seaborn as sns
from skimage import io
import argparse
import torch
# Test options
parser = argparse.ArgumentParser(description="Run deep learning experiments on"
" various hyperspectral datasets")
parser.add_argument('--model', type=str, default=None,
help="Model to train. Available:\n"
"SVM (linear), "
"SVM_grid (grid search on linear, poly and RBF kernels), "
"baseline (fully connected NN), "
"hu (1D CNN), "
"hamida (3D CNN + 1D classifier), "
"lee (3D FCN), "
"chen (3D CNN), "
"li (3D CNN), "
"he (3D CNN), "
"luo (3D CNN), "
"sharma (2D CNN), "
"boulch (1D semi-supervised CNN), "
"liu (3D semi-supervised CNN), "
"mou (1D RNN)")
parser.add_argument('--cuda', type=int, default=-1,
help="Specify CUDA device (defaults to -1, which learns on CPU)")
parser.add_argument('--checkpoint', type=str, default=None,
help="Weights to use for initialization, e.g. a checkpoint")
group_test = parser.add_argument_group('Test')
group_test.add_argument('--test_stride', type=int, default=1,
help="Sliding window step stride during inference (default = 1)")
group_test.add_argument('--image', type=str, default=None, nargs='?',
help="Path to an image on which to run inference.")
group_test.add_argument('--only_test', type=str, default=None, nargs='?',
help="Choose the data on which to test the trained algorithm ")
group_test.add_argument('--mat', type=str, default=None, nargs='?',
help="In case of a .mat file, define the variable to call inside the file")
group_test.add_argument('--n_classes', type=int, default=None, nargs='?',
help="When using a trained algorithm, specified  the number of classes of this algorithm")
# Training options
group_train = parser.add_argument_group('Model')
group_train.add_argument('--patch_size', type=int,
help="Size of the spatial neighbourhood (optional, if "
"absent will be set by the model)")
group_train.add_argument('--batch_size', type=int,
help="Batch size (optional, if absent will be set by the model")
args = parser.parse_args()
CUDA_DEVICE = get_device(args.cuda)
MODEL = args.model
# Testing file
MAT = args.mat
N_CLASSES = args.n_classes
INFERENCE = args.image
TEST_STRIDE = args.test_stride
CHECKPOINT = args.checkpoint
img_filename = os.path.basename(INFERENCE)
basename = MODEL + img_filename
dirname = os.path.dirname(INFERENCE)
img = open_file(INFERENCE)
if MAT is not None:
img = img[MAT]
# Normalization
img = np.asarray(img, dtype='float32')
img = (img - np.min(img)) / (np.max(img) - np.min(img))
N_BANDS = img.shape[-1]
hyperparams = vars(args)
hyperparams.update({'n_classes': N_CLASSES, 'n_bands': N_BANDS, 'device': CUDA_DEVICE, 'ignored_labels': [0]})
hyperparams = dict((k, v) for k, v in hyperparams.items() if v is not None)
palette = {0: (0, 0, 0)}
for k, color in enumerate(sns.color_palette("hls", N_CLASSES)):
palette[k + 1] = tuple(np.asarray(255 * np.array(color), dtype='uint8'))
invert_palette = {v: k for k, v in palette.items()}
def convert_to_color(x):
return convert_to_color_(x, palette=palette)
def convert_from_color(x):
return convert_from_color_(x, palette=invert_palette)
if MODEL in ['SVM', 'SVM_grid', 'SGD', 'nearest']:
from sklearn.externals import joblib
model = joblib.load(CHECKPOINT)
w, h = img.shape[:2]
X = img.reshape((w*h, N_BANDS))
prediction = model.predict(X)
prediction = prediction.reshape(img.shape[:2])
else:
model, _, _, hyperparams = get_model(MODEL, **hyperparams)
model.load_state_dict(torch.load(CHECKPOINT))
probabilities = test(model, img, hyperparams)
prediction = np.argmax(probabilities, axis=-1)
filename = dirname + '/' + basename + '.tif'
io.imsave(filename, prediction)
basename = 'color_' + basename
filename = dirname + '/' + basename + '.tif'
io.imsave(filename, convert_to_color(prediction))
import torch
import torch.utils.data as data
from torchsummary import summary
import numpy as np
import sklearn.svm
import sklearn.model_selection
from skimage import io
import seaborn as sns
import visdom
import os
from utils import metrics, convert_to_color_, convert_from_color_, \
display_dataset, display_predictions, explore_spectrums, plot_spectrums, \
sample_gt, build_dataset, show_results, compute_imf_weights, get_device
from datasets import get_dataset, HyperX, open_file, DATASETS_CONFIG
from models import get_model, train, test, save_model
import argparse
dataset_names = [v['name'] if 'name' in v.keys() else k for k, v in DATASETS_CONFIG.items()]
parser = argparse.ArgumentParser(description="Run deep learning experiments on various hyperspectral datasets")
parser.add_argument('--dataset', type=str, default=None, choices=dataset_names, help="Dataset to use.")
parser.add_argument('--model', type=str, default=None, help="Model to train.")
parser.add_argument('--folder', type=str, default="./Datasets/", help="Folder where to store the datasets.")
parser.add_argument('--cuda', type=int, default=-1, help="Specify CUDA device (defaults to -1, which learns on CPU)")
parser.add_argument('--runs', type=int, default=1, help="Number of runs (default: 1)")
parser.add_argument('--restore', type=str, default=None, help="Weights to use for initialization, e.g. a checkpoint")
group_dataset = parser.add_argument_group('Dataset')
group_dataset.add_argument('--training_sample', type=float, default=0.1, help="Percentage of samples to use for training (default: 10%)")
group_dataset.add_argument('--sampling_mode', type=str, default='random', help="Sampling mode (random sampling or disjoint, default: random)")
group_dataset.add_argument('--train_set', type=str, default=None, help="Path to the train ground truth (optional)")
group_dataset.add_argument('--test_set', type=str, default=None, help="Path to the test set (optional)")
group_train = parser.add_argument_group('Training')
group_train.add_argument('--epoch', type=int, help="Training epochs (optional, if absent will be set by the model)")
group_train.add_argument('--patch_size', type=int, help="Size of the spatial neighbourhood (optional, if absent will be set by the model)")
group_train.add_argument('--lr', type=float, help="Learning rate, set by the model if not specified.")
group_train.add_argument('--class_balancing', action='store_true', help="Inverse median frequency class balancing (default = False)")
group_train.add_argument('--batch_size', type=int, help="Batch size (optional, if absent will be set by the model")
group_train.add_argument('--test_stride', type=int, default=1, help="Sliding window step stride during inference (default = 1)")
group_da = parser.add_argument_group('Data augmentation')
group_da.add_argument('--flip_augmentation', action='store_true', help="Random flips (if patch_size > 1)")
group_da.add_argument('--radiation_augmentation', action='store_true', help="Random radiation noise (illumination)")
group_da.add_argument('--mixture_augmentation', action='store_true', help="Random mixes between spectra")
parser.add_argument('--with_exploration', action='store_true', help="See data exploration visualization")
parser.add_argument('--download', type=str, default=None, nargs='+', choices=dataset_names, help="Download the specified datasets and quits.")
parser.add_argument('--custom_dataset', type=str, default=None, help="Path to a custom dataset provided by the user.")
args = parser.parse_args()
CUDA_DEVICE = get_device(args.cuda)
SAMPLE_PERCENTAGE = args.training_sample
FLIP_AUGMENTATION = args.flip_augmentation
RADIATION_AUGMENTATION = args.radiation_augmentation
MIXTURE_AUGMENTATION = args.mixture_augmentation
DATASET = args.dataset
CUSTOM_DATASET = args.custom_dataset
MODEL = args.model
N_RUNS = args.runs
PATCH_SIZE = args.patch_size
DATAVIZ = args.with_exploration
FOLDER = args.folder
EPOCH = args.epoch
SAMPLING_MODE = args.sampling_mode
CHECKPOINT = args.restore
LEARNING_RATE = args.lr
CLASS_BALANCING = args.class_balancing
TRAIN_GT = args.train_set
TEST_GT = args.test_set
TEST_STRIDE = args.test_stride
if args.download is not None and len(args.download) > 0:
for dataset in args.download:
get_dataset(dataset, target_folder=FOLDER)
quit()
viz = visdom.Visdom(env=DATASET + ' ' + MODEL)
if not viz.check_connection:
print("Visdom is not connected. Did you run 'python -m visdom.server' ?")
hyperparams = vars(args)
if CUSTOM_DATASET:
img, gt, LABEL_VALUES, IGNORED_LABELS, RGB_BANDS, palette = open_file(CUSTOM_DATASET)
else:
img, gt, LABEL_VALUES, IGNORED_LABELS, RGB_BANDS, palette = get_dataset(DATASET, FOLDER)
N_CLASSES = len(LABEL_VALUES)
N_BANDS = img.shape[-1]
SVM_GRID_PARAMS = [{'kernel': ['rbf'], 'gamma': [1e-1, 1e-2, 1e-3], 'C': [1, 10, 100, 1000]},
{'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]},
{'kernel': ['poly'], 'degree': [3], 'gamma': [1e-1, 1e-2, 1e-3]}]
if palette is None:
palette = {0: (0, 0, 0)}
for k, color in enumerate(sns.color_palette("hls", len(LABEL_VALUES) - 1)):
palette[k + 1] = tuple(np.asarray(255 * np.array(color), dtype='uint8'))
invert_palette = {v: k for k, v in palette.items()}
def convert_to_color(x):
return convert_to_color_(x, palette=palette)
def convert_from_color(x):
return convert_from_color_(x, palette=invert_palette)
hyperparams.update({'n_classes': N_CLASSES, 'n_bands': N_BANDS, 'ignored_labels': IGNORED_LABELS, 'device': CUDA_DEVICE})
hyperparams = dict((k, v) for k, v in hyperparams.items() if v is not None)
display_dataset(img, gt, RGB_BANDS, LABEL_VALUES, palette, viz)
color_gt = convert_to_color(gt)
if DATAVIZ:
mean_spectrums = explore_spectrums(img, gt, LABEL_VALUES, viz, ignored_labels=IGNORED_LABELS)
plot_spectrums(mean_spectrums, viz, title='Mean spectrum/class')
results = []
for run in range(N_RUNS):
if TRAIN_GT is not None and TEST_GT is not None:
train_gt = open_file(TRAIN_GT)
test_gt = open_file(TEST_GT)
elif TRAIN_GT is not None:
train_gt = open_file(TRAIN_GT)
test_gt = np.copy(gt)
w, h = test_gt.shape
test_gt[(train_gt > 0)[:w, :h]] = 0
elif TEST_GT is not None:
test_gt = open_file(TEST_GT)
else:
train_gt, test_gt = sample_gt(gt, SAMPLE_PERCENTAGE, mode=SAMPLING_MODE)
print("{} samples selected (over {})".format(np.count_nonzero(train_gt), np.count_nonzero(gt)))
print("Running an experiment with the {} model".format(MODEL), "run {}/{}".format(run + 1, N_RUNS))
display_predictions(convert_to_color(train_gt), viz, caption="Train ground truth")
display_predictions(convert_to_color(test_gt), viz, caption="Test ground truth")
prediction = None  # Initialize prediction to avoid NameError
if MODEL == 'SVM_grid':
print("Running a grid search SVM")
X_train, y_train = build_dataset(img, train_gt, ignored_labels=IGNORED_LABELS)
class_weight = 'balanced' if CLASS_BALANCING else None
clf = sklearn.svm.SVC(class_weight=class_weight)
clf = sklearn.model_selection.GridSearchCV(clf, SVM_GRID_PARAMS, verbose=5, n_jobs=4)
clf.fit(X_train, y_train)
print("SVM best parameters : {}".format(clf.best_params_))
prediction = clf.predict(img.reshape(-1, N_BANDS))
save_model(clf, MODEL, DATASET)
prediction = prediction.reshape(img.shape[:2])
elif MODEL == 'SVM':
X_train, y_train = build_dataset(img, train_gt, ignored_labels=IGNORED_LABELS)
class_weight = 'balanced' if CLASS_BALANCING else None
clf = sklearn.svm.SVC(class_weight=class_weight)
clf.fit(X_train, y_train)
save_model(clf, MODEL, DATASET)
prediction = clf.predict(img.reshape(-1, N_BANDS))
prediction = prediction.reshape(img.shape[:2])
elif MODEL == 'SGD':
X_train, y_train = build_dataset(img, train_gt, ignored_labels=IGNORED_LABELS)
X_train, y_train = sklearn.utils.shuffle(X_train, y_train)
scaler = sklearn.preprocessing.StandardScaler()
X_train = scaler.fit_transform(X_train)
class_weight = 'balanced' if CLASS_BALANCING else None
clf = sklearn.linear_model.SGDClassifier(class_weight=class_weight, learning_rate='optimal', tol=1e-3, average=10)
clf.fit(X_train, y_train)
save_model(clf, MODEL, DATASET)
prediction = clf.predict(scaler.transform(img.reshape(-1, N_BANDS)))
prediction = prediction.reshape(img.shape[:2])
elif MODEL == 'nearest':
X_train, y_train = build_dataset(img, train_gt, ignored_labels=IGNORED_LABELS)
X_train, y_train = sklearn.utils.shuffle(X_train, y_train)
class_weight = 'balanced' if CLASS_BALANCING else None
clf = sklearn.neighbors.KNeighborsClassifier(weights='distance')
clf = sklearn.model_selection.GridSearchCV(clf, {'n_neighbors': [1, 3, 5, 10, 20]}, verbose=5, n_jobs=4)
clf.fit(X_train, y_train)
save_model(clf, MODEL, DATASET)
prediction = clf.predict(img.reshape(-1, N_BANDS))
prediction = prediction.reshape(img.shape[:2])
else:
if CLASS_BALANCING:
weights = compute_imf_weights(train_gt, N_CLASSES, IGNORED_LABELS)
hyperparams['weights'] = torch.from_numpy(weights)
model, optimizer, loss, hyperparams = get_model(MODEL, **hyperparams)
train_gt, val_gt = sample_gt(train_gt, 0.95, mode='random')
train_dataset = HyperX(img, train_gt, **hyperparams)
train_loader = data.DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)
val_dataset = HyperX(img, val_gt, **hyperparams)
val_loader = data.DataLoader(val_dataset, batch_size=hyperparams['batch_size'])
print(hyperparams)
print("Network :")
with torch.no_grad():
for input, _ in train_loader:
break
summary(model.to(hyperparams['device']), input.size()[1:])
continue_training = True
if CHECKPOINT is not None:
if os.path.isfile(CHECKPOINT):
print(f"Loading checkpoint '{CHECKPOINT}'")
model.load_state_dict(torch.load(CHECKPOINT))
print("Checkpoint loaded successfully.")
continue_training = False  # Skip training if checkpoint is loaded successfully
else:
print(f"No checkpoint found at '{CHECKPOINT}'")
try:
train(model, optimizer, loss, train_loader, hyperparams['epoch'],
scheduler=hyperparams['scheduler'], device=hyperparams['device'],
supervision=hyperparams['supervision'], val_loader=val_loader,
display=viz, continue_training=continue_training)
except KeyboardInterrupt:
pass
if not continue_training:
probabilities = test(model, img, hyperparams)
prediction = np.argmax(probabilities, axis=-1)
if prediction is None:
probabilities = test(model, img, hyperparams)
prediction = np.argmax(probabilities, axis=-1)
run_results = metrics(prediction, test_gt, ignored_labels=hyperparams['ignored_labels'], n_classes=N_CLASSES)
mask = np.zeros(gt.shape, dtype='bool')
for l in IGNORED_LABELS:
mask[gt == l] = True
prediction[mask] = 0
color_prediction = convert_to_color(prediction)
display_predictions(color_prediction, viz, gt=convert_to_color(test_gt), caption="Prediction vs. test ground truth")
results.append(run_results)
show_results(run_results, viz, label_values=LABEL_VALUES)
if N_RUNS > 1:
show_results(results, viz, label_values=LABEL_VALUES, agregated=True)
# -*- coding: utf-8 -*-
import random
import numpy as np
from sklearn.metrics import confusion_matrix
import sklearn.model_selection
import seaborn as sns
import itertools
import spectral
import visdom
import matplotlib.pyplot as plt
from scipy import io, misc
import imageio
import os
import re
import torch
def get_device(ordinal):
# Use GPU ?
if ordinal < 0:
print("Computation on CPU")
device = torch.device('cpu')
elif torch.cuda.is_available():
print("Computation on CUDA GPU device {}".format(ordinal))
device = torch.device('cuda:{}'.format(ordinal))
else:
print("/!\\ CUDA was requested but is not available! Computation will go on CPU. /!\\")
device = torch.device('cpu')
return device
def open_file(dataset):
_, ext = os.path.splitext(dataset)
ext = ext.lower()
if ext == '.mat':
# Load Matlab array
return io.loadmat(dataset)
elif ext == '.tif' or ext == '.tiff':
# Load TIFF file
return imageio.imread(dataset)
elif ext == '.hdr':
img = spectral.open_image(dataset)
return img.load()
else:
raise ValueError("Unknown file format: {}".format(ext))
def convert_to_color_(arr_2d, palette=None):
"""Convert an array of labels to RGB color-encoded image.
Args:
arr_2d: int 2D array of labels
palette: dict of colors used (label number -> RGB tuple)
Returns:
arr_3d: int 2D images of color-encoded labels in RGB format
"""
arr_3d = np.zeros((arr_2d.shape[0], arr_2d.shape[1], 3), dtype=np.uint8)
if palette is None:
raise Exception("Unknown color palette")
for c, i in palette.items():
m = arr_2d == c
arr_3d[m] = i
return arr_3d
def convert_from_color_(arr_3d, palette=None):
"""Convert an RGB-encoded image to grayscale labels.
Args:
arr_3d: int 2D image of color-coded labels on 3 channels
palette: dict of colors used (RGB tuple -> label number)
Returns:
arr_2d: int 2D array of labels
"""
if palette is None:
raise Exception("Unknown color palette")
arr_2d = np.zeros((arr_3d.shape[0], arr_3d.shape[1]), dtype=np.uint8)
for c, i in palette.items():
m = np.all(arr_3d == np.array(c).reshape(1, 1, 3), axis=2)
arr_2d[m] = i
return arr_2d
def display_predictions(pred, vis, gt=None, caption=""):
if gt is None:
vis.images([np.transpose(pred, (2, 0, 1))],
opts={'caption': caption})
else:
vis.images([np.transpose(pred, (2, 0, 1)),
np.transpose(gt, (2, 0, 1))],
nrow=2,
opts={'caption': caption})
def display_dataset(img, gt, bands, labels, palette, vis):
"""Display the specified dataset.
Args:
img: 3D hyperspectral image
gt: 2D array labels
bands: tuple of RGB bands to select
labels: list of label class names
palette: dict of colors
display (optional): type of display, if any
"""
print("Image has dimensions {}x{} and {} channels".format(*img.shape))
rgb = spectral.get_rgb(img, bands)
rgb /= np.max(rgb)
rgb = np.asarray(255 * rgb, dtype='uint8')
# Display the RGB composite image
caption = "RGB (bands {}, {}, {})".format(*bands)
# send to visdom server
vis.images([np.transpose(rgb, (2, 0, 1))],
opts={'caption': caption})
def explore_spectrums(img, complete_gt, class_names, vis,
ignored_labels=None):
"""Plot sampled spectrums with mean + std for each class.
Args:
img: 3D hyperspectral image
complete_gt: 2D array of labels
class_names: list of class names
ignored_labels (optional): list of labels to ignore
vis : Visdom display
Returns:
mean_spectrums: dict of mean spectrum by class
"""
mean_spectrums = {}
for c in np.unique(complete_gt):
if c in ignored_labels:
continue
mask = complete_gt == c
class_spectrums = img[mask].reshape(-1, img.shape[-1])
step = max(1, class_spectrums.shape[0] // 100)
fig = plt.figure()
plt.title(class_names[c])
# Sample and plot spectrums from the selected class
for spectrum in class_spectrums[::step, :]:
plt.plot(spectrum, alpha=0.25)
mean_spectrum = np.mean(class_spectrums, axis=0)
std_spectrum = np.std(class_spectrums, axis=0)
lower_spectrum = np.maximum(0, mean_spectrum - std_spectrum)
higher_spectrum = mean_spectrum + std_spectrum
# Plot the mean spectrum with thickness based on std
plt.fill_between(range(len(mean_spectrum)), lower_spectrum,
higher_spectrum, color="#3F5D7D")
plt.plot(mean_spectrum, alpha=1, color="#FFFFFF", lw=2)
vis.matplot(plt)
mean_spectrums[class_names[c]] = mean_spectrum
return mean_spectrums
def plot_spectrums(spectrums, vis, title=""):
"""Plot the specified dictionary of spectrums.
Args:
spectrums: dictionary (name -> spectrum) of spectrums to plot
vis: Visdom display
"""
win = None
for k, v in spectrums.items():
n_bands = len(v)
update = None if win is None else 'append'
win = vis.line(X=np.arange(n_bands), Y=v, name=k, win=win, update=update,
opts={'title': title})
def build_dataset(mat, gt, ignored_labels=None):
"""Create a list of training samples based on an image and a mask.
Args:
mat: 3D hyperspectral matrix to extract the spectrums from
gt: 2D ground truth
ignored_labels (optional): list of classes to ignore, e.g. 0 to remove
unlabeled pixels
return_indices (optional): bool set to True to return the indices of
the chosen samples
"""
samples = []
labels = []
# Check that image and ground truth have the same 2D dimensions
assert mat.shape[:2] == gt.shape[:2]
for label in np.unique(gt):
if label in ignored_labels:
continue
else:
indices = np.nonzero(gt == label)
samples += list(mat[indices])
labels += len(indices[0]) * [label]
return np.asarray(samples), np.asarray(labels)
def get_random_pos(img, window_shape):
""" Return the corners of a random window in the input image
Args:
img: 2D (or more) image, e.g. RGB or grayscale image
window_shape: (width, height) tuple of the window
Returns:
xmin, xmax, ymin, ymax: tuple of the corners of the window
"""
w, h = window_shape
W, H = img.shape[:2]
x1 = random.randint(0, W - w - 1)
x2 = x1 + w
y1 = random.randint(0, H - h - 1)
y2 = y1 + h
return x1, x2, y1, y2
def sliding_window(image, step=10, window_size=(20, 20), with_data=True):
"""Sliding window generator over an input image.
Args:
image: 2D+ image to slide the window on, e.g. RGB or hyperspectral
step: int stride of the sliding window
window_size: int tuple, width and height of the window
with_data (optional): bool set to True to return both the data and the
corner indices
Yields:
([data], x, y, w, h) where x and y are the top-left corner of the
window, (w,h) the window size
"""
# slide a window across the image
w, h = window_size
W, H = image.shape[:2]
offset_w = (W - w) % step
offset_h = (H - h) % step
for x in range(0, W - w + offset_w, step):
if x + w > W:
x = W - w
for y in range(0, H - h + offset_h, step):
if y + h > H:
y = H - h
if with_data:
yield image[x:x + w, y:y + h], x, y, w, h
else:
yield x, y, w, h
def count_sliding_window(top, step=10, window_size=(20, 20)):
""" Count the number of windows in an image.
Args:
image: 2D+ image to slide the window on, e.g. RGB or hyperspectral, ...
step: int stride of the sliding window
window_size: int tuple, width and height of the window
Returns:
int number of windows
"""
sw = sliding_window(top, step, window_size, with_data=False)
return sum(1 for _ in sw)
def grouper(n, iterable):
""" Browse an iterable by grouping n elements by n elements.
Args:
n: int, size of the groups
iterable: the iterable to Browse
Yields:
chunk of n elements from the iterable
"""
it = iter(iterable)
while True:
chunk = tuple(itertools.islice(it, n))
if not chunk:
return
yield chunk
def metrics(prediction, target, ignored_labels=[], n_classes=None):
"""Compute and print metrics (accuracy, confusion matrix and F1 scores).
Args:
prediction: list of predicted labels
target: list of target labels
ignored_labels (optional): list of labels to ignore, e.g. 0 for undef
n_classes (optional): number of classes, max(target) by default
Returns:
accuracy, F1 score by class, confusion matrix
"""
ignored_mask = np.zeros(target.shape[:2], dtype=np.bool_)
for l in ignored_labels:
ignored_mask[target == l] = True
ignored_mask = ~ignored_mask
#target = target[ignored_mask] -1
target = target[ignored_mask]
prediction = prediction[ignored_mask]
results = {}
n_classes = np.max(target) + 1 if n_classes is None else n_classes
cm = confusion_matrix(
target,
prediction,
labels=range(n_classes))
results["Confusion matrix"] = cm
# Compute global accuracy
total = np.sum(cm)
accuracy = sum([cm[x][x] for x in range(len(cm))])
accuracy *= 100 / float(total)
results["Accuracy"] = accuracy
# Compute F1 score
F1scores = np.zeros(len(cm))
for i in range(len(cm)):
try:
F1 = 2. * cm[i, i] / (np.sum(cm[i, :]) + np.sum(cm[:, i]))
except ZeroDivisionError:
F1 = 0.
F1scores[i] = F1
results["F1 scores"] = F1scores
# Compute kappa coefficient
pa = np.trace(cm) / float(total)
pe = np.sum(np.sum(cm, axis=0) * np.sum(cm, axis=1)) / \
float(total * total)
kappa = (pa - pe) / (1 - pe)
results["Kappa"] = kappa
return results
def show_results(results, vis, label_values=None, agregated=False):
text = ""
if agregated:
accuracies = [r["Accuracy"] for r in results]
kappas = [r["Kappa"] for r in results]
F1_scores = [r["F1 scores"] for r in results]
F1_scores_mean = np.mean(F1_scores, axis=0)
F1_scores_std = np.std(F1_scores, axis=0)
cm = np.mean([r["Confusion matrix"] for r in results], axis=0)
text += "Agregated results :\n"
else:
cm = results["Confusion matrix"]
accuracy = results["Accuracy"]
F1scores = results["F1 scores"]
kappa = results["Kappa"]
#label_values = label_values[1:]
vis.heatmap(cm, opts={'title': "Confusion matrix",
'marginbottom': 150,
'marginleft': 150,
'width': 500,
'height': 500,
'rownames': label_values, 'columnnames': label_values})
text += "Confusion matrix :\n"
text += str(cm)
text += "---\n"
if agregated:
text += ("Accuracy: {:.03f} +- {:.03f}\n".format(np.mean(accuracies),
np.std(accuracies)))
else:
text += "Accuracy : {:.03f}%\n".format(accuracy)
text += "---\n"
text += "F1 scores :\n"
if agregated:
for label, score, std in zip(label_values, F1_scores_mean,
F1_scores_std):
text += "\t{}: {:.03f} +- {:.03f}\n".format(label, score, std)
else:
for label, score in zip(label_values, F1scores):
text += "\t{}: {:.03f}\n".format(label, score)
text += "---\n"
if agregated:
text += ("Kappa: {:.03f} +- {:.03f}\n".format(np.mean(kappas),
np.std(kappas)))
else:
text += "Kappa: {:.03f}\n".format(kappa)
vis.text(text.replace('\n', '<br/>'))
print(text)
def sample_gt(gt, train_size, mode='random'):
"""Extract a fixed percentage of samples from an array of labels.
Args:
gt: a 2D array of int labels
percentage: [0, 1] float
Returns:
train_gt, test_gt: 2D arrays of int labels
"""
indices = np.nonzero(gt)
X = list(zip(*indices)) # x,y features
y = gt[indices].ravel() # classes
train_gt = np.zeros_like(gt)
test_gt = np.zeros_like(gt)
if train_size > 1:
train_size = int(train_size)
if mode == 'random':
train_indices, test_indices = sklearn.model_selection.train_test_split(X, train_size=train_size, stratify=y)
train_indices = [list(t) for t in zip(*train_indices)]
test_indices = [list(t) for t in zip(*test_indices)]
train_gt[tuple(train_indices)] = gt[tuple(train_indices)]
test_gt[tuple(test_indices)] = gt[tuple(test_indices)]
elif mode == 'fixed':
print("Sampling {} with train size = {}".format(mode, train_size))
train_indices, test_indices = [], []
for c in np.unique(gt):
if c == 0:
continue
indices = np.nonzero(gt == c)
X = list(zip(*indices)) # x,y features
train, test = sklearn.model_selection.train_test_split(X, train_size=train_size)
train_indices += train
test_indices += test
train_indices = [list(t) for t in zip(*train_indices)]
test_indices = [list(t) for t in zip(*test_indices)]
train_gt[train_indices] = gt[train_indices]
test_gt[test_indices] = gt[test_indices]
elif mode == 'disjoint':
train_gt = np.copy(gt)
test_gt = np.copy(gt)
for c in np.unique(gt):
mask = gt == c
for x in range(gt.shape[0]):
first_half_count = np.count_nonzero(mask[:x, :])
second_half_count = np.count_nonzero(mask[x:, :])
try:
ratio = first_half_count / second_half_count
if ratio > 0.9 * train_size and ratio < 1.1 * train_size:
break
except ZeroDivisionError:
continue
mask[:x, :] = 0
train_gt[mask] = 0
test_gt[train_gt > 0] = 0
else:
raise ValueError("{} sampling is not implemented yet.".format(mode))
return train_gt, test_gt
def compute_imf_weights(ground_truth, n_classes=None, ignored_classes=[]):
""" Compute inverse median frequency weights for class balancing.
For each class i, it computes its frequency f_i, i.e the ratio between
the number of pixels from class i and the total number of pixels.
Then, it computes the median m of all frequencies. For each class the
associated weight is m/f_i.
Args:
ground_truth: the annotations array
n_classes: number of classes (optional, defaults to max(ground_truth))
ignored_classes: id of classes to ignore (optional)
Returns:
numpy array with the IMF coefficients
"""
n_classes = np.max(ground_truth) if n_classes is None else n_classes
weights = np.zeros(n_classes)
frequencies = np.zeros(n_classes)
for c in range(0, n_classes):
if c in ignored_classes:
continue
frequencies[c] = np.count_nonzero(ground_truth == c)
# Normalize the pixel counts to obtain frequencies
frequencies /= np.sum(frequencies)
# Obtain the median on non-zero frequencies
idx = np.nonzero(frequencies)
median = np.median(frequencies[idx])
weights[idx] = median / frequencies[idx]
weights[frequencies == 0] = 0.
return weights
def camel_to_snake(name):
s = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s).lower()
# -*- coding: utf-8 -*-
# Torch
import torch.nn as nn
import torch.nn.functional as F
import torch
import torch.optim as optim
from torch.nn import init
from torchsummary import summary
# utils
import math
import os
import datetime
import numpy as np
#from sklearn.externals
import joblib
from tqdm import tqdm
from utils import grouper, sliding_window, count_sliding_window,\
camel_to_snake
def get_model(name, **kwargs):
"""
Instantiate and obtain a model with adequate hyperparameters
Args:
name: string of the model name
kwargs: hyperparameters
Returns:
model: PyTorch network
optimizer: PyTorch optimizer
criterion: PyTorch loss Function
kwargs: hyperparameters with sane defaults
"""
device = kwargs.setdefault('device', torch.device('cpu'))
n_classes = kwargs['n_classes']
n_bands = kwargs['n_bands']
weights = torch.ones(n_classes)
#weights[torch.LongTensor(kwargs['ignored_labels'])] = 0.
weights[torch.LongTensor(kwargs['ignored_labels'])] = 0.
weights = weights.to(device)
weights = kwargs.setdefault('weights', weights)
# if name == 'CNN':
#     # Set up hyperparameters
#     patch_size = kwargs.setdefault('patch_size', 5)
#     center_pixel = True
#     lr = kwargs.setdefault('learning_rate', 0.01)
#     batch_size = kwargs.setdefault('batch_size', 100)
#     n_classes = kwargs.setdefault('n_classes', 9)
#     n_bands = kwargs.setdefault('n_bands', 103)
#     # Initialize model, loss function, and optimizer
#     model = CNN(patch_size, center_pixel, n_classes, n_bands).to(device)
#     criterion = torch.nn.CrossEntropyLoss()
#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)
#     # Return model, optimizer, loss function, and kwargs
#     return model, optimizer, criterion, kwargs
if name == 'nn':
kwargs.setdefault('patch_size', 1)
center_pixel = True
model = Baseline(n_bands, n_classes,
kwargs.setdefault('dropout', False))
lr = kwargs.setdefault('learning_rate', 0.0001)
optimizer = optim.Adam(model.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss(weight=kwargs['weights'])
kwargs.setdefault('epoch', 100)
kwargs.setdefault('batch_size', 100)
elif name == 'hamida':
patch_size = kwargs.setdefault('patch_size', 5)
center_pixel = True
model = HamidaEtAl(n_bands, n_classes, patch_size=patch_size)
lr = kwargs.setdefault('learning_rate', 0.01)
optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=0.0005)
kwargs.setdefault('batch_size', 100)
criterion = nn.CrossEntropyLoss(weight=kwargs['weights'])
elif name == 'lee':
kwargs.setdefault('epoch', 200)
patch_size = kwargs.setdefault('patch_size', 5)
center_pixel = False
model = LeeEtAl(n_bands, n_classes)
lr = kwargs.setdefault('learning_rate', 0.001)
optimizer = optim.Adam(model.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss(weight=kwargs['weights'])
elif name == 'chen':
patch_size = kwargs.setdefault('patch_size', 27)
center_pixel = True
model = ChenEtAl(n_bands, n_classes, patch_size=patch_size)
lr = kwargs.setdefault('learning_rate', 0.003)
optimizer = optim.SGD(model.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss(weight=kwargs['weights'])
kwargs.setdefault('epoch', 400)
kwargs.setdefault('batch_size', 100)
elif name == 'li':
patch_size = kwargs.setdefault('patch_size', 5)
center_pixel = True
model = LiEtAl(n_bands, n_classes, n_planes=16, patch_size=patch_size)
lr = kwargs.setdefault('learning_rate', 0.01)
optimizer = optim.SGD(model.parameters(),
lr=lr, momentum=0.9, weight_decay=0.0005)
epoch = kwargs.setdefault('epoch', 200)
criterion = nn.CrossEntropyLoss(weight=kwargs['weights'])
#kwargs.setdefault('scheduler', optim.lr_scheduler.MultiStepLR(optimizer, milestones=[epoch // 2, (5 * epoch) // 6], gamma=0.1))
elif name == 'hu':
kwargs.setdefault('patch_size', 1)
center_pixel = True
model = HuEtAl(n_bands, n_classes)
# From what I infer from the paper (Eq.7 and Algorithm 1), it is standard SGD with lr = 0.01
lr = kwargs.setdefault('learning_rate', 0.01)
optimizer = optim.SGD(model.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss(weight=kwargs['weights'])
kwargs.setdefault('epoch', 100)
kwargs.setdefault('batch_size', 100)
elif name == 'he':
# We train our model by AdaGrad [18] algorithm, in which
# the base learning rate is 0.01. In addition, we set the batch
# as 40, weight decay as 0.01 for all the layers
# The input of our network is the HSI 3D patch in the size of 7×7×Band
kwargs.setdefault('patch_size', 7)
kwargs.setdefault('batch_size', 40)
lr = kwargs.setdefault('learning_rate', 0.01)
center_pixel = True
model = HeEtAl(n_bands, n_classes, patch_size=kwargs['patch_size'])
# For Adagrad, we need to load the model on GPU before creating the optimizer
model = model.to(device)
optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay=0.01)
criterion = nn.CrossEntropyLoss(weight=kwargs['weights'])
elif name == 'luo':
# All  the  experiments  are  settled  by  the  learning  rate  of  0.1,
# the  decay  term  of  0.09  and  batch  size  of  100.
kwargs.setdefault('patch_size', 3)
kwargs.setdefault('batch_size', 100)
lr = kwargs.setdefault('learning_rate', 0.1)
center_pixel = True
model = LuoEtAl(n_bands, n_classes, patch_size=kwargs['patch_size'])
optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=0.09)
criterion = nn.CrossEntropyLoss(weight=kwargs['weights'])
elif name == 'sharma':
# We train our S-CNN from scratch using stochastic gradient descent with
# momentum set to 0.9, weight decay of 0.0005, and with a batch size
# of 60.  We initialize an equal learning rate for all trainable layers
# to 0.05, which is manually decreased by a factor of 10 when the validation
# error stopped decreasing. Prior to the termination the learning rate was
# reduced two times at 15th and 25th epoch. [...]
# We trained the network for 30 epochs
kwargs.setdefault('batch_size', 60)
epoch = kwargs.setdefault('epoch', 30)
lr = kwargs.setdefault('lr', 0.05)
center_pixel = True
# We assume patch_size = 64
kwargs.setdefault('patch_size', 64)
model = SharmaEtAl(n_bands, n_classes, patch_size=kwargs['patch_size'])
optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=0.0005)
criterion = nn.CrossEntropyLoss(weight=kwargs['weights'])
kwargs.setdefault('scheduler', optim.lr_scheduler.MultiStepLR(optimizer, milestones=[epoch // 2, (5 * epoch) // 6], gamma=0.1))
elif name == 'liu':
kwargs['supervision'] = 'semi'
# "The learning rate is set to 0.001 empirically. The number of epochs is set to be 40."
kwargs.setdefault('epoch', 40)
lr = kwargs.setdefault('lr', 0.001)
center_pixel = True
patch_size = kwargs.setdefault('patch_size', 9)
model = LiuEtAl(n_bands, n_classes, patch_size)
optimizer = optim.SGD(model.parameters(), lr=lr)
# "The unsupervised cost is the squared error of the difference"
criterion = (nn.CrossEntropyLoss(weight=kwargs['weights']), lambda rec, data: F.mse_loss(rec, data[:,:,:,patch_size//2,patch_size//2].squeeze()))
elif name == 'boulch':
kwargs['supervision'] = 'semi'
kwargs.setdefault('patch_size', 1)
kwargs.setdefault('epoch', 100)
lr = kwargs.setdefault('lr', 0.001)
center_pixel = True
model = BoulchEtAl(n_bands, n_classes)
optimizer = optim.SGD(model.parameters(), lr=lr)
criterion = (nn.CrossEntropyLoss(weight=kwargs['weights']), lambda rec, data: F.mse_loss(rec, data.squeeze()))
elif name == 'HICNN':
patch_size = kwargs.setdefault('patch_size', 5)
center_pixel = True
in_channels = n_bands
model = HyperspectralCNN(in_channels, n_classes)
model.to(device)
lr = kwargs.setdefault('learning_rate', 0.01)
# optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=0.0005)
# optimizer = optim.Adadelta(model.parameters(), lr=lr)
# optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay=0.01)
optimizer = optim.Adam(model.parameters(), lr=lr)
kwargs.setdefault('batch_size', 100)
criterion = nn.CrossEntropyLoss(weight=kwargs['weights'])
elif name == 'mou':
kwargs.setdefault('patch_size', 1)
center_pixel = True
kwargs.setdefault('epoch', 100)
# "The RNN was trained with the Adadelta algorithm [...] We made use of a
# fairly  high  learning  rate  of  1.0  instead  of  the  relatively  low
# default of  0.002 to  train the  network"
lr = kwargs.setdefault('lr', 1.0)
model = MouEtAl(n_bands, n_classes)
# For Adadelta, we need to load the model on GPU before creating the optimizer
model = model.to(device)
optimizer = optim.Adadelta(model.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss(weight=kwargs['weights'])
else:
raise KeyError("{} model is unknown.".format(name))
model = model.to(device)
epoch = kwargs.setdefault('epoch', 100)
kwargs.setdefault('scheduler', optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=epoch//4, verbose=True))
#kwargs.setdefault('scheduler', None)
kwargs.setdefault('batch_size', 100)
kwargs.setdefault('supervision', 'full')
kwargs.setdefault('flip_augmentation', False)
kwargs.setdefault('radiation_augmentation', False)
kwargs.setdefault('mixture_augmentation', False)
kwargs['center_pixel'] = center_pixel
return model, optimizer, criterion, kwargs
class HyperspectralCNN(nn.Module):
def __init__(self, in_channels, n_classes):
super(HyperspectralCNN, self).__init__()
# First convolutional block
self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)
self.bn1 = nn.BatchNorm2d(64)
self.relu1 = nn.ReLU(inplace=True)
self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
# Second convolutional block
self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
self.bn2 = nn.BatchNorm2d(128)
self.relu2 = nn.ReLU(inplace=True)
self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
# Third convolutional block
self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
self.bn3 = nn.BatchNorm2d(256)
self.relu3 = nn.ReLU(inplace=True)
self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)
# Fully connected layers
self.fc1 = nn.Linear(256 * 1 * 1, 1024)
self.fc2 = nn.Linear(1024, n_classes)
# Softmax layer
self.softmax = nn.Softmax(dim=1)
def forward(self, x):
# Pass the input through each layer
x = x.view(-1, 103, 7, 7)
x = self.conv1(x)
x = self.bn1(x)
x = self.relu1(x)
x = self.maxpool1(x)
x = self.conv2(x)
x = self.bn2(x)
x = self.relu2(x)
x = self.maxpool2(x)
x = self.conv3(x)
x = self.bn3(x)
x = self.relu3(x)
x = self.maxpool3(x)
# Flatten the output tensor before passing it through the fully connected layers
x = x.view(-1, 256 * 1 * 1)
x = self.fc1(x)
x = F.relu(x)
x = self.fc2(x)
# Convert the output to probabilities
x = self.softmax(x)
return x
# class HyperspectralCNN(nn.Module):
#     def __init__(self, in_channels, n_classes):
#         super(HyperspectralCNN, self).__init__()
#         # First convolutional block
#         self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)
#         self.bn1 = nn.BatchNorm2d(64)
#         self.relu1 = nn.ReLU(inplace=True)
#         self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=1)
#         # Second convolutional block
#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
#         self.bn2 = nn.BatchNorm2d(128)
#         self.relu2 = nn.ReLU(inplace=True)
#         self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=1)
#         # Third convolutional block
#         self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
#         self.bn3 = nn.BatchNorm2d(256)
#         self.relu3 = nn.ReLU(inplace=True)
#         self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=1)
#         # Fourth convolutional block
#         self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
#         self.bn4 = nn.BatchNorm2d(512)
#         self.relu4 = nn.ReLU(inplace=True)
#         self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=1)
#         # Fifth convolutional block
#         self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)
#         self.bn5 = nn.BatchNorm2d(1024)
#         self.relu5 = nn.ReLU(inplace=True)
#         self.maxpool5 = nn.MaxPool2d(kernel_size=2, stride=1)
#         # Fully connected layers
#         self.fc1 = nn.Linear(1024 * 2 * 2, 1024)
#         self.fc2 = nn.Linear(1024, n_classes)
#     def forward(self, x):
#         # Pass the input through each layer
#         x = x.view(-1, 103, 7, 7)
#         x = self.conv1(x)
#         x = self.bn1(x)
#         x = self.relu1(x)
#         x = self.maxpool1(x)
#         x = self.conv2(x)
#         x = self.bn2(x)
#         x = self.relu2(x)
#         x = self.maxpool2(x)
#         x = self.conv3(x)
#         x = self.bn3(x)
#         x = self.relu3(x)
#         x = self.maxpool3(x)
#         x = self.conv4(x)
#         x = self.bn4(x)
#         x = self.relu4(x)
#         x = self.maxpool4(x)
#         x = self.conv5(x)
#         x = self.bn5(x)
#         x = self.relu5(x)
#         x = self.maxpool5(x)
#         # Flatten the output tensor before passing it through the fully connected layers
#         x = x.reshape(x.size(0), 1024 * 2 * 2)
#         x = self.fc1(x)
#         x = F.relu(x)
#         x = self.fc2(x)
#         return x
class HyperspectralCNN(nn.Module):
def __init__(self, in_channels, n_classes):
super(HyperspectralCNN, self).__init__()
# First convolutional block
self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)
self.bn1 = nn.BatchNorm2d(64)
self.relu1 = nn.ReLU(inplace=True)
self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=1)
# Second convolutional block
self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
self.bn2 = nn.BatchNorm2d(128)
self.relu2 = nn.ReLU(inplace=True)
self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=1)
# Third convolutional block
self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
self.bn3 = nn.BatchNorm2d(256)
self.relu3 = nn.ReLU(inplace=True)
self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=1)
# Fourth convolutional block
self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
self.bn4 = nn.BatchNorm2d(512)
self.relu4 = nn.ReLU(inplace=True)
self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=1)
# Fifth convolutional block
self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)
self.bn5 = nn.BatchNorm2d(1024)
self.relu5 = nn.ReLU(inplace=True)
self.maxpool5 = nn.MaxPool2d(kernel_size=2, stride=1)
# Sixth convolutional block
self.conv6 = nn.Conv2d(1024, 2048, kernel_size=3, padding=1)
self.bn6 = nn.BatchNorm2d(2048)
self.relu6 = nn.ReLU(inplace=True)
self.maxpool6 = nn.MaxPool2d(kernel_size=2, stride=1)
# Seventh convolutional block
self.conv7 = nn.Conv2d(2048, 4096, kernel_size=3, padding=1)
self.bn7 = nn.BatchNorm2d(4096)
self.relu7 = nn.ReLU(inplace=True)
self.maxpool7 = nn.MaxPool2d(kernel_size=1, stride=1)
# Fully connected layers
self.fc1 = nn.Linear(4096 * 1 * 1, 1024)
self.fc2 = nn.Linear(1024, n_classes)
def forward(self, x):
# Pass the input through each layer
x = x.view(-1, 103, 7, 7)
x = self.conv1(x)
x = self.bn1(x)
x = self.relu1(x)
x = self.maxpool1(x)
x = self.conv2(x)
x = self.bn2(x)
x = self.relu2(x)
x = self.maxpool2(x)
x = self.conv3(x)
x = self.bn3(x)
x = self.relu3(x)
x = self.maxpool3(x)
x = self.conv4(x)
x = self.bn4(x)
x = self.relu4(x)
x = self.maxpool4(x)
x = self.conv5(x)
x = self.bn5(x)
x = self.relu5(x)
x = self.maxpool5(x)
x = self.conv6(x)
x = self.bn6(x)
x = self.relu6(x)
x = self.maxpool6(x)
x = self.conv7(x)
x = self.bn7(x)
x = self.relu7(x)
x = self.maxpool7(x)
# Flatten the output tensor before passing it through the fully connected layers
# x = x.reshape(x.size(0), 4096 * 2 * 2)
# x = x.reshape(x.size(0), 4096, 2, 2)
x = x.view(x.size(0), -1)
x = self.fc1(x)
x = F.relu(x)
x = self.fc2(x)
return x
class Baseline(nn.Module):
"""
Baseline network
"""
@staticmethod
def weight_init(m):
if isinstance(m, nn.Linear):
init.kaiming_normal_(m.weight)
init.zeros_(m.bias)
def __init__(self, input_channels, n_classes, dropout=False):
super(Baseline, self).__init__()
self.use_dropout = dropout
if dropout:
self.dropout = nn.Dropout(p=0.5)
self.fc1 = nn.Linear(input_channels, 2048)
self.fc2 = nn.Linear(2048, 4096)
self.fc3 = nn.Linear(4096, 2048)
self.fc4 = nn.Linear(2048, n_classes)
self.apply(self.weight_init)
def forward(self, x):
x = F.relu(self.fc1(x))
if self.use_dropout:
x = self.dropout(x)
x = F.relu(self.fc2(x))
if self.use_dropout:
x = self.dropout(x)
x = F.relu(self.fc3(x))
if self.use_dropout:
x = self.dropout(x)
x = self.fc4(x)
return x
class HuEtAl(nn.Module):
"""
Deep Convolutional Neural Networks for Hyperspectral Image Classification
Wei Hu, Yangyu Huang, Li Wei, Fan Zhang and Hengchao Li
Journal of Sensors, Volume 2015 (2015)
https://www.hindawi.com/journals/js/2015/258619/
"""
@staticmethod
def weight_init(m):
# [All the trainable parameters in our CNN should be initialized to
# be a random value between −0.05 and 0.05.]
if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):
init.uniform_(m.weight, -0.05, 0.05)
init.zeros_(m.bias)
def _get_final_flattened_size(self):
with torch.no_grad():
x = torch.zeros(1, 1, self.input_channels)
x = self.pool(self.conv(x))
return x.numel()
def __init__(self, input_channels, n_classes, kernel_size=None, pool_size=None):
super(HuEtAl, self).__init__()
if kernel_size is None:
# [In our experiments, k1 is better to be [ceil](n1/9)]
kernel_size = math.ceil(input_channels / 9)
if pool_size is None:
# The authors recommand that k2's value is chosen so that the pooled features have 30~40 values
# ceil(kernel_size/5) gives the same values as in the paper so let's assume it's okay
pool_size = math.ceil(kernel_size / 5)
self.input_channels = input_channels
# [The first hidden convolution layer C1 filters the n1 x 1 input data with 20 kernels of size k1 x 1]
self.conv = nn.Conv1d(1, 20, kernel_size)
self.pool = nn.MaxPool1d(pool_size)
self.features_size = self._get_final_flattened_size()
# [n4 is set to be 100]
self.fc1 = nn.Linear(self.features_size, 100)
self.fc2 = nn.Linear(100, n_classes)
self.apply(self.weight_init)
def forward(self, x):
# [In our design architecture, we choose the hyperbolic tangent function tanh(u)]
x = x.squeeze(dim=-1).squeeze(dim=-1)
x = x.unsqueeze(1)
x = self.conv(x)
x = torch.tanh(self.pool(x))
x = x.view(-1, self.features_size)
x = torch.tanh(self.fc1(x))
x = self.fc2(x)
return x
class HamidaEtAl(nn.Module):
"""
3-D Deep Learning Approach for Remote Sensing Image Classification
Amina Ben Hamida, Alexandre Benoit, Patrick Lambert, Chokri Ben Amar
IEEE TGRS, 2018
https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8344565
"""
@staticmethod
def weight_init(m):
if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d):
init.kaiming_normal_(m.weight)
init.zeros_(m.bias)
def __init__(self, input_channels, n_classes, patch_size=5, dilation=1):
super(HamidaEtAl, self).__init__()
# The first layer is a (3,3,3) kernel sized Conv characterized
# by a stride equal to 1 and number of neurons equal to 20
self.patch_size = patch_size
self.input_channels = input_channels
dilation = (dilation, 1, 1)
if patch_size == 3:
self.conv1 = nn.Conv3d(
1, 20, (3, 3, 3), stride=(1, 1, 1), dilation=dilation, padding=1)
else:
self.conv1 = nn.Conv3d(
1, 20, (3, 3, 3), stride=(1, 1, 1), dilation=dilation, padding=0)
# Next pooling is applied using a layer identical to the previous one
# with the difference of a 1D kernel size (1,1,3) and a larger stride
# equal to 2 in order to reduce the spectral dimension
self.pool1 = nn.Conv3d(
20, 20, (3, 1, 1), dilation=dilation, stride=(2, 1, 1), padding=(1, 0, 0))
# Then, a duplicate of the first and second layers is created with
# 35 hidden neurons per layer.
self.conv2 = nn.Conv3d(
20, 35, (3, 3, 3), dilation=dilation, stride=(1, 1, 1), padding=(1, 0, 0))
self.pool2 = nn.Conv3d(
35, 35, (3, 1, 1), dilation=dilation, stride=(2, 1, 1), padding=(1, 0, 0))
# Finally, the 1D spatial dimension is progressively reduced
# thanks to the use of two Conv layers, 35 neurons each,
# with respective kernel sizes of (1,1,3) and (1,1,2) and strides
# respectively equal to (1,1,1) and (1,1,2)
self.conv3 = nn.Conv3d(
35, 35, (3, 1, 1), dilation=dilation, stride=(1, 1, 1), padding=(1, 0, 0))
self.conv4 = nn.Conv3d(
35, 35, (2, 1, 1), dilation=dilation, stride=(2, 1, 1), padding=(1, 0, 0))
#self.dropout = nn.Dropout(p=0.5)
self.features_size = self._get_final_flattened_size()
# The architecture ends with a fully connected layer where the number
# of neurons is equal to the number of input classes.
self.fc = nn.Linear(self.features_size, n_classes)
self.apply(self.weight_init)
def _get_final_flattened_size(self):
with torch.no_grad():
x = torch.zeros((1, 1, self.input_channels,
self.patch_size, self.patch_size))
x = self.pool1(self.conv1(x))
x = self.pool2(self.conv2(x))
x = self.conv3(x)
x = self.conv4(x)
_, t, c, w, h = x.size()
return t * c * w * h
def forward(self, x):
x = F.relu(self.conv1(x))
x = self.pool1(x)
x = F.relu(self.conv2(x))
x = self.pool2(x)
x = F.relu(self.conv3(x))
x = F.relu(self.conv4(x))
x = x.view(-1, self.features_size)
#x = self.dropout(x)
x = self.fc(x)
return x
class LeeEtAl(nn.Module):
"""
CONTEXTUAL DEEP CNN BASED HYPERSPECTRAL CLASSIFICATION
Hyungtae Lee and Heesung Kwon
IGARSS 2016
"""
@staticmethod
def weight_init(m):
if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d):
init.kaiming_uniform_(m.weight)
init.zeros_(m.bias)
def __init__(self, in_channels, n_classes):
super(LeeEtAl, self).__init__()
# The first convolutional layer applied to the input hyperspectral
# image uses an inception module that locally convolves the input
# image with two convolutional filters with different sizes
# (1x1xB and 3x3xB where B is the number of spectral bands)
self.conv_3x3 = nn.Conv3d(
1, 128, (in_channels, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
self.conv_1x1 = nn.Conv3d(
1, 128, (in_channels, 1, 1), stride=(1, 1, 1), padding=0)
# We use two modules from the residual learning approach
# Residual block 1
self.conv1 = nn.Conv2d(256, 128, (1, 1))
self.conv2 = nn.Conv2d(128, 128, (1, 1))
self.conv3 = nn.Conv2d(128, 128, (1, 1))
# Residual block 2
self.conv4 = nn.Conv2d(128, 128, (1, 1))
self.conv5 = nn.Conv2d(128, 128, (1, 1))
# The layer combination in the last three convolutional layers
# is the same as the fully connected layers of Alexnet
self.conv6 = nn.Conv2d(128, 128, (1, 1))
self.conv7 = nn.Conv2d(128, 128, (1, 1))
self.conv8 = nn.Conv2d(128, n_classes, (1, 1))
self.lrn1 = nn.LocalResponseNorm(256)
self.lrn2 = nn.LocalResponseNorm(128)
# The 7 th and 8 th convolutional layers have dropout in training
self.dropout = nn.Dropout(p=0.5)
self.apply(self.weight_init)
def forward(self, x):
# Inception module
x_3x3 = self.conv_3x3(x)
x_1x1 = self.conv_1x1(x)
x = torch.cat([x_3x3, x_1x1], dim=1)
# Remove the third dimension of the tensor
x = torch.squeeze(x)
# Local Response Normalization
x = F.relu(self.lrn1(x))
# First convolution
x = self.conv1(x)
# Local Response Normalization
x = F.relu(self.lrn2(x))
# First residual block
x_res = F.relu(self.conv2(x))
x_res = self.conv3(x_res)
x = F.relu(x + x_res)
# Second residual block
x_res = F.relu(self.conv4(x))
x_res = self.conv5(x_res)
x = F.relu(x + x_res)
x = F.relu(self.conv6(x))
x = self.dropout(x)
x = F.relu(self.conv7(x))
x = self.dropout(x)
x = self.conv8(x)
return x
class ChenEtAl(nn.Module):
"""
DEEP FEATURE EXTRACTION AND CLASSIFICATION OF HYPERSPECTRAL IMAGES BASED ON
CONVOLUTIONAL NEURAL NETWORKS
Yushi Chen, Hanlu Jiang, Chunyang Li, Xiuping Jia and Pedram Ghamisi
IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2017
"""
@staticmethod
def weight_init(m):
# In the beginning, the weights are randomly initialized
# with standard deviation 0.001
if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d):
init.normal_(m.weight, std=0.001)
init.zeros_(m.bias)
def __init__(self, input_channels, n_classes, patch_size=27, n_planes=32):
super(ChenEtAl, self).__init__()
self.input_channels = input_channels
self.n_planes = n_planes
self.patch_size = patch_size
self.conv1 = nn.Conv3d(1, n_planes, (32, 4, 4))
self.pool1 = nn.MaxPool3d((1, 2, 2))
self.conv2 = nn.Conv3d(n_planes, n_planes, (32, 4, 4))
self.pool2 = nn.MaxPool3d((1, 2, 2))
self.conv3 = nn.Conv3d(n_planes, n_planes, (32, 4, 4))
self.features_size = self._get_final_flattened_size()
self.fc = nn.Linear(self.features_size, n_classes)
self.dropout = nn.Dropout(p=0.5)
self.apply(self.weight_init)
def _get_final_flattened_size(self):
with torch.no_grad():
x = torch.zeros((1, 1, self.input_channels,
self.patch_size, self.patch_size))
x = self.pool1(self.conv1(x))
x = self.pool2(self.conv2(x))
x = self.conv3(x)
_, t, c, w, h = x.size()
return t * c * w * h
def forward(self, x):
x = F.relu(self.conv1(x))
x = self.pool1(x)
x = self.dropout(x)
x = F.relu(self.conv2(x))
x = self.pool2(x)
x = self.dropout(x)
x = F.relu(self.conv3(x))
x = self.dropout(x)
x = x.view(-1, self.features_size)
x = self.fc(x)
return x
class LiEtAl(nn.Module):
"""
SPECTRAL–SPATIAL CLASSIFICATION OF HYPERSPECTRAL IMAGERY
WITH 3D CONVOLUTIONAL NEURAL NETWORK
Ying Li, Haokui Zhang and Qiang Shen
MDPI Remote Sensing, 2017
http://www.mdpi.com/2072-4292/9/1/67
"""
@staticmethod
def weight_init(m):
if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d):
init.xavier_uniform_(m.weight.data)
init.constant_(m.bias.data, 0)
def __init__(self, input_channels, n_classes, n_planes=2, patch_size=5):
super(LiEtAl, self).__init__()
self.input_channels = input_channels
self.n_planes = n_planes
self.patch_size = patch_size
# The proposed 3D-CNN model has two 3D convolution layers (C1 and C2)
# and a fully-connected layer (F1)
# we fix the spatial size of the 3D convolution kernels to 3 × 3
# while only slightly varying the spectral depth of the kernels
# for the Pavia University and Indian Pines scenes, those in C1 and C2
# were set to seven and three, respectively
self.conv1 = nn.Conv3d(1, n_planes, (7, 3, 3), padding=(1, 0, 0))
# the number of kernels in the second convolution layer is set to be
# twice as many as that in the first convolution layer
self.conv2 = nn.Conv3d(n_planes, 2 * n_planes,
(3, 3, 3), padding=(1, 0, 0))
#self.dropout = nn.Dropout(p=0.5)
self.features_size = self._get_final_flattened_size()
self.fc = nn.Linear(self.features_size, n_classes)
self.apply(self.weight_init)
def _get_final_flattened_size(self):
with torch.no_grad():
x = torch.zeros((1, 1, self.input_channels,
self.patch_size, self.patch_size))
x = self.conv1(x)
x = self.conv2(x)
_, t, c, w, h = x.size()
return t * c * w * h
def forward(self, x):
x = F.relu(self.conv1(x))
x = F.relu(self.conv2(x))
x = x.view(-1, self.features_size)
#x = self.dropout(x)
x = self.fc(x)
return x
class HeEtAl(nn.Module):
"""
MULTI-SCALE 3D DEEP CONVOLUTIONAL NEURAL NETWORK FOR HYPERSPECTRAL
IMAGE CLASSIFICATION
Mingyi He, Bo Li, Huahui Chen
IEEE International Conference on Image Processing (ICIP) 2017
https://ieeexplore.ieee.org/document/8297014/
"""
@staticmethod
def weight_init(m):
if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d):
init.kaiming_uniform(m.weight)
init.zeros_(m.bias)
def __init__(self, input_channels, n_classes, patch_size=7):
super(HeEtAl, self).__init__()
self.input_channels = input_channels
self.patch_size = patch_size
self.conv1 = nn.Conv3d(1, 16, (11, 3, 3), stride=(3,1,1))
self.conv2_1 = nn.Conv3d(16, 16, (1, 1, 1), padding=(0,0,0))
self.conv2_2 = nn.Conv3d(16, 16, (3, 1, 1), padding=(1,0,0))
self.conv2_3 = nn.Conv3d(16, 16, (5, 1, 1), padding=(2,0,0))
self.conv2_4 = nn.Conv3d(16, 16, (11, 1, 1), padding=(5,0,0))
self.conv3_1 = nn.Conv3d(16, 16, (1, 1, 1), padding=(0,0,0))
self.conv3_2 = nn.Conv3d(16, 16, (3, 1, 1), padding=(1,0,0))
self.conv3_3 = nn.Conv3d(16, 16, (5, 1, 1), padding=(2,0,0))
self.conv3_4 = nn.Conv3d(16, 16, (11, 1, 1), padding=(5,0,0))
self.conv4 = nn.Conv3d(16, 16, (3, 2, 2))
self.pooling = nn.MaxPool2d((3,2,2), stride=(3,2,2))
# the ratio of dropout is 0.6 in our experiments
self.dropout = nn.Dropout(p=0.6)
self.features_size = self._get_final_flattened_size()
self.fc = nn.Linear(self.features_size, n_classes)
self.apply(self.weight_init)
def _get_final_flattened_size(self):
with torch.no_grad():
x = torch.zeros((1, 1, self.input_channels,
self.patch_size, self.patch_size))
x = self.conv1(x)
x2_1 = self.conv2_1(x)
x2_2 = self.conv2_2(x)
x2_3 = self.conv2_3(x)
x2_4 = self.conv2_4(x)
x = x2_1 + x2_2 + x2_3 + x2_4
x3_1 = self.conv3_1(x)
x3_2 = self.conv3_2(x)
x3_3 = self.conv3_3(x)
x3_4 = self.conv3_4(x)
x = x3_1 + x3_2 + x3_3 + x3_4
x = self.conv4(x)
_, t, c, w, h = x.size()
return t * c * w * h
def forward(self, x):
x = F.relu(self.conv1(x))
x2_1 = self.conv2_1(x)
x2_2 = self.conv2_2(x)
x2_3 = self.conv2_3(x)
x2_4 = self.conv2_4(x)
x = x2_1 + x2_2 + x2_3 + x2_4
x = F.relu(x)
x3_1 = self.conv3_1(x)
x3_2 = self.conv3_2(x)
x3_3 = self.conv3_3(x)
x3_4 = self.conv3_4(x)
x = x3_1 + x3_2 + x3_3 + x3_4
x = F.relu(x)
x = F.relu(self.conv4(x))
x = x.view(-1, self.features_size)
x = self.dropout(x)
x = self.fc(x)
return x
class LuoEtAl(nn.Module):
"""
HSI-CNN: A Novel Convolution Neural Network for Hyperspectral Image
Yanan Luo, Jie Zou, Chengfei Yao, Tao Li, Gang Bai
International Conference on Pattern Recognition 2018
"""
@staticmethod
def weight_init(m):
if isinstance(m, (nn.Linear, nn.Conv2d, nn.Conv3d)):
init.kaiming_uniform_(m.weight)
init.zeros_(m.bias)
def __init__(self, input_channels, n_classes, patch_size=3, n_planes=90):
super(LuoEtAl, self).__init__()
self.input_channels = input_channels
self.patch_size = patch_size
self.n_planes = n_planes
# the 8-neighbor pixels [...] are fed into the Conv1 convolved by n1 kernels
# and s1 stride. Conv1 results are feature vectors each with height of and
# the width is 1. After reshape layer, the feature vectors becomes an image-like
# 2-dimension data.
# Conv2 has 64 kernels size of 3x3, with stride s2.
# After that, the 64 results are drawn into a vector as the input of the fully
# connected layer FC1 which has n4 nodes.
# In the four datasets, the kernel height nk1 is 24 and stride s1, s2 is 9 and 1
self.conv1 = nn.Conv3d(1, 90, (24, 3, 3), padding=0, stride=(9,1,1))
self.conv2 = nn.Conv2d(1, 64, (3, 3), stride=(1, 1))
self.features_size = self._get_final_flattened_size()
self.fc1 = nn.Linear(self.features_size, 1024)
self.fc2 = nn.Linear(1024, n_classes)
self.apply(self.weight_init)
def _get_final_flattened_size(self):
with torch.no_grad():
x = torch.zeros((1, 1, self.input_channels,
self.patch_size, self.patch_size))
x = self.conv1(x)
b = x.size(0)
x = x.view(b, 1, -1, self.n_planes)
x = self.conv2(x)
_, c, w, h = x.size()
return c * w * h
def forward(self, x):
x = F.relu(self.conv1(x))
b = x.size(0)
x = x.view(b, 1, -1, self.n_planes)
x = F.relu(self.conv2(x))
x = x.view(-1, self.features_size)
x = F.relu(self.fc1(x))
x = self.fc2(x)
return x
class SharmaEtAl(nn.Module):
"""
HYPERSPECTRAL CNN FOR IMAGE CLASSIFICATION & BAND SELECTION, WITH APPLICATION
TO FACE RECOGNITION
Vivek Sharma, Ali Diba, Tinne Tuytelaars, Luc Van Gool
Technical Report, KU Leuven/ETH Zürich
"""
@staticmethod
def weight_init(m):
if isinstance(m, (nn.Linear, nn.Conv3d)):
init.kaiming_normal_(m.weight)
init.zeros_(m.bias)
def __init__(self, input_channels, n_classes, patch_size=64):
super(SharmaEtAl, self).__init__()
self.input_channels = input_channels
self.patch_size = patch_size
# An input image of size 263x263 pixels is fed to conv1
# with 96 kernels of size 6x6x96 with a stride of 2 pixels
self.conv1 = nn.Conv3d(1, 96, (input_channels, 6, 6), stride=(1,2,2))
self.conv1_bn = nn.BatchNorm3d(96)
self.pool1 = nn.MaxPool3d((1, 2, 2))
#  256 kernels of size 3x3x256 with a stride of 2 pixels
self.conv2 = nn.Conv3d(1, 256, (96, 3, 3), stride=(1,2,2))
self.conv2_bn = nn.BatchNorm3d(256)
self.pool2 = nn.MaxPool3d((1, 2, 2))
# 512 kernels of size 3x3x512 with a stride of 1 pixel
self.conv3 = nn.Conv3d(1, 512, (256, 3, 3), stride=(1,1,1))
# Considering those large kernel values, I assume they actually merge the
# 3D tensors at each step
self.features_size = self._get_final_flattened_size()
# The fc1 has 1024 outputs, where dropout was applied after
# fc1 with a rate of 0.5
self.fc1 = nn.Linear(self.features_size, 1024)
self.dropout = nn.Dropout(p=0.5)
self.fc2 = nn.Linear(1024, n_classes)
self.apply(self.weight_init)
def _get_final_flattened_size(self):
with torch.no_grad():
x = torch.zeros((1, 1, self.input_channels,
self.patch_size, self.patch_size))
x = F.relu(self.conv1_bn(self.conv1(x)))
x = self.pool1(x)
print(x.size())
b, t, c, w, h = x.size()
x = x.view(b, 1, t*c, w, h)
x = F.relu(self.conv2_bn(self.conv2(x)))
x = self.pool2(x)
print(x.size())
b, t, c, w, h = x.size()
x = x.view(b, 1, t*c, w, h)
x = F.relu(self.conv3(x))
print(x.size())
_, t, c, w, h = x.size()
return t * c * w * h
def forward(self, x):
x = F.relu(self.conv1_bn(self.conv1(x)))
x = self.pool1(x)
b, t, c, w, h = x.size()
x = x.view(b, 1, t*c, w, h)
x = F.relu(self.conv2_bn(self.conv2(x)))
x = self.pool2(x)
b, t, c, w, h = x.size()
x = x.view(b, 1, t*c, w, h)
x = F.relu(self.conv3(x))
x = x.view(-1, self.features_size)
x = self.fc1(x)
x = self.dropout(x)
x = self.fc2(x)
return x
class LiuEtAl(nn.Module):
"""
A semi-supervised convolutional neural network for hyperspectral image classification
Bing Liu, Xuchu Yu, Pengqiang Zhang, Xiong Tan, Anzhu Yu, Zhixiang Xue
Remote Sensing Letters, 2017
"""
@staticmethod
def weight_init(m):
if isinstance(m, (nn.Linear, nn.Conv2d)):
init.kaiming_normal_(m.weight)
init.zeros_(m.bias)
def __init__(self, input_channels, n_classes, patch_size=9):
super(LiuEtAl, self).__init__()
self.input_channels = input_channels
self.patch_size = patch_size
self.aux_loss_weight = 1
# "W1 is a 3x3xB1 kernel [...] B1 is the number of the output bands for the convolutional
# "and pooling layer" -> actually 3x3 2D convolutions with B1 outputs
# "the value of B1 is set to be 80"
self.conv1 = nn.Conv2d(input_channels, 80, (3, 3))
self.pool1 = nn.MaxPool2d((2, 2))
self.conv1_bn = nn.BatchNorm2d(80)
self.features_sizes = self._get_sizes()
self.fc_enc = nn.Linear(self.features_sizes[2], n_classes)
# Decoder
self.fc1_dec = nn.Linear(self.features_sizes[2], self.features_sizes[2])
self.fc1_dec_bn = nn.BatchNorm1d(self.features_sizes[2])
self.fc2_dec = nn.Linear(self.features_sizes[2], self.features_sizes[1])
self.fc2_dec_bn = nn.BatchNorm1d(self.features_sizes[1])
self.fc3_dec = nn.Linear(self.features_sizes[1], self.features_sizes[0])
self.fc3_dec_bn = nn.BatchNorm1d(self.features_sizes[0])
self.fc4_dec = nn.Linear(self.features_sizes[0], input_channels)
self.apply(self.weight_init)
def _get_sizes(self):
x = torch.zeros((1, self.input_channels,
self.patch_size, self.patch_size))
x = F.relu(self.conv1_bn(self.conv1(x)))
_, c, w, h = x.size()
size0 = c * w * h
x = self.pool1(x)
_, c, w, h = x.size()
size1 = c * w * h
x = self.conv1_bn(x)
_, c, w, h = x.size()
size2 = c * w * h
return size0, size1, size2
def forward(self, x):
x = x.squeeze()
x_conv1 = self.conv1_bn(self.conv1(x))
x = x_conv1
x_pool1 = self.pool1(x)
x = x_pool1
x_enc = F.relu(x).view(-1, self.features_sizes[2])
x = x_enc
x_classif = self.fc_enc(x)
#x = F.relu(self.fc1_dec_bn(self.fc1_dec(x) + x_enc))
x = F.relu(self.fc1_dec(x))
x = F.relu(self.fc2_dec_bn(self.fc2_dec(x) + x_pool1.view(-1, self.features_sizes[1])))
x = F.relu(self.fc3_dec_bn(self.fc3_dec(x) +x_conv1.view(-1, self.features_sizes[0])))
x = self.fc4_dec(x)
return x_classif, x
class BoulchEtAl(nn.Module):
"""
Autoencodeurs pour la visualisation d'images hyperspectrales
A.Boulch, N. Audebert, D. Dubucq
GRETSI 2017
"""
@staticmethod
def weight_init(m):
if isinstance(m, (nn.Linear, nn.Conv1d)):
init.kaiming_normal_(m.weight)
init.zeros_(m.bias)
def __init__(self, input_channels, n_classes, planes=16):
super(BoulchEtAl, self).__init__()
self.input_channels = input_channels
self.aux_loss_weight = 0.1
encoder_modules = []
n = input_channels
with torch.no_grad():
x = torch.zeros((10, 1, self.input_channels))
print(x.size())
while(n > 1):
print("---------- {} ---------".format(n))
if n == input_channels:
p1, p2 = 1, 2 * planes
elif n == input_channels // 2:
p1, p2 = 2 * planes, planes
else:
p1, p2 = planes, planes
encoder_modules.append(nn.Conv1d(p1, p2, 3, padding=1))
x = encoder_modules[-1](x)
print(x.size())
encoder_modules.append(nn.MaxPool1d(2))
x = encoder_modules[-1](x)
print(x.size())
encoder_modules.append(nn.ReLU(inplace=True))
x = encoder_modules[-1](x)
print(x.size())
encoder_modules.append(nn.BatchNorm1d(p2))
x = encoder_modules[-1](x)
print(x.size())
n = n // 2
encoder_modules.append(nn.Conv1d(planes, 3, 3, padding=1))
encoder_modules.append(nn.Tanh())
self.encoder = nn.Sequential(*encoder_modules)
self.features_sizes = self._get_sizes()
self.classifier = nn.Linear(self.features_sizes, n_classes)
self.regressor = nn.Linear(self.features_sizes, input_channels)
self.apply(self.weight_init)
def _get_sizes(self):
with torch.no_grad():
x = torch.zeros((10, 1, self.input_channels))
x = self.encoder(x)
_, c, w = x.size()
return c*w
def forward(self, x):
x = x.unsqueeze(1)
x = self.encoder(x)
x = x.view(-1, self.features_sizes)
x_classif = self.classifier(x)
x = self.regressor(x)
return x_classif, x
class MouEtAl(nn.Module):
"""
Deep recurrent neural networks for hyperspectral image classification
Lichao Mou, Pedram Ghamisi, Xiao Xang Zhu
https://ieeexplore.ieee.org/document/7914752/
"""
@staticmethod
def weight_init(m):
# All weight matrices in our RNN and bias vectors are initialized with a uniform distribution, and the values of these weight matrices and bias vectors are initialized in the range [−0.1,0.1]
if isinstance(m, (nn.Linear, nn.GRU)):
init.uniform_(m.weight.data, -0.1, 0.1)
init.uniform_(m.bias.data, -0.1, 0.1)
def __init__(self, input_channels, n_classes):
# The proposed network model uses a single recurrent layer that adopts our modified GRUs of size 64 with sigmoid gate activation and PRetanh activation functions for hidden representations
super(MouEtAl, self).__init__()
self.input_channels = input_channels
self.gru = nn.GRU(1, 64, 1, bidirectional=False) # TODO: try to change this ?
self.gru_bn = nn.BatchNorm1d(64*input_channels)
self.tanh = nn.Tanh()
self.fc = nn.Linear(64*input_channels, n_classes)
def forward(self, x):
x = x.squeeze()
x = x.unsqueeze(0)
# x is in 1, N, C but we expect C, N, 1 for GRU layer
x = x.permute(2, 1,0)
x = self.gru(x)[0]
# x is in C, N, 64, we permute back
x = x.permute(1,2,0).contiguous()
x = x.view(x.size(0), -1)
x = self.gru_bn(x)
x = self.tanh(x)
x = self.fc(x)
return x
# def train(net, optimizer, criterion, data_loader, epoch, scheduler=None,
#           display_iter=100, device=torch.device('cpu'), display=None,
#           val_loader=None, supervision='full'):
#     """
#     Training loop to optimize a network for several epochs and a specified loss
#     Args:
#         net: a PyTorch model
#         optimizer: a PyTorch optimizer
#         data_loader: a PyTorch dataset loader
#         epoch: int specifying the number of training epochs
#         criterion: a PyTorch-compatible loss function, e.g. nn.CrossEntropyLoss
#         device (optional): torch device to use (defaults to CPU)
#         display_iter (optional): number of iterations before refreshing the
#         display (False/None to switch off).
#         scheduler (optional): PyTorch scheduler
#         val_loader (optional): validation dataset
#         supervision (optional): 'full' or 'semi'
#     """
#     if criterion is None:
#         raise Exception("Missing criterion. You must specify a loss function.")
#     net.to(device)
#     save_epoch = epoch // 20 if epoch > 20 else 1
#     losses = np.zeros(1000000)
#     mean_losses = np.zeros(100000000)
#     iter_ = 1
#     loss_win, val_win = None, None
#     val_accuracies = []
#     for e in tqdm(range(1, epoch + 1), desc="Training the network"):
#         # Set the network to training mode
#         net.train()
#         avg_loss = 0.
#         # Run the training loop for one epoch
#         for batch_idx, (data, target) in tqdm(enumerate(data_loader), total=len(data_loader)):
#             # Load the data into the GPU if required
#             data, target = data.to(device), target.to(device)
#             optimizer.zero_grad()
#             if supervision == 'full':
#                 output = net(data)
#                 #target = target - 1
#                 loss = criterion(output, target)
#             elif supervision == 'semi':
#                 outs = net(data)
#                 output, rec = outs
#                 #target = target - 1
#                 loss = criterion[0](output, target) + net.aux_loss_weight * criterion[1](rec, data)
#             else:
#                 raise ValueError("supervision mode \"{}\" is unknown.".format(supervision))
#             loss.backward()
#             optimizer.step()
#             avg_loss += loss.item()
#             losses[iter_] = loss.item()
#             mean_losses[iter_] = np.mean(losses[max(0, iter_ - 100):iter_ + 1])
#             if display_iter and iter_ % display_iter == 0:
#                 string = 'Train (epoch {}/{}) [{}/{} ({:.0f}%)]\tLoss: {:.6f}'
#                 string = string.format(
#                     e, epoch, batch_idx *
#                     len(data), len(data) * len(data_loader),
#                     100. * batch_idx / len(data_loader), mean_losses[iter_])
#                 update = None if loss_win is None else 'append'
#                 loss_win = display.line(
#                     X=np.arange(iter_ - display_iter, iter_),
#                     Y=mean_losses[iter_ - display_iter:iter_],
#                     win=loss_win,
#                     update=update,
#                     opts={'title': "Training loss",
#                           'xlabel': "Iterations",
#                           'ylabel': "Loss"
#                          }
#                 )
#                 tqdm.write(string)
#                 if len(val_accuracies) > 0:
#                     val_win = display.line(Y=np.array(val_accuracies),
#                                            X=np.arange(len(val_accuracies)),
#                                            win=val_win,
#                                            opts={'title': "Validation accuracy",
#                                                  'xlabel': "Epochs",
#                                                  'ylabel': "Accuracy"
#                                                 })
#             iter_ += 1
#             del(data, target, loss, output)
#         # Update the scheduler
#         avg_loss /= len(data_loader)
#         if val_loader is not None:
#             val_acc = val(net, val_loader, device=device, supervision=supervision)
#             val_accuracies.append(val_acc)
#             metric = -val_acc
#         else:
#             metric = avg_loss
#         if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
#             scheduler.step(metric)
#         elif scheduler is not None:
#             scheduler.step()
#         # Save the weights
#         if e % save_epoch == 0:
#             save_model(net, camel_to_snake(str(net.__class__.__name__)), data_loader.dataset.name, epoch=e, metric=abs(metric))
# def save_model(model, model_name, dataset_name, **kwargs):
#      model_dir = './checkpoints/' + model_name + "/" + dataset_name + "/"
#      if not os.path.isdir(model_dir):
#          os.makedirs(model_dir, exist_ok=True)
#      if isinstance(model, torch.nn.Module):
#          filename = str('run') + "_epoch{epoch}_{metric:.2f}".format(**kwargs)
#          tqdm.write("Saving neural network weights in {}".format(filename))
#          torch.save(model.state_dict(), model_dir + filename + '.pth')
#      else:
#          filename = str('run')
#          tqdm.write("Saving model params in {}".format(filename))
#          joblib.dump(model, model_dir + filename + '.pkl')
# def test(net, img, hyperparams):
#     """
#     Test a model on a specific image
#     """
#     net.eval()
#     patch_size = hyperparams['patch_size']
#     center_pixel = hyperparams['center_pixel']
#     batch_size, device = hyperparams['batch_size'], hyperparams['device']
#     n_classes = hyperparams['n_classes']
#     kwargs = {'step': hyperparams['test_stride'], 'window_size': (patch_size, patch_size)}
#     probs = np.zeros(img.shape[:2] + (n_classes,))
#     iterations = count_sliding_window(img, **kwargs) // batch_size
#     for batch in tqdm(grouper(batch_size, sliding_window(img, **kwargs)),
#                       total=(iterations),
#                       desc="Inference on the image"
#                       ):
#         with torch.no_grad():
#             if patch_size == 1:
#                 data = [b[0][0, 0] for b in batch]
#                 data = np.copy(data)
#                 data = torch.from_numpy(data)
#             else:
#                 data = [b[0] for b in batch]
#                 data = np.copy(data)
#                 data = data.transpose(0, 3, 1, 2)
#                 data = torch.from_numpy(data)
#                 data = data.unsqueeze(1)
#             indices = [b[1:] for b in batch]
#             data = data.to(device)
#             output = net(data)
#             if isinstance(output, tuple):
#                 output = output[0]
#             output = output.to('cpu')
#             if patch_size == 1 or center_pixel:
#                 output = output.numpy()
#             else:
#                 output = np.transpose(output.numpy(), (0, 2, 3, 1))
#             for (x, y, w, h), out in zip(indices, output):
#                 if center_pixel:
#                     probs[x + w // 2, y + h // 2] += out
#                 else:
#                     probs[x:x + w, y:y + h] += out
#     return probs
# def val(net, data_loader, device='cpu', supervision='full'):
# # TODO : fix me using metrics()
#     net.eval()
#     accuracy, total = 0., 0.
#     ignored_labels = data_loader.dataset.ignored_labels
#     for batch_idx, (data, target) in enumerate(data_loader):
#         with torch.no_grad():
#             # Load the data into the GPU if required
#             data, target = data.to(device), target.to(device)
#             if supervision == 'full':
#                 output = net(data)
#             elif supervision == 'semi':
#                 outs = net(data)
#                 output, rec = outs
#             _, output = torch.max(output, dim=1)
#             #target = target - 1
#             for pred, out in zip(output.view(-1), target.view(-1)):
#                 if out.item() in ignored_labels:
#                     continue
#                 else:
#                     accuracy += out.item() == pred.item()
#                     total += 1
#     return accuracy / total
import os
import torch
from tqdm import tqdm
import numpy as np
def train(net, optimizer, criterion, data_loader, epoch, scheduler=None,
display_iter=100, device=torch.device('cpu'), display=None,
val_loader=None, supervision='full', continue_training=True):
"""
Training loop to optimize a network for several epochs and a specified loss
Args:
net: a PyTorch model
optimizer: a PyTorch optimizer
data_loader: a PyTorch dataset loader
epoch: int specifying the number of training epochs
criterion: a PyTorch-compatible loss function, e.g. nn.CrossEntropyLoss
device (optional): torch device to use (defaults to CPU)
display_iter (optional): number of iterations before refreshing the
display (False/None to switch off).
scheduler (optional): PyTorch scheduler
val_loader (optional): validation dataset
supervision (optional): 'full' or 'semi'
continue_training (optional): boolean flag to continue training
"""
if criterion is None:
raise Exception("Missing criterion. You must specify a loss function.")
if not continue_training:
print("Skipping training as continue_training is set to False.")
return
net.to(device)
best_metric = float('-inf')  # Best validation metric (accuracy or loss)
best_model_path = None  # Path to the best model
losses = np.zeros(1000000)
mean_losses = np.zeros(100000000)
iter_ = 1
loss_win, val_win = None, None
val_accuracies = []
for e in tqdm(range(1, epoch + 1), desc="Training the network"):
# Set the network to training mode
net.train()
avg_loss = 0.
# Run the training loop for one epoch
for batch_idx, (data, target) in tqdm(enumerate(data_loader), total=len(data_loader)):
# Load the data into the GPU if required
data, target = data.to(device), target.to(device)
optimizer.zero_grad()
if supervision == 'full':
output = net(data)
loss = criterion(output, target)
elif supervision == 'semi':
outs = net(data)
output, rec = outs
loss = criterion[0](output, target) + net.aux_loss_weight * criterion[1](rec, data)
else:
raise ValueError("supervision mode \"{}\" is unknown.".format(supervision))
loss.backward()
optimizer.step()
avg_loss += loss.item()
losses[iter_] = loss.item()
mean_losses[iter_] = np.mean(losses[max(0, iter_ - 100):iter_ + 1])
if display_iter and iter_ % display_iter == 0:
string = 'Train (epoch {}/{}) [{}/{} ({:.0f}%)]\tLoss: {:.6f}'
string = string.format(
e, epoch, batch_idx * len(data), len(data) * len(data_loader),
100. * batch_idx / len(data_loader), mean_losses[iter_])
update = None if loss_win is None else 'append'
loss_win = display.line(
X=np.arange(iter_ - display_iter, iter_),
Y=mean_losses[iter_ - display_iter:iter_],
win=loss_win,
update=update,
opts={'title': "Training loss",
'xlabel': "Iterations",
'ylabel': "Loss"
}
)
tqdm.write(string)
if len(val_accuracies) > 0:
val_win = display.line(Y=np.array(val_accuracies),
X=np.arange(len(val_accuracies)),
win=val_win,
opts={'title': "Validation accuracy",
'xlabel': "Epochs",
'ylabel': "Accuracy"
})
iter_ += 1
del(data, target, loss, output)
# Update the scheduler
avg_loss /= len(data_loader)
if val_loader is not None:
val_acc = val(net, val_loader, device=device, supervision=supervision)
val_accuracies.append(val_acc)
metric = val_acc  # Use validation accuracy as the metric
else:
metric = avg_loss  # Use training loss if no validation set is provided
if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
scheduler.step(metric)
elif scheduler is not None:
scheduler.step()
# Save the best model
if metric > best_metric:
best_metric = metric
if best_model_path:
os.remove(best_model_path)  # Remove the previous best model file
best_model_path = save_model(net, camel_to_snake(str(net.__class__.__name__)), data_loader.dataset.name, epoch=e, metric=abs(metric))
print(f"New best model saved with metric {best_metric:.4f}")
def save_model(model, model_name, dataset_name, **kwargs):
model_dir = './checkpoints/' + model_name + "/" + dataset_name + "/"
if not os.path.isdir(model_dir):
os.makedirs(model_dir, exist_ok=True)
if isinstance(model, torch.nn.Module):
filename = str('best_model') + "_epoch{epoch}_{metric:.2f}".format(**kwargs)
tqdm.write("Saving neural network weights in {}".format(filename))
model_path = model_dir + filename + '.pth'
torch.save(model.state_dict(), model_path)
return model_path
else:
filename = str('best_model')
tqdm.write("Saving model params in {}".format(filename))
joblib.dump(model, model_dir + filename + '.pkl')
return model_dir + filename + '.pkl'
def test(net, img, hyperparams):
"""
Test a model on a specific image
"""
net.eval()
patch_size = hyperparams['patch_size']
center_pixel = hyperparams['center_pixel']
batch_size, device = hyperparams['batch_size'], hyperparams['device']
n_classes = hyperparams['n_classes']
kwargs = {'step': hyperparams['test_stride'], 'window_size': (patch_size, patch_size)}
probs = np.zeros(img.shape[:2] + (n_classes,))
iterations = count_sliding_window(img, **kwargs) // batch_size
for batch in tqdm(grouper(batch_size, sliding_window(img, **kwargs)),
total=(iterations),
desc="Inference on the image"
):
with torch.no_grad():
if patch_size == 1:
data = [b[0][0, 0] for b in batch]
data = np.copy(data)
data = torch.from_numpy(data)
else:
data = [b[0] for b in batch]
data = np.copy(data)
data = data.transpose(0, 3, 1, 2)
data = torch.from_numpy(data)
data = data.unsqueeze(1)
indices = [b[1:] for b in batch]
data = data.to(device)
output = net(data)
if isinstance(output, tuple):
output = output[0]
output = output.to('cpu')
if patch_size == 1 or center_pixel:
output = output.numpy()
else:
output = np.transpose(output.numpy(), (0, 2, 3, 1))
for (x, y, w, h), out in zip(indices, output):
if center_pixel:
probs[x + w // 2, y + h // 2] += out
else:
probs[x:x + w, y:y + h] += out
return probs
def val(net, data_loader, device='cpu', supervision='full'):
"""
Validate a model on a specific dataset
"""
net.eval()
accuracy, total = 0., 0.
ignored_labels = data_loader.dataset.ignored_labels
for batch_idx, (data, target) in enumerate(data_loader):
with torch.no_grad():
# Load the data into the GPU if required
data, target = data.to(device), target.to(device)
if supervision == 'full':
output = net(data)
elif supervision == 'semi':
outs = net(data)
output, rec = outs
_, output = torch.max(output, dim=1)
for pred, out in zip(output.view(-1), target.view(-1)):
if out.item() in ignored_labels:
continue
else:
accuracy += out.item() == pred.item()
total += 1
return accuracy / total