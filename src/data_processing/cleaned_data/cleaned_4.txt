from __future__ import absolute_import, division, print_function, unicode_literals
import argparse
import cv2
import torch
import numpy as np
from pysot.core.config import cfg
from pysot.models.model_builder import ModelBuilder
from pysot.tracker.tracker_builder import build_tracker
import logging
import time
from collections import deque
import random
import math
torch.backends.cudnn.benchmark = True
logging.basicConfig(level=logging.INFO,
format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
JUMP_THRESHOLD_BASE = 500
class AppearanceModel:
def __init__(self, feature_memory_size=5, spectrum_type=None):
self.initialize_spectrum_adaptive_orb(spectrum_type)
self.feature_memory = deque(maxlen=feature_memory_size)
self.color_memory = deque(maxlen=feature_memory_size)
self.min_match_count = 10
self.template = None
self.template_size = None
self.alpha = 0.95
self.spectrum_type = spectrum_type
def initialize_spectrum_adaptive_orb(self, spectrum_type=None):
"""
Initialize ORB with parameters optimized for different spectrum types.
"""
nfeatures = 2000
scaleFactor = 1.15
nlevels = 10
edgeThreshold = 35
patchSize = 37
fastThreshold = 15
if spectrum_type:
if spectrum_type == "MWIR":
fastThreshold = 12
edgeThreshold = 31
nlevels = 12
elif spectrum_type == "SWIR":
fastThreshold = 17
scaleFactor = 1.2
patchSize = 33
elif spectrum_type == "LWIR":
fastThreshold = 10
nfeatures = 2500
edgeThreshold = 41
patchSize = 41
elif spectrum_type == "FLIR":
fastThreshold = 13
edgeThreshold = 33
nfeatures = 2200
elif spectrum_type == "VIS":
fastThreshold = 20
edgeThreshold = 31
patchSize = 31
nfeatures = 1500
self.feature_extractor = cv2.ORB_create(
nfeatures=nfeatures,
scaleFactor=scaleFactor,
nlevels=nlevels,
edgeThreshold=edgeThreshold,
firstLevel=0,
WTA_K=3,
scoreType=cv2.ORB_HARRIS_SCORE,
patchSize=patchSize,
fastThreshold=fastThreshold
)
self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
def extract_adaptive_features(self, frame, bbox=None, object_type=None):
"""
Extract features with adaptive processing based on image characteristics and object type.
"""
if len(frame.shape) == 3:
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
else:
gray = frame.copy()
mean_val = np.mean(gray)
std_val = np.std(gray)
if std_val < 30:
clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
gray = clahe.apply(gray)
sigma_color = 20 if std_val < 40 else 10
sigma_space = 10 if mean_val < 100 else 7
gray = cv2.bilateralFilter(gray, 9, sigma_color, sigma_space)
original_nfeatures = self.feature_extractor.getMaxFeatures()
if object_type:
if object_type == 'large':
self.feature_extractor.setMaxFeatures(int(original_nfeatures * 1.5))
elif object_type == 'small':
self.feature_extractor.setMaxFeatures(int(original_nfeatures * 0.8))
original_patch_size = self.feature_extractor.getPatchSize()
self.feature_extractor.setPatchSize(max(15, original_patch_size - 10))
elif object_type == 'flat':
self.feature_extractor.setMaxFeatures(int(original_nfeatures * 1.2))
original_fast_threshold = self.feature_extractor.getFastThreshold()
self.feature_extractor.setFastThreshold(original_fast_threshold + 5)
if bbox:
x, y, w, h = [int(v) for v in bbox]
margin = int(min(w, h) * 0.2)
x = max(0, x - margin)
y = max(0, y - margin)
w = min(gray.shape[1] - x, w + 2*margin)
h = min(gray.shape[0] - y, h + 2*margin)
roi = gray[y:y+h, x:x+w]
keypoints = self.feature_extractor.detect(roi, None)
for kp in keypoints:
kp.pt = (kp.pt[0] + x, kp.pt[1] + y)
else:
keypoints = self.feature_extractor.detect(gray, None)
keypoints, descriptors = self.feature_extractor.compute(gray, keypoints)
if object_type:
self.feature_extractor.setMaxFeatures(original_nfeatures)
if object_type == 'small':
self.feature_extractor.setPatchSize(original_patch_size)
elif object_type == 'flat':
self.feature_extractor.setFastThreshold(original_fast_threshold)
return keypoints, descriptors
def get_precise_object_boundaries(self, frame, initial_bbox, spectrum_type=None):
"""
Enhanced boundary detection with context awareness
"""
x, y, w, h = [int(v) for v in initial_bbox]
search_margin = max(int(min(w, h) * 0.3), 30)
roi_x = max(0, x - search_margin)
roi_y = max(0, y - search_margin)
roi_w = min(frame.shape[1] - roi_x, w + 2*search_margin)
roi_h = min(frame.shape[0] - roi_y, h + 2*search_margin)
roi = frame[roi_y:roi_y+roi_h, roi_x:roi_x+roi_w]
if roi.size == 0:
return initial_bbox
if len(roi.shape) == 3:
gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
else:
gray_roi = roi.copy()
mean_val = np.mean(gray_roi)
std_val = np.std(gray_roi)
low_thresh_base = max(10, int(mean_val * 0.2))
high_thresh_base = max(30, int(mean_val * 0.5))
if spectrum_type in ["LWIR", "MWIR"]:
low_thresh_base = max(5, int(low_thresh_base * 0.7))
high_thresh_base = max(15, int(high_thresh_base * 0.7))
elif spectrum_type == "SWIR":
low_thresh_base = int(low_thresh_base * 1.1)
high_thresh_base = int(high_thresh_base * 1.1)
edges_combined = np.zeros_like(gray_roi)
for scale in [0.8, 1.0, 1.3]:
low_thresh = int(low_thresh_base * scale)
high_thresh = int(high_thresh_base * scale)
edges = cv2.Canny(gray_roi, low_thresh, high_thresh)
edges_combined = cv2.bitwise_or(edges_combined, edges)
kernel = np.ones((3, 3), np.uint8)
edges_combined = cv2.morphologyEx(edges_combined, cv2.MORPH_CLOSE, kernel, iterations=1)
object_size = min(w, h)
object_type = 'large' if object_size > 200 else 'small' if object_size < 50 else None
keypoints, _ = self.extract_adaptive_features(roi, object_type=object_type)
kp_mask = np.zeros_like(edges_combined)
for kp in keypoints:
x_kp, y_kp = int(kp.pt[0]), int(kp.pt[1])
if 0 <= x_kp < kp_mask.shape[1] and 0 <= y_kp < kp_mask.shape[0]:
cv2.circle(kp_mask, (x_kp, y_kp), 3, 255, -1)
combined_map = cv2.addWeighted(edges_combined, 0.7, kp_mask, 0.3, 0)
contours, _ = cv2.findContours(combined_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
if not contours:
return initial_bbox
center_x, center_y = roi_w // 2, roi_h // 2
rel_x, rel_y = search_margin, search_margin
best_contour = None
best_score = float('-inf')
min_area = max(100, w * h * 0.2)
max_area = w * h * 3.0
for cnt in contours:
area = cv2.contourArea(cnt)
if area < min_area or area > max_area:
continue
M = cv2.moments(cnt)
if M['m00'] == 0:
continue
cx = int(M['m10'] / M['m00'])
cy = int(M['m01'] / M['m00'])
dist_to_center = np.sqrt((cx - rel_x)**2 + (cy - rel_y)**2)
normalized_dist = dist_to_center / max(1, min(w, h))
x_cnt, y_cnt, w_cnt, h_cnt = cv2.boundingRect(cnt)
original_aspect = w / max(1, h)
new_aspect = w_cnt / max(1, h_cnt)
aspect_similarity = min(original_aspect/max(0.1, new_aspect), new_aspect/max(0.1, original_aspect))
size_similarity = min(area/(w*h), (w*h)/area)
score = (
size_similarity * 0.4 +
aspect_similarity * 0.3 +
max(0, 1 - normalized_dist) * 0.3
)
if score > best_score:
best_score = score
best_contour = cnt
if best_contour is not None and best_score > 0.5:
x_cnt, y_cnt, w_cnt, h_cnt = cv2.boundingRect(best_contour)
return (roi_x + x_cnt, roi_y + y_cnt, w_cnt, h_cnt)
if keypoints:
pts = np.array([kp.pt for kp in keypoints])
min_x = np.percentile(pts[:, 0], 5)
min_y = np.percentile(pts[:, 1], 5)
max_x = np.percentile(pts[:, 0], 95)
max_y = np.percentile(pts[:, 1], 95)
center_original_x = search_margin
center_original_y = search_margin
center_feature_x = (min_x + max_x) / 2
center_feature_y = (min_y + max_y) / 2
offset_x = 0.3 * (center_feature_x - center_original_x)
offset_y = 0.3 * (center_feature_y - center_original_y)
width_feature = max_x - min_x
height_feature = max_y - min_y
return (
int(x + offset_x),
int(y + offset_y),
int(0.7 * w + 0.3 * width_feature),
int(0.7 * h + 0.3 * height_feature)
)
return initial_bbox
def update_features(self, frame, bbox):
x, y, w, h = map(int, bbox)
# Ensure ROI stays within frame bounds
x = max(0, min(x, frame.shape[1] - 1))
y = max(0, min(y, frame.shape[0] - 1))
w = min(w, frame.shape[1] - x)
h = min(h, frame.shape[0] - y)
roi = frame[y:y+h, x:x+w]
# Relaxed size check for hypersonic objects
if roi.shape[0] <= 2 or roi.shape[1] <= 2:  # Reduced from 5
return False
keypoints, descriptors = self.extract_adaptive_features(
roi,
object_type='large' if max(w, h) > 200 else 'small' if max(w, h) < 50 else None
)
if descriptors is not None and len(descriptors) > 0:
self.feature_memory.append(descriptors)
else:
return False
if self.template is None or max(w, h) > 200:
if len(roi.shape) == 3:
self.template = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
else:
self.template = roi.copy()
self.template_size = (w, h)
hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV) if len(roi.shape) == 3 else roi
hist = cv2.calcHist([hsv_roi], channels=[0, 1] if len(roi.shape) == 3 else [0], mask=None,
histSize=[50, 60] if len(roi.shape) == 3 else [50],
ranges=[0, 180, 0, 256] if len(roi.shape) == 3 else [0, 256])
cv2.normalize(hist, hist)
self.color_memory.append(hist)
return True
def compute_similarity(self, frame, bbox):
if not self.feature_memory:
return 0.0
x, y, w, h = map(int, bbox)
roi = frame[y:y+h, x:x+w]
if roi.size == 0:
return 0.0
keypoints, descriptors = self.extract_adaptive_features(
roi,
object_type='large' if max(w, h) > 200 else 'small' if max(w, h) < 50 else None
)
orb_similarity = 0.0
if descriptors is not None and len(descriptors) > 0:
for stored_descriptors in self.feature_memory:
matches = self.matcher.match(descriptors, stored_descriptors)
if len(matches) > self.min_match_count:
distances = [m.distance for m in matches]
avg_distance = sum(distances) / len(distances)
similarity = 1.0 - (avg_distance / 100.0)
orb_similarity = max(orb_similarity, similarity)
if self.template is not None and max(w, h) > 200:
try:
if len(roi.shape) == 3:
gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
else:
gray_roi = roi.copy()
resized_template = cv2.resize(self.template, (w, h))
result = cv2.matchTemplate(gray_roi, resized_template, cv2.TM_CCOEFF_NORMED)
template_similarity = np.max(result)
orb_similarity = 0.7 * orb_similarity + 0.3 * template_similarity
except:
pass
if len(roi.shape) == 3:
hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
curr_hist = cv2.calcHist([hsv_roi], channels=[0, 1], mask=None,
histSize=[50, 60], ranges=[0, 180, 0, 256])
else:
curr_hist = cv2.calcHist([roi], channels=[0], mask=None,
histSize=[50], ranges=[0, 256])
cv2.normalize(curr_hist, curr_hist)
color_similarity = 0.0
if self.color_memory:
for stored_hist in self.color_memory:
sim = cv2.compareHist(curr_hist, stored_hist, cv2.HISTCMP_CORREL)
color_similarity = max(color_similarity, max(sim, 0.0))
combined_similarity = 0.6 * orb_similarity + 0.4 * color_similarity
return combined_similarity
class MotionModel:
def __init__(self, history_size=5):
self.position_history = deque(maxlen=history_size)
self.velocity_history = deque(maxlen=history_size-1)
self.max_acceleration_base = 10000  # Increased from 2000 for hypersonic
self.max_velocity_base = 7500       # Increased from 1500 for hypersonic
self.object_size = 1.0
self.covariance = np.eye(2) * 1e-2
def set_object_size(self, bbox):
w, h = bbox[2], bbox[3]
self.object_size = max(w, h) / 100.0
self.max_acceleration = self.max_acceleration_base * self.object_size
self.max_velocity = self.max_velocity_base * self.object_size
def update(self, bbox):
center_x = bbox[0] + bbox[2] / 2
center_y = bbox[1] + bbox[3] / 2
current_pos = np.array([center_x, center_y])
self.position_history.append(current_pos)
if len(self.position_history) >= 2:
velocity = self.position_history[-1] - self.position_history[-2]
self.velocity_history.append(velocity)
self.covariance = np.cov(np.array(self.position_history).T) + np.eye(2) * 1e-4
def check_motion_consistency(self, bbox):
if len(self.velocity_history) < 2:
return True
center_x = bbox[0] + bbox[2] / 2
center_y = bbox[1] + bbox[3] / 2
predicted_pos = self.position_history[-1] + self.velocity_history[-1]
actual_pos = np.array([center_x, center_y])
current_velocity = actual_pos - self.position_history[-1]
acceleration = current_velocity - self.velocity_history[-1]
# More permissive thresholds for hypersonic speeds
velocity_norm = np.linalg.norm(current_velocity)
if (np.linalg.norm(acceleration) > self.max_acceleration * 3.0 or
velocity_norm > self.max_velocity * 3.0):
return False
return True
def mahalanobis_distance(self, bbox):
center_x = bbox[0] + bbox[2] / 2
center_y = bbox[1] + bbox[3] / 2
actual_pos = np.array([center_x, center_y])
predicted_pos = self.position_history[-1] + self.velocity_history[-1] if self.velocity_history else actual_pos
diff = actual_pos - predicted_pos
inv_cov = np.linalg.inv(self.covariance + np.eye(2) * 1e-6)
return np.sqrt(diff.T @ inv_cov @ diff)
def predict_next_position(self):
if len(self.velocity_history) > 0:
# Extrapolate further for hypersonic speeds
return self.position_history[-1] + self.velocity_history[-1] * 2.0  # Double the prediction range
return self.position_history[-1] if self.position_history else None
class EnhancedKalmanBox:
def __init__(self, bbox):
self.kalman = cv2.KalmanFilter(10, 4)
dt = 1.0
self.kalman.transitionMatrix = np.array([
[1, 0, 0, 0, dt, 0, 0, 0, 0, 0],
[0, 1, 0, 0, 0, dt, 0, 0, 0, 0],
[0, 0, 1, 0, 0, 0, dt, 0, 0, 0],
[0, 0, 0, 1, 0, 0, 0, dt, 0, 0],
[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
], np.float32)
self.kalman.measurementMatrix = np.array([
[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
], np.float32)
process_noise = np.eye(10, dtype=np.float32) * 1e-3
process_noise[0:2, 0:2] *= 50
process_noise[4:6, 4:6] *= 20
process_noise[8, 8] *= 5
self.kalman.processNoiseCov = process_noise
self.kalman.measurementNoiseCov = np.eye(4, dtype=np.float32) * 1e-1
self.kalman.errorCovPost = np.eye(10, dtype=np.float32)
x, y, w, h = bbox
aspect_ratio = w / h if h != 0 else 1.0
self.kalman.statePre = np.array([[x], [y], [w], [h], [0], [0], [0], [0], [0], [aspect_ratio]], np.float32)
self.initial_aspect_ratio = aspect_ratio
self.max_aspect_ratio_change = 0.4
self.size_history = deque(maxlen=5)
self.initial_size = (w, h)
def predict(self):
prediction = self.kalman.predict()
return tuple(prediction[:4].flatten())
def update(self, bbox, confidence=1.0):
x, y, w, h = bbox
current_aspect_ratio = w / h if h != 0 else 1.0
aspect_ratio_change = abs(current_aspect_ratio - self.initial_aspect_ratio) / self.initial_aspect_ratio
growth_factor = max(0.1, min(0.5, 1.0 - confidence))
max_size_change = 0.7
if aspect_ratio_change > self.max_aspect_ratio_change:
prediction = self.predict()
alpha = 0.7
x = alpha * prediction[0] + (1 - alpha) * x
y = alpha * prediction[1] + (1 - alpha) * y
w = alpha * prediction[2] + (1 - alpha) * w
h = alpha * prediction[3] + (1 - alpha) * h
self.size_history.append((w, h))
if len(self.size_history) > 3:
filtered_w = np.median([s[0] for s in self.size_history])
filtered_h = np.median([s[1] for s in self.size_history])
w_change = abs(w - self.initial_size[0]) / self.initial_size[0]
h_change = abs(h - self.initial_size[1]) / self.initial_size[1]
if max(w_change, h_change) > max_size_change:
w = filtered_w
h = filtered_h
measurement = np.array([[np.float32(x)], [np.float32(y)], [np.float32(w)], [np.float32(h)]])
self.kalman.correct(measurement)
return self.predict()
def get_covariance(self):
return self.kalman.errorCovPost[:2, :2]
class ConfidenceTracker:
def __init__(self, confidence_threshold=0.6, lost_frames_threshold=60, det_thresh=0.3):  # Increased from 30
self.confidence_threshold = confidence_threshold
self.lost_frames_threshold = lost_frames_threshold
self.det_thresh = det_thresh
self.lost_frames_count = 0
self.appearance_model = AppearanceModel()
self.motion_model = MotionModel()
self.initial_size = None
self.max_size_change = 0.7
self.hit_streak = 0
self.time_since_update = 0
self.size_history = deque(maxlen=5)
def dlo_confidence_boost(self, bbox, tracker_confidence, kalman_bbox):
iou_score = iou(bbox, kalman_bbox)
mahalanobis_dist = self.motion_model.mahalanobis_distance(bbox)
shape_similarity = min(bbox[2] / (kalman_bbox[2] + 1e-6), kalman_bbox[2] / (bbox[2] + 1e-6)) * \
min(bbox[3] / (kalman_bbox[3] + 1e-6), kalman_bbox[3] / (bbox[3] + 1e-6))
if iou_score > 0.5 and mahalanobis_dist < 3.0 and shape_similarity > 0.7:
return min(1.0, tracker_confidence + 0.2 * (iou_score + shape_similarity))
return tracker_confidence
def duo_confidence_boost(self, bbox, tracker_confidence, kalman_bbox):
mahalanobis_dist = self.motion_model.mahalanobis_distance(bbox)
iou_score = iou(bbox, kalman_bbox)
if mahalanobis_dist > 5.0 or iou_score < 0.3:
return max(0.0, tracker_confidence - 0.3)
return tracker_confidence
def update(self, frame, bbox, tracker_confidence, kalman_bbox):
if self.initial_size is None:
self.initial_size = (bbox[2], bbox[3])
self.motion_model.set_object_size(bbox)
self.motion_model.update(bbox)
# Use motion prediction for hypersonic objects
predicted_pos = self.motion_model.predict_next_position()
if predicted_pos is not None:
center_x = bbox[0] + bbox[2]/2
center_y = bbox[1] + bbox[3]/2
pred_center_x, pred_center_y = predicted_pos
w, h = bbox[2], bbox[3]
# Larger expansion based on predicted motion
motion_dist = np.hypot(pred_center_x - center_x, pred_center_y - center_y)
expansion_factor = max(20, min(100, int(motion_dist * 0.5)))
adjusted_bbox = (
int(pred_center_x - w/2 - expansion_factor),
int(pred_center_y - h/2 - expansion_factor),
w + 2 * expansion_factor,
h + 2 * expansion_factor
)
# Ensure within frame bounds
adjusted_bbox = (
max(0, min(adjusted_bbox[0], frame.shape[1] - adjusted_bbox[2])),
max(0, min(adjusted_bbox[1], frame.shape[0] - adjusted_bbox[3])),
min(adjusted_bbox[2], frame.shape[1] - adjusted_bbox[0]),
min(adjusted_bbox[3], frame.shape[0] - adjusted_bbox[1])
)
else:
adjusted_bbox = bbox
success = self.appearance_model.update_features(frame, adjusted_bbox)
if not success:
logger.warning("Appearance model update failed, skipping frame")
# Larger fallback ROI for hypersonic objects
expanded_bbox = (
bbox[0] - 50, bbox[1] - 50,
bbox[2] + 100, bbox[3] + 100
)
expanded_bbox = (
max(0, min(expanded_bbox[0], frame.shape[1] - expanded_bbox[2])),
max(0, min(expanded_bbox[1], frame.shape[0] - expanded_bbox[3])),
min(expanded_bbox[2], frame.shape[1] - expanded_bbox[0]),
min(expanded_bbox[3], frame.shape[0] - expanded_bbox[1])
)
success = self.appearance_model.update_features(frame, expanded_bbox)
if not success:
self.time_since_update += 1
self.lost_frames_count += 1
return 0.0, self.lost_frames_count >= self.lost_frames_threshold
appearance_score = self.appearance_model.compute_similarity(frame, bbox)
motion_consistent = self.motion_model.check_motion_consistency(bbox)
size_consistent = self.check_size_consistency(bbox)
iou_score = iou(bbox, kalman_bbox)
mahalanobis_dist = self.motion_model.mahalanobis_distance(bbox)
shape_similarity = min(bbox[2] / (kalman_bbox[2] + 1e-6), kalman_bbox[2] / (bbox[2] + 1e-6)) * \
min(bbox[3] / (kalman_bbox[3] + 1e-6), kalman_bbox[3] / (bbox[3] + 1e-6))
boosted_confidence = self.dlo_confidence_boost(bbox, tracker_confidence, kalman_bbox)
final_confidence = self.duo_confidence_boost(bbox, boosted_confidence, kalman_bbox)
combined_confidence = (
0.3 * final_confidence +
0.25 * appearance_score +
0.2 * iou_score +
0.15 * (1.0 - min(mahalanobis_dist / 5.0, 1.0)) +
0.1 * shape_similarity
)
if combined_confidence >= self.confidence_threshold:
self.hit_streak += 1
self.time_since_update = 0
self.lost_frames_count = max(0, self.lost_frames_count - 1)
else:
self.time_since_update += 1
self.lost_frames_count += 1
self.hit_streak = max(0, self.hit_streak - 1)
return combined_confidence, self.lost_frames_count >= self.lost_frames_threshold
def check_size_consistency(self, bbox):
if self.initial_size is None:
return True
current_size = (bbox[2], bbox[3])
if self.initial_size[0] == 0 or self.initial_size[1] == 0:
self.initial_size = current_size
return True
self.size_history.append(current_size)
if len(self.size_history) > 3:
filtered_w = np.median([s[0] for s in self.size_history])
filtered_h = np.median([s[1] for s in self.size_history])
width_change = abs(filtered_w - self.initial_size[0]) / self.initial_size[0]
height_change = abs(filtered_h - self.initial_size[1]) / self.initial_size[1]
else:
width_change = abs(current_size[0] - self.initial_size[0]) / self.initial_size[0]
height_change = abs(current_size[1] - self.initial_size[1]) / self.initial_size[1]
return max(width_change, height_change) <= self.max_size_change
class GlobalVars:
def __init__(self):
self.init_rect = None
self.frame = None
self.tracking_started = False
self.paused = False
self.click_point = None
self.prev_gray = None
self.kalman_initialized = False
self.kalman_box = None
self.mode = "selection"
self.frame_idx = 0
self.total_frames = 0
self.new_fps = 0
self.cursor_pos = None
self.last_mouse_move = 0
self.tracked_object = None
self.dragging = False
self.drag_start = None
self.drag_end = None
self.message_display = ""
g = GlobalVars()
def iou(boxA, boxB):
xA = max(boxA[0], boxB[0])
yA = max(boxA[1], boxB[1])
xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])
interArea = max(0, xB - xA) * max(0, yB - yA)
boxAArea = boxA[2] * boxA[3]
boxBArea = boxB[2] * boxB[3]
denom = boxAArea + boxBArea - interArea
return interArea / float(denom) if denom > 0 else 0.0
class TemporalEvidenceAccumulator:
def __init__(self, evidence_threshold=3, position_tolerance_ratio=0.2, convergence_threshold=3,
iou_threshold=0.6, max_age=30, min_hits=3):
self.evidence_threshold = evidence_threshold
self.position_tolerance_ratio = position_tolerance_ratio
self.convergence_threshold = convergence_threshold
self.iou_threshold = iou_threshold
self.max_age = max_age
self.min_hits = min_hits
self.candidate_buffer = deque()
self.missed_count = 0
self.last_refined_bbox = None
self.hit_streak = 0
self.time_since_update = 0
def reset(self):
self.candidate_buffer.clear()
self.missed_count = 0
self.last_refined_bbox = None
self.hit_streak = 0
self.time_since_update = 0
def update(self, bbox, frame_index):
if bbox is None:
self.missed_count += 1
self.time_since_update += 1
if self.time_since_update >= self.max_age:
self.reset()
return (None, False)
else:
self.missed_count = 0
self.hit_streak += 1
self.time_since_update = 0
bbox = tuple(map(int, bbox))
if self.last_refined_bbox is not None:
lx, ly, lw, lh = self.last_refined_bbox
if (bbox[0] >= lx and bbox[1] >= ly and
bbox[0] + bbox[2] <= lx + lw and
bbox[1] + bbox[3] <= ly + lh):
self.candidate_buffer.append((frame_index, bbox))
else:
if self.candidate_buffer:
_, last_bbox = self.candidate_buffer[-1]
if iou(last_bbox, bbox) < self.iou_threshold:
self.reset()
self.candidate_buffer.append((frame_index, bbox))
else:
if self.candidate_buffer:
_, last_bbox = self.candidate_buffer[-1]
if iou(last_bbox, bbox) < self.iou_threshold:
self.reset()
self.candidate_buffer.append((frame_index, bbox))
if len(self.candidate_buffer) < self.evidence_threshold or self.hit_streak < self.min_hits:
return (None, False)
centers = []
widths = []
heights = []
for _, b in self.candidate_buffer:
cx = b[0] + b[2] / 2.0
cy = b[1] + b[3] / 2.0
centers.append([cx, cy])
widths.append(b[2])
heights.append(b[3])
centers = np.array(centers)
median_center = np.median(centers, axis=0)
median_width = np.median(widths)
median_height = np.median(heights)
if np.max(np.std(centers, axis=0)) > self.convergence_threshold:
return (None, False)
min_width = min(widths)
min_height = min(heights)
if (median_width <= min_width * (1 + self.position_tolerance_ratio) and
median_height <= min_height * (1 + self.position_tolerance_ratio)):
refined_x = int(median_center[0] - median_width / 2.0)
refined_y = int(median_center[1] - median_height / 2.0)
refined_bbox = (refined_x, refined_y, int(median_width), int(median_height))
self.last_refined_bbox = refined_bbox
self.candidate_buffer.clear()
return (refined_bbox, True)
return (None, False)
def get_adaptive_bbox(frame, x, y):
temp_appearance_model = AppearanceModel()
window_size = 240
h, w = frame.shape[:2]
x1 = max(0, x - window_size // 2)
y1 = max(0, y - window_size // 2)
x2 = min(w, x + window_size // 2)
y2 = min(h, y + window_size // 2)
initial_bbox = (x1, y1, x2 - x1, y2 - y1)
return temp_appearance_model.get_precise_object_boundaries(frame, initial_bbox)
def mouse_callback(event, x, y, flags, param):
if event == cv2.EVENT_LBUTTONDOWN:
g.dragging = True
g.drag_start = (x, y)
g.drag_end = (x, y)
elif event == cv2.EVENT_MOUSEMOVE:
if g.dragging and (flags & cv2.EVENT_FLAG_LBUTTON):
g.drag_end = (x, y)
x1 = min(g.drag_start[0], g.drag_end[0])
y1 = min(g.drag_start[1], g.drag_end[1])
x2 = max(g.drag_start[0], g.drag_end[0])
y2 = max(g.drag_start[1], g.drag_end[1])
g.init_rect = (x1, y1, x2 - x1, y2 - y1)
else:
g.cursor_pos = (x, y)
g.last_mouse_move = time.time()
elif event == cv2.EVENT_LBUTTONUP:
if g.dragging:
g.dragging = False
g.drag_end = (x, y)
x1 = min(g.drag_start[0], g.drag_end[0])
y1 = min(g.drag_start[1], g.drag_end[1])
x2 = max(g.drag_start[0], g.drag_end[0])
y2 = max(g.drag_start[1], g.drag_end[1])
g.init_rect = (x1, y1, x2 - x1, y2 - y1)
g.click_point = ((x1 + x2) // 2, (y1 + y2) // 2)
g.message_display = "Object selected via drag-and-drop. Press 'c' to track."
elif event == cv2.EVENT_LBUTTONDBLCLK:
if g.frame is None:
return
try:
g.tracking_started = False
g.kalman_initialized = False
g.mode = "selection"
adaptive_box = get_adaptive_bbox(g.frame, x, y)
g.init_rect = adaptive_box
g.click_point = (x, y)
g.message_display = "Object selected adaptively. Press 'c' to start tracking."
except Exception as e:
logger.error("Error in mouse callback: %s", e)
g.init_rect = None
g.click_point = None
def optical_flow_update(prev_frame, current_frame, bbox):
x, y, w, h = map(int, bbox)
if x < 0 or y < 0 or w <= 0 or h <= 0 or x + w > prev_frame.shape[1] or y + h > prev_frame.shape[0]:
return (0.0, 0.0)
try:
prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY) if len(prev_frame.shape) == 3 else prev_frame.copy()
curr_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY) if len(current_frame.shape) == 3 else current_frame.copy()
roi = prev_gray[y:y+h, x:x+w]
max_corners = max(15, min(500, int(10 * math.log2(1 + (w * h) / 100))))
p0 = cv2.goodFeaturesToTrack(roi, maxCorners=max_corners, qualityLevel=0.005, minDistance=5, blockSize=7,
useHarrisDetector=True, k=0.04)
if p0 is None:
return (0.0, 0.0)
p0 = np.array(p0, dtype=np.float32).reshape(-1, 1, 2)
p0[:, 0, 0] += x
p0[:, 0, 1] += y
# Larger window and more pyramid levels for hypersonic motion
p1, st, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, p0, None, winSize=(51, 51), maxLevel=7,
criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 0.01))
if p1 is None or len(p1[st.flatten() == 1]) == 0:
return (0.0, 0.0)
good_new = p1[st.flatten() == 1].reshape(-1, 2)
good_old = p0[st.flatten() == 1].reshape(-1, 2)
displacement = np.mean(good_new - good_old, axis=0)
dx, dy = float(displacement[0]), float(displacement[1])
object_diagonal = np.sqrt(w*w + h*h)
size_factor = 1.0 / (1.0 + 0.02 * min(w, h))
max_shift = object_diagonal * (0.2 + 0.3 * size_factor)  # Increased max shift
shift_magnitude = np.sqrt(dx*dx + dy*dy)
if shift_magnitude > max_shift:
dampening = max_shift / shift_magnitude
logger.warning(f"Optical flow shift too high ({shift_magnitude:.1f} > {max_shift:.1f}), dampening")
return (dx * dampening, dy * dampening)
return (dx, dy)
except Exception as e:
logger.error(f"Error in optical flow calculation: %s", e)
return (0.0, 0.0)
def universal_redetection(frame, last_bbox, last_frame=None):
x, y, w, h = map(int, last_bbox)
if w <= 0 or h <= 0:
return last_bbox
if last_frame is not None:
search_w = min(frame.shape[1], w * 3)
search_h = min(frame.shape[0], h * 3)
search_x = max(0, x - (search_w - w)//2)
search_y = max(0, y - (search_h - h)//2)
template = last_frame[y:y+h, x:x+w]
search_area = frame[
search_y:min(search_y+search_h, frame.shape[0]),
search_x:min(search_x+search_w, frame.shape[1])
]
if search_area.size > 0 and template.size > 0:
if len(search_area.shape) == 3:
search_gray = cv2.cvtColor(search_area, cv2.COLOR_BGR2GRAY)
template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)
else:
search_gray = search_area
template_gray = template
res = cv2.matchTemplate(search_gray, template_gray, cv2.TM_CCOEFF_NORMED)
_, max_val, _, max_loc = cv2.minMaxLoc(res)
if max_val > 0.5:
new_x = search_x + max_loc[0]
new_y = search_y + max_loc[1]
return (new_x, new_y, w, h)
return last_bbox
def main():
parser = argparse.ArgumentParser(description='Single-Object Tracking System')
parser.add_argument('--config', type=str, required=True, help='Config file for SiamRPN')
parser.add_argument('--snapshot', type=str, required=True, help='Model snapshot for SiamRPN')
parser.add_argument('--video_name', type=str, required=True, help='Video file path')
args = parser.parse_args()
cfg.merge_from_file(args.config)
cfg.CUDA = torch.cuda.is_available()
device = torch.device('cuda' if cfg.CUDA else 'cpu')
try:
model = ModelBuilder()
model.load_state_dict(torch.load(args.snapshot, map_location=lambda storage, loc: storage.cpu(), weights_only=True))
model.eval().to(device)
logger.info("PySOT model initialized successfully")
except Exception as e:
logger.error("Error initializing PySOT model: %s", e)
return
cap = cv2.VideoCapture(args.video_name)
if not cap.isOpened():
logger.error("Error: Could not open video")
return
g.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
orig_fps = cap.get(cv2.CAP_PROP_FPS) or 25
g.new_fps = min(120, 4 * orig_fps)  # Aim for higher FPS for hypersonic tracking
cv2.namedWindow("Tracking")
cv2.setMouseCallback("Tracking", mouse_callback)
logger.info("\nControls:")
logger.info("Drag and Drop or Double Click: Select tracking object (adaptive bounding box)")
logger.info("'c' key: Start tracking selected object")
logger.info("Space: Pause/Resume")
logger.info("'r' key: Reset tracking")
logger.info("q: Quit")
prev_time = time.time()
prev_frame = None
while True:
current_time = time.time()
fps = 1 / (current_time - prev_time) if (current_time - prev_time) > 0 else 0
prev_time = current_time
current_mode = g.mode
paused = g.paused
if current_mode == "selection":
ret, frame = cap.read()
if not ret:
logger.info("End of video reached.")
break
g.frame_idx += 1
g.frame = frame.copy()
progress = (g.frame_idx / g.total_frames) * 100 if g.total_frames > 0 else 0
display_frame = frame.copy()
cv2.putText(display_frame, f"Frame: {g.frame_idx}/{g.total_frames} ({progress:.2f}%)",
(10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
cv2.putText(display_frame, f"FPS: {fps:.2f}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
if g.init_rect is not None:
x, y, w, h = g.init_rect
cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
if g.click_point:
cv2.circle(display_frame, g.click_point, 5, (0, 255, 0), -1)
if g.dragging and g.drag_start and g.drag_end:
x1 = min(g.drag_start[0], g.drag_end[0])
y1 = min(g.drag_start[1], g.drag_end[1])
x2 = max(g.drag_start[0], g.drag_end[0])
y2 = max(g.drag_start[1], g.drag_end[1])
cv2.rectangle(display_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)
if g.message_display:
cv2.putText(display_frame, g.message_display, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
cv2.imshow("Tracking", display_frame)
key = cv2.waitKey(20)
if key & 0xFF == ord('q'):
break
elif key & 0xFF == ord(' '):
g.paused = not g.paused
elif key & 0xFF == ord('r'):
g.tracked_object = None
g.tracking_started = False
g.init_rect = None
g.click_point = None
g.kalman_initialized = False
g.message_display = ""
elif key & 0xFF == ord('c'):
if g.init_rect is not None:
box = g.init_rect
elif g.click_point is not None:
x_center, y_center = g.click_point
box = get_adaptive_bbox(frame, x_center, y_center)
else:
box = None
if box is not None:
tracker = build_tracker(model)
tracker.init(frame, box)
kalman = EnhancedKalmanBox(box)
g.tracked_object = {
'tracker': tracker,
'init_rect': box,
'kalman_box': box,
'confidence_tracker': ConfidenceTracker(),
'color': (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)),
'prev_center': (box[0] + box[2] // 2, box[1] + box[3] // 2),
'accumulator': TemporalEvidenceAccumulator(),
'kalman': kalman,
'prev_frame': None
}
g.mode = "tracking"
g.init_rect = None
g.click_point = None
g.message_display = "Object tracking started."
else:
logger.warning("No selection made. Please select an object before pressing 'c'.")
prev_frame = frame.copy()
elif current_mode == "tracking":
if paused:
cv2.waitKey(1)
continue
ret, frame = cap.read()
if not ret:
break
g.frame_idx += 1
g.frame = frame.copy()
display_frame = frame.copy()
if g.tracked_object:
tracker = g.tracked_object['tracker']
outputs = tracker.track(frame)
bbox = outputs['bbox']
accumulator = g.tracked_object['accumulator']
refined_bbox, confirmed = accumulator.update(bbox, g.frame_idx)
if confirmed:
bbox = refined_bbox
if prev_frame is not None:
dx, dy = optical_flow_update(prev_frame, frame, g.tracked_object['kalman_box'])
x, y, w, h = g.tracked_object['kalman_box']
new_box = (int(x + dx), int(y + dy), w, h)
new_center = (new_box[0] + new_box[2] // 2, new_box[1] + new_box[3] // 2)
g.tracked_object['kalman_box'] = new_box
g.tracked_object['prev_center'] = new_center
tracker_confidence = outputs.get('score', 0.8)
x1, y1, w1, h1 = g.tracked_object['kalman_box']
x2, y2, w2, h2 = bbox
kalman_bbox = g.tracked_object['kalman'].update((x2, y2, w2, h2), tracker_confidence)
tracker_weight = max(0.3, min(0.7, tracker_confidence))
size_consistency = min(w1/w2, w2/w1) * min(h1/h2, h2/h1) if w1*w2*h1*h2 > 0 else 0
size_weight = 0.8 if size_consistency < 0.7 else 0.7
fused_box = (
int(tracker_weight * x2 + (1-tracker_weight) * kalman_bbox[0]),
int(tracker_weight * y2 + (1-tracker_weight) * kalman_bbox[1]),
int(size_weight * w2 + (1-size_weight) * kalman_bbox[2]),
int(size_weight * h2 + (1-size_weight) * kalman_bbox[3])
)
new_center = (fused_box[0] + fused_box[2] // 2, fused_box[1] + fused_box[3] // 2)
prev_center = g.tracked_object['prev_center']
object_size = max(fused_box[2], fused_box[3]) / 100.0
jump_threshold = JUMP_THRESHOLD_BASE * object_size
jump_distance = np.hypot(new_center[0] - prev_center[0], new_center[1] - prev_center[1])
if jump_distance > jump_threshold:
jump_factor = jump_threshold / (jump_distance + 1e-6)
new_center_x = prev_center[0] + (new_center[0] - prev_center[0]) * jump_factor
new_center_y = prev_center[1] + (new_center[1] - prev_center[1]) * jump_factor
g.tracked_object['kalman_box'] = (
int(new_center_x - fused_box[2]/2),
int(new_center_y - fused_box[3]/2),
fused_box[2], fused_box[3]
)
g.tracked_object['prev_center'] = (new_center_x, new_center_y)
else:
g.tracked_object['kalman_box'] = fused_box
g.tracked_object['prev_center'] = new_center
conf, lost = g.tracked_object['confidence_tracker'].update(
frame, g.tracked_object['kalman_box'], tracker_confidence, kalman_bbox
)
if conf < 0.6:
redetected_box = universal_redetection(
frame,
g.tracked_object['kalman_box'],
g.tracked_object['prev_frame']
)
blend = max(0.3, min(0.7, conf))
g.tracked_object['kalman_box'] = (
int(blend * g.tracked_object['kalman_box'][0] + (1-blend) * redetected_box[0]),
int(blend * g.tracked_object['kalman_box'][1] + (1-blend) * redetected_box[1]),
int(blend * g.tracked_object['kalman_box'][2] + (1-blend) * redetected_box[2]),
int(blend * g.tracked_object['kalman_box'][3] + (1-blend) * redetected_box[3])
)
if lost or g.tracked_object['confidence_tracker'].time_since_update > 30:
logger.info("Lost track of object; removing from tracking list")
g.tracked_object = None
g.mode = "selection"
continue
x, y, w, h = g.tracked_object['kalman_box']
cv2.rectangle(display_frame, (x, y), (x+w, y+h), g.tracked_object['color'], 2)
g.tracked_object['prev_frame'] = frame.copy()
cv2.putText(display_frame, f"FPS: {fps:.2f}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
if g.message_display:
cv2.putText(display_frame, g.message_display, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
cv2.imshow("Tracking", display_frame)
key = cv2.waitKey(1)
if key & 0xFF == ord('q'):
break
elif key & 0xFF == ord('r'):
g.tracked_object = None
g.tracking_started = False
g.mode = "selection"
elif key & 0xFF == ord(' '):
g.paused = not g.paused
prev_frame = frame.copy()
cap.release()
cv2.destroyAllWindows()
if __name__ == "__main__":
main()
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals
import os
import cv2
import torch
import numpy as np
from glob import glob
import time
from pysot.core.config import cfg
from pysot.models.model_builder import ModelBuilder
from pysot.tracker.tracker_builder import build_tracker
torch.set_num_threads(1)
# Hardcoded paths
CONFIG_PATH = r"D:\TestingPysot\experiments\siamrpn_mobilev2_l234_dwxcorr\config.yaml"
SNAPSHOT_PATH = r"D:\TestingPysot\experiments\siamrpn_mobilev2_l234_dwxcorr\model.pth"
# VIDEO_PATH = r"D:\TestingPysot\demo\M16.avi"
VIDEO_PATH = 0
# Global variables for mouse callback
drawing = False
start_point = None
end_point = None
new_box = None
tracking_initialized = False
def mouse_callback(event, x, y, flags, param):
global drawing, start_point, end_point, new_box
if event == cv2.EVENT_LBUTTONDOWN:
# Start drawing the rectangle
drawing = True
start_point = (x, y)
end_point = None
elif event == cv2.EVENT_MOUSEMOVE:
# Update the rectangle as the mouse moves
if drawing:
end_point = (x, y)
elif event == cv2.EVENT_LBUTTONUP:
# Finalize the rectangle
drawing = False
if start_point and end_point:
x1, y1 = start_point
x2, y2 = end_point
new_box = [min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)]
if __name__ == '__main__':
# load config
cfg.merge_from_file(CONFIG_PATH)
cfg.CUDA = torch.cuda.is_available() and cfg.CUDA
device = torch.device('cuda' if cfg.CUDA else 'cpu')
# create model
model = ModelBuilder()
# load model weights
try:
model.load_state_dict(torch.load(SNAPSHOT_PATH,
map_location=lambda storage, loc: storage.cpu(),
weights_only=True))
except TypeError:
# For older PyTorch versions that don't support weights_only
model.load_state_dict(torch.load(SNAPSHOT_PATH,
map_location=lambda storage, loc: storage.cpu()))
model.eval().to(device)
# build tracker
tracker = build_tracker(model)
# Initialize video capture
cap = cv2.VideoCapture(VIDEO_PATH)
if not cap.isOpened():
print(f"Error: Could not open video {VIDEO_PATH}")
exit()
if VIDEO_PATH:
video_name = os.path.basename(VIDEO_PATH).split('.')[0]
else:
video_name = 'webcam'
# Create window and set mouse callback
cv2.namedWindow(video_name)
cv2.setMouseCallback(video_name, mouse_callback)
frame_count = 0
frame_skip = 2
# Main video loop
print("Video is starting. Click and drag to select an object to track.")
while True:
# Get new frame
ret, frame = cap.read()
if not ret:
print("Video ended")
break
# Create a copy for display
display_frame = frame.copy()
# Check for new box selection
if new_box is not None:
# Convert to format expected by tracker (x, y, w, h)
x1, y1, x2, y2 = new_box
init_rect = (x1, y1, x2 - x1, y2 - y1)
# Re-initialize the tracker with the new selection
tracker.init(frame, init_rect)
tracking_initialized = True
new_box = None
print("New object selected for tracking")
# Draw rectangle during selection
if drawing and start_point and end_point:
cv2.rectangle(display_frame, start_point, end_point, (0, 255, 255), 2)
# If tracking is initialized, track the object
if tracking_initialized:
# Perform tracking
outputs = tracker.track(frame)
# Draw the tracking results
if 'polygon' in outputs:
polygon = np.array(outputs['polygon']).astype(np.int32)
cv2.polylines(display_frame, [polygon.reshape((-1, 1, 2))], True, (0, 255, 0), 3)
mask = ((outputs['mask'] > cfg.TRACK.MASK_THERSHOLD) * 255)
mask = mask.astype(np.uint8)
mask = np.stack([mask, mask * 255, mask]).transpose(1, 2, 0)
display_frame = cv2.addWeighted(display_frame, 0.77, mask, 0.23, -1)
else:
bbox = list(map(int, outputs['bbox']))
cv2.rectangle(display_frame, (bbox[0], bbox[1]),
(bbox[0] + bbox[2], bbox[1] + bbox[3]),
(0, 255, 0), 3)
# Show status message
cv2.putText(display_frame, "Tracking active - Click and drag for new object", (10, 30),
cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
else:
# Show selection instructions
cv2.putText(display_frame, "Click and drag to select an object to track", (10, 30),
cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
# Display the current frame
cv2.imshow(video_name, display_frame)
# Check for user input
key = cv2.waitKey(1) & 0xFF
if key == 27 or key == ord('q'):  # Press 'Esc' or 'q' to quit
print("Exiting...")
break
elif key == ord('c'):  # Press 'c' to cancel tracking
tracking_initialized = False
print("Tracking canceled. Select a new object.")
frame_count += 1
cap.release()
cv2.destroyAllWindows()
from __future__ import absolute_import, division, print_function, unicode_literals
import cv2
import torch
import numpy as np
from pysot.core.config import cfg
from pysot.models.model_builder import ModelBuilder
from pysot.tracker.tracker_builder import build_tracker
import logging
from collections import deque
import random
import math
import time
# Hardcoded configuration paths
CONFIG_PATH = r"D:\TestingPysot\experiments\siamrpn_mobilev2_l234_dwxcorr\config.yaml"
SNAPSHOT_PATH = r"D:\TestingPysot\experiments\siamrpn_mobilev2_l234_dwxcorr\model.pth"
DEFAULT_VIDEO_PATH = r"D:\TestingPysot\demo\M16.avi"
# DEFAULT_VIDEO_PATH= 0
# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
JUMP_THRESHOLD_BASE = 500
torch.backends.cudnn.benchmark = True
class AppearanceModel:
"""Handles feature extraction and object appearance tracking"""
def __init__(self, feature_memory_size=5, spectrum_type=None):
self.feature_memory = deque(maxlen=feature_memory_size)
self.color_memory = deque(maxlen=feature_memory_size)
self.min_match_count = 5
self.original_template = None
self.template = None
self.template_size = None
self.alpha = 0.95
self.spectrum_type = spectrum_type
self._initialize_feature_extractor(spectrum_type)
def _initialize_feature_extractor(self, spectrum_type=None):
params = {'nfeatures': 2000, 'scaleFactor': 1.15, 'nlevels': 10,
'edgeThreshold': 35, 'patchSize': 37, 'fastThreshold': 15}
if spectrum_type == "MWIR":
params.update({'fastThreshold': 12, 'edgeThreshold': 31, 'nlevels': 12})
elif spectrum_type == "SWIR":
params.update({'fastThreshold': 17, 'scaleFactor': 1.2, 'patchSize': 33})
elif spectrum_type == "LWIR":
params.update({'fastThreshold': 10, 'nfeatures': 2500, 'edgeThreshold': 41, 'patchSize': 41})
elif spectrum_type == "FLIR":
params.update({'fastThreshold': 13, 'edgeThreshold': 33, 'nfeatures': 2200})
elif spectrum_type == "VIS":
params.update({'fastThreshold': 20, 'edgeThreshold': 31, 'patchSize': 31, 'nfeatures': 1500})
self.feature_extractor = cv2.ORB_create(**params)
self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
def extract_adaptive_features(self, frame, bbox=None, object_type=None):
try:
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame.copy()
if gray.size == 0:
return [], None
mean_val, std_val = np.mean(gray), np.std(gray)
if std_val < 30:
clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
gray = clahe.apply(gray)
sigma_color = 20 if std_val < 40 else 10
sigma_space = 10 if mean_val < 100 else 7
gray = cv2.bilateralFilter(gray, 9, sigma_color, sigma_space)
original_nfeatures = self.feature_extractor.getMaxFeatures()
if object_type:
if object_type == 'large':
self.feature_extractor.setMaxFeatures(int(original_nfeatures * 1.5))
elif object_type == 'small':
self.feature_extractor.setMaxFeatures(int(original_nfeatures * 0.8))
patch_size = self.feature_extractor.getPatchSize()
self.feature_extractor.setPatchSize(max(15, patch_size - 10))
elif object_type == 'flat':
self.feature_extractor.setMaxFeatures(int(original_nfeatures * 1.2))
fast_thresh = self.feature_extractor.getFastThreshold()
self.feature_extractor.setFastThreshold(fast_thresh + 5)
if bbox:
x, y, w, h = map(int, bbox)
margin = int(min(w, h) * 0.2)
roi = gray[max(0, y-margin):min(gray.shape[0], y+h+margin),
max(0, x-margin):min(gray.shape[1], x+w+margin)]
if roi.size == 0:
return [], None
keypoints = self.feature_extractor.detect(roi, None)
for kp in keypoints:
kp.pt = (kp.pt[0] + max(0, x-margin), kp.pt[1] + max(0, y-margin))
else:
keypoints = self.feature_extractor.detect(gray, None)
keypoints, descriptors = self.feature_extractor.compute(gray, keypoints)
if object_type:
self.feature_extractor.setMaxFeatures(original_nfeatures)
if object_type == 'small':
self.feature_extractor.setPatchSize(patch_size)
elif object_type == 'flat':
self.feature_extractor.setFastThreshold(fast_thresh)
return keypoints, descriptors
except Exception as e:
logger.error(f"Error in feature extraction: {e}")
return [], None
def get_precise_object_boundaries(self, frame, initial_bbox, spectrum_type=None):
try:
x, y, w, h = map(int, initial_bbox)
search_margin = max(int(min(w, h) * 0.3), 30)
roi = frame[max(0, y-search_margin):min(frame.shape[0], y+h+search_margin),
max(0, x-search_margin):min(frame.shape[1], x+w+search_margin)]
if roi.size == 0:
return initial_bbox
gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) == 3 else roi.copy()
mean_val, std_val = np.mean(gray_roi), np.std(gray_roi)
low_thresh_base = max(10, int(mean_val * 0.2))
high_thresh_base = max(30, int(mean_val * 0.5))
if spectrum_type in ["LWIR", "MWIR"]:
low_thresh_base *= 0.7
high_thresh_base *= 0.7
elif spectrum_type == "SWIR":
low_thresh_base *= 1.1
high_thresh_base *= 1.1
edges_combined = np.zeros_like(gray_roi)
for scale in [0.8, 1.0, 1.3]:
edges = cv2.Canny(gray_roi, int(low_thresh_base * scale), int(high_thresh_base * scale))
edges_combined = cv2.bitwise_or(edges_combined, edges)
edges_combined = cv2.morphologyEx(edges_combined, cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8), iterations=1)
object_type = 'large' if min(w, h) > 200 else 'small' if min(w, h) < 50 else None
keypoints, _ = self.extract_adaptive_features(roi, object_type=object_type)
kp_mask = np.zeros_like(edges_combined)
for kp in keypoints:
x_kp, y_kp = map(int, kp.pt)
if 0 <= x_kp < kp_mask.shape[1] and 0 <= y_kp < kp_mask.shape[0]:
cv2.circle(kp_mask, (x_kp, y_kp), 3, 255, -1)
combined_map = cv2.addWeighted(edges_combined, 0.7, kp_mask, 0.3, 0)
contours, _ = cv2.findContours(combined_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
if not contours:
return initial_bbox
best_contour, best_score = None, float('-inf')
min_area, max_area = max(100, w * h * 0.2), w * h * 3.0
for cnt in contours:
area = cv2.contourArea(cnt)
if not (min_area <= area <= max_area):
continue
M = cv2.moments(cnt)
if M['m00'] == 0:
continue
cx, cy = int(M['m10'] / M['m00']), int(M['m01'] / M['m00'])
dist_to_center = np.sqrt((cx - w//2)**2 + (cy - h//2)**2)
x_cnt, y_cnt, w_cnt, h_cnt = cv2.boundingRect(cnt)
score = (
min(area/(w*h), (w*h)/area) * 0.4 +
min(w/h if h else 1, w_cnt/h_cnt if h_cnt else 1) * 0.3 +
max(0, 1 - dist_to_center/max(1, min(w, h))) * 0.3
)
if score > best_score:
best_score = score
best_contour = cnt
if best_contour is not None and best_score > 0.3:
x_cnt, y_cnt, w_cnt, h_cnt = cv2.boundingRect(best_contour)
return (x + x_cnt - search_margin, y + y_cnt - search_margin, w_cnt, h_cnt)
if keypoints:
pts = np.array([kp.pt for kp in keypoints])
min_x, min_y = np.percentile(pts[:, 0], 5), np.percentile(pts[:, 1], 5)
max_x, max_y = np.percentile(pts[:, 0], 95), np.percentile(pts[:, 1], 95)
return (
int(x + 0.3 * ((min_x + max_x)/2 - w/2)),
int(y + 0.3 * ((min_y + max_y)/2 - h/2)),
int(0.7 * w + 0.3 * (max_x - min_x)),
int(0.7 * h + 0.3 * (max_y - min_y))
)
return initial_bbox
except Exception as e:
logger.error(f"Error in get_precise_object_boundaries: {e}")
return initial_bbox
def update_features(self, frame, bbox):
try:
x, y, w, h = map(int, bbox)
x, y = max(0, min(x, frame.shape[1] - 1)), max(0, min(y, frame.shape[0] - 1))
w, h = min(w, frame.shape[1] - x), min(h, frame.shape[0] - y)
if w <= 0 or h <= 0:
return False
roi = frame[y:y+h, x:x+w]
object_type = 'large' if max(w, h) > 200 else 'small' if max(w, h) < 50 else None
keypoints, descriptors = self.extract_adaptive_features(roi, object_type=object_type)
if descriptors is not None and len(descriptors) > 0 and object_type != 'small':
self.feature_memory.append(descriptors)
else:
return False
if self.original_template is None:
self.original_template = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) == 3 else roi.copy()
self.template = self.original_template.copy()
self.template_size = (w, h)
elif max(w, h) > 200:
self.template = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) == 3 else roi.copy()
self.template_size = (w, h)
hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV) if len(roi.shape) == 3 else roi
hist = cv2.calcHist([hsv_roi], [0, 1] if len(roi.shape) == 3 else [0], None,
[50, 60] if len(roi.shape) == 3 else [50],
[0, 180, 0, 256] if len(roi.shape) == 3 else [0, 256])
cv2.normalize(hist, hist)
if object_type != 'small':
self.color_memory.append(hist)
return True
except Exception as e:
logger.error(f"Error in update_features: {e}")
return False
def compute_similarity(self, frame, bbox):
try:
if not self.feature_memory and self.original_template is None:
return 0.0
x, y, w, h = map(int, bbox)
roi = frame[y:y+h, x:x+w]
if roi.size == 0:
return 0.0
object_type = 'large' if max(w, h) > 200 else 'small' if max(w, h) < 50 else None
keypoints, descriptors = self.extract_adaptive_features(roi, object_type=object_type)
orb_similarity = 0.0
if descriptors is not None and len(descriptors) > 0:
if object_type == 'small' and self.original_template is not None:
gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) == 3 else roi.copy()
resized_template = cv2.resize(self.original_template, (w, h))
result = cv2.matchTemplate(gray_roi, resized_template, cv2.TM_CCOEFF_NORMED)
orb_similarity = np.max(result) if result.size > 0 else 0.0
else:
for stored_descriptors in self.feature_memory:
matches = self.matcher.match(descriptors, stored_descriptors)
if len(matches) > self.min_match_count:
avg_distance = sum(m.distance for m in matches) / len(matches)
orb_similarity = max(orb_similarity, 1.0 - (avg_distance / 100.0))
if self.template is not None and max(w, h) > 200:
gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) == 3 else roi.copy()
resized_template = cv2.resize(self.template, (w, h))
result = cv2.matchTemplate(gray_roi, resized_template, cv2.TM_CCOEFF_NORMED)
orb_similarity = 0.7 * orb_similarity + 0.3 * np.max(result) if result.size > 0 else orb_similarity
hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV) if len(roi.shape) == 3 else roi
curr_hist = cv2.calcHist([hsv_roi], [0, 1] if len(roi.shape) == 3 else [0], None,
[50, 60] if len(roi.shape) == 3 else [50],
[0, 180, 0, 256] if len(roi.shape) == 3 else [0, 256])
cv2.normalize(curr_hist, curr_hist)
color_similarity = max(0.0, max(cv2.compareHist(curr_hist, hist, cv2.HISTCMP_CORREL)
for hist in self.color_memory)) if self.color_memory and object_type != 'small' else 0.0
uniqueness_score = 0.0
if object_type == 'small' and keypoints and self.original_template is not None:
keypoint_ratio = len(keypoints) / (w * h / 1000)
uniqueness_score = min(1.0, keypoint_ratio * 0.5 + np.std([kp.response for kp in keypoints]) / 100.0 * 0.5)
return (0.6 * orb_similarity + 0.4 * color_similarity) * (1.0 + uniqueness_score) if object_type == 'small' else (0.6 * orb_similarity + 0.4 * color_similarity)
except Exception as e:
logger.error(f"Error in compute_similarity: {e}")
return 0.0
class MotionModel:
"""Tracks object motion and predicts positions"""
def __init__(self, history_size=5):
self.position_history = deque(maxlen=history_size)
self.velocity_history = deque(maxlen=history_size-1)
self.max_acceleration_base = 10000
self.max_velocity_base = 7500
self.object_size = 1.0
self.covariance = np.eye(2) * 1e-2
def set_object_size(self, bbox):
w, h = bbox[2], bbox[3]
self.object_size = max(w, h) / 100.0
self.max_acceleration = self.max_acceleration_base * self.object_size * (0.5 if max(w, h) < 50 else 1.0)
self.max_velocity = self.max_velocity_base * self.object_size * (0.5 if max(w, h) < 50 else 1.0)
def update(self, bbox):
center_x, center_y = bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2
current_pos = np.array([center_x, center_y])
self.position_history.append(current_pos)
if len(self.position_history) >= 2:
velocity = self.position_history[-1] - self.position_history[-2]
self.velocity_history.append(velocity)
self.covariance = np.cov(np.array(self.position_history).T) + np.eye(2) * 1e-4
def check_motion_consistency(self, bbox):
if len(self.velocity_history) < 2:
return True
center_x, center_y = bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2
predicted_pos = self.position_history[-1] + self.velocity_history[-1]
current_velocity = np.array([center_x, center_y]) - self.position_history[-1]
acceleration = current_velocity - self.velocity_history[-1]
return not (np.linalg.norm(acceleration) > self.max_acceleration * 2.0 or
np.linalg.norm(current_velocity) > self.max_velocity * 2.0)
def mahalanobis_distance(self, bbox):
center_x, center_y = bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2
actual_pos = np.array([center_x, center_y])
predicted_pos = (self.position_history[-1] + self.velocity_history[-1]
if self.velocity_history else actual_pos)
diff = actual_pos - predicted_pos
inv_cov = np.linalg.inv(self.covariance + np.eye(2) * 1e-6)
return np.sqrt(diff.T @ inv_cov @ diff)
def predict_next_position(self):
return (self.position_history[-1] + self.velocity_history[-1] * 2.0
if self.velocity_history else self.position_history[-1]
if self.position_history else None)
class EnhancedKalmanBox:
"""Kalman filter for bounding box tracking"""
def __init__(self, bbox):
self.kalman = cv2.KalmanFilter(10, 4)
dt = 1.0
self.kalman.transitionMatrix = np.array([
[1, 0, 0, 0, dt, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, dt, 0, 0, 0, 0],
[0, 0, 1, 0, 0, 0, dt, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, dt, 0, 0],
[0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
[0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
], np.float32)
self.kalman.measurementMatrix = np.array([
[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
], np.float32)
process_noise = np.eye(10, dtype=np.float32) * 1e-3
process_noise[0:2, 0:2] *= 50; process_noise[4:6, 4:6] *= 20; process_noise[8, 8] *= 5
self.kalman.processNoiseCov = process_noise
self.kalman.measurementNoiseCov = np.eye(4, dtype=np.float32) * 1e-1
self.kalman.errorCovPost = np.eye(10, dtype=np.float32)
x, y, w, h = bbox
self.kalman.statePre = np.array([[x], [y], [w], [h], [0], [0], [0], [0], [0], [w/h if h != 0 else 1.0]], np.float32)
self.initial_aspect_ratio = w/h if h != 0 else 1.0
self.max_aspect_ratio_change = 0.6
self.size_history = deque(maxlen=5)
self.initial_size = (w, h)
def predict(self):
return tuple(self.kalman.predict()[:4].flatten())
def update(self, bbox, confidence=1.0):
try:
x, y, w, h = bbox
current_aspect_ratio = w/h if h != 0 else 1.0
aspect_change = abs(current_aspect_ratio - self.initial_aspect_ratio) / self.initial_aspect_ratio
if aspect_change > self.max_aspect_ratio_change:
prediction = self.predict()
alpha = 0.7
x, y, w, h = [alpha * pred + (1 - alpha) * val for pred, val in zip(prediction, [x, y, w, h])]
self.size_history.append((w, h))
if len(self.size_history) > 3:
filtered_w, filtered_h = np.median([s[0] for s in self.size_history]), np.median([s[1] for s in self.size_history])
w_change, h_change = abs(w - self.initial_size[0]) / self.initial_size[0], abs(h - self.initial_size[1]) / self.initial_size[1]
if max(w_change, h_change) > 0.9:
w, h = filtered_w, filtered_h
self.kalman.correct(np.array([[np.float32(x)], [np.float32(y)], [np.float32(w)], [np.float32(h)]]))
return self.predict()
except Exception as e:
logger.error(f"Error in Kalman update: {e}")
return bbox
class ConfidenceTracker:
"""Manages tracking confidence and object persistence"""
def __init__(self, confidence_threshold=0.5, lost_frames_threshold=60, det_thresh=0.2):
self.confidence_threshold = confidence_threshold
self.lost_frames_threshold = lost_frames_threshold
self.det_thresh = det_thresh
self.lost_frames_count = 0
self.appearance_model = AppearanceModel()
self.motion_model = MotionModel()
self.initial_size = None
self.max_size_change = 0.9
self.hit_streak = 0
self.time_since_update = 0
self.size_history = deque(maxlen=5)
self.initial_context = None
def _dlo_confidence_boost(self, bbox, tracker_confidence, kalman_bbox):
iou_score = iou(bbox, kalman_bbox)
mahalanobis_dist = self.motion_model.mahalanobis_distance(bbox)
shape_similarity = min(bbox[2] / (kalman_bbox[2] + 1e-6), kalman_bbox[2] / (bbox[2] + 1e-6)) * \
min(bbox[3] / (kalman_bbox[3] + 1e-6), kalman_bbox[3] / (bbox[3] + 1e-6))
if iou_score > 0.3 and mahalanobis_dist < 5.0 and shape_similarity > 0.5:
return min(1.0, tracker_confidence + 0.2 * (iou_score + shape_similarity))
return tracker_confidence
def _duo_confidence_boost(self, bbox, tracker_confidence, kalman_bbox):
mahalanobis_dist = self.motion_model.mahalanobis_distance(bbox)
iou_score = iou(bbox, kalman_bbox)
if mahalanobis_dist > 7.0 or iou_score < 0.2:
return max(0.0, tracker_confidence - 0.2)
return tracker_confidence
def _check_context_consistency(self, frame, bbox):
try:
if self.initial_context is None or max(bbox[2], bbox[3]) > 200:
x, y, w, h = map(int, bbox)
margin = int(min(w, h) * 0.5)
context_roi = frame[max(0, y-margin):min(frame.shape[0], y+h+margin),
max(0, x-margin):min(frame.shape[1], x+w+margin)]
if context_roi.size > 0:
self.initial_context = cv2.calcHist([cv2.cvtColor(context_roi, cv2.COLOR_BGR2HSV) if len(context_roi.shape) == 3 else context_roi],
[0, 1] if len(context_roi.shape) == 3 else [0], None,
[50, 60] if len(context_roi.shape) == 3 else [50],
[0, 180, 0, 256] if len(context_roi.shape) == 3 else [0, 256])
cv2.normalize(self.initial_context, self.initial_context)
return True
x, y, w, h = map(int, bbox)
margin = int(min(w, h) * 0.5)
context_roi = frame[max(0, y-margin):min(frame.shape[0], y+h+margin),
max(0, x-margin):min(frame.shape[1], x+w+margin)]
if context_roi.size == 0:
return False
curr_context = cv2.calcHist([cv2.cvtColor(context_roi, cv2.COLOR_BGR2HSV) if len(context_roi.shape) == 3 else context_roi],
[0, 1] if len(context_roi.shape) == 3 else [0], None,
[50, 60] if len(context_roi.shape) == 3 else [50],
[0, 180, 0, 256] if len(context_roi.shape) == 3 else [0, 256])
cv2.normalize(curr_context, curr_context)
context_similarity = cv2.compareHist(self.initial_context, curr_context, cv2.HISTCMP_CORREL)
return context_similarity > 0.5
except Exception as e:
logger.error(f"Error in context consistency: {e}")
return False
def update(self, frame, bbox, tracker_confidence, kalman_bbox):
try:
if self.initial_size is None:
self.initial_size = (bbox[2], bbox[3])
self.motion_model.set_object_size(bbox)
self._check_context_consistency(frame, bbox)
self.motion_model.update(bbox)
predicted_pos = self.motion_model.predict_next_position()
if predicted_pos is not None:
center_x, center_y = bbox[0] + bbox[2]/2, bbox[1] + bbox[3]/2
pred_center_x, pred_center_y = predicted_pos
w, h = bbox[2], bbox[3]
motion_dist = np.hypot(pred_center_x - center_x, pred_center_y - center_y)
expansion = max(20, min(100, int(motion_dist * 0.5)))
adjusted_bbox = (
max(0, min(int(pred_center_x - w/2 - expansion), frame.shape[1] - w - 2*expansion)),
max(0, min(int(pred_center_y - h/2 - expansion), frame.shape[0] - h - 2*expansion)),
min(w + 2 * expansion, frame.shape[1]),
min(h + 2 * expansion, frame.shape[0])
)
else:
adjusted_bbox = bbox
success = self.appearance_model.update_features(frame, adjusted_bbox)
if not success:
expanded_bbox = (
max(0, min(bbox[0] - 50, frame.shape[1] - bbox[2] - 100)),
max(0, min(bbox[1] - 50, frame.shape[0] - bbox[3] - 100)),
min(bbox[2] + 100, frame.shape[1]),
min(bbox[3] + 100, frame.shape[0])
)
success = self.appearance_model.update_features(frame, expanded_bbox)
if not success:
self.time_since_update += 1
self.lost_frames_count += 1
return 0.0, self.lost_frames_count >= self.lost_frames_threshold
appearance_score = self.appearance_model.compute_similarity(frame, bbox)
motion_consistent = self.motion_model.check_motion_consistency(bbox)
size_consistent = self._check_size_consistency(bbox)
context_consistent = self._check_context_consistency(frame, bbox)
boosted_confidence = self._dlo_confidence_boost(bbox, tracker_confidence, kalman_bbox)
final_confidence = self._duo_confidence_boost(bbox, boosted_confidence, kalman_bbox)
combined_confidence = (
0.3 * final_confidence +
0.25 * appearance_score +
0.2 * iou(bbox, kalman_bbox) +
0.15 * (1.0 - min(self.motion_model.mahalanobis_distance(bbox) / 5.0, 1.0)) +
0.1 * min(bbox[2] / (kalman_bbox[2] + 1e-6), kalman_bbox[2] / (bbox[2] + 1e-6)) *
min(bbox[3] / (kalman_bbox[3] + 1e-6), kalman_bbox[3] / (bbox[3] + 1e-6))
) * (0.7 if not context_consistent else 1.0)
if combined_confidence >= self.confidence_threshold:
self.hit_streak += 1
self.time_since_update = 0
self.lost_frames_count = max(0, self.lost_frames_count - 1)
else:
self.time_since_update += 1
self.lost_frames_count += 1
self.hit_streak = max(0, self.hit_streak - 1)
if self.initial_size and max(bbox[2], bbox[3]) < 50:
initial_center = (self.initial_size[0] / 2, self.initial_size[1] / 2)
current_center = (bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2)
drift_distance = np.hypot(current_center[0] - initial_center[0], current_center[1] - initial_center[1])
if drift_distance > 2.0 * min(self.initial_size):
self.lost_frames_count += 3
return combined_confidence, self.lost_frames_count >= self.lost_frames_threshold
except Exception as e:
logger.error(f"Error in confidence update: {e}")
return 0.0, True
def _check_size_consistency(self, bbox):
try:
if self.initial_size is None:
return True
current_size = (bbox[2], bbox[3])
if self.initial_size[0] == 0 or self.initial_size[1] == 0:
self.initial_size = current_size
return True
self.size_history.append(current_size)
if len(self.size_history) > 3:
filtered_w, filtered_h = np.median([s[0] for s in self.size_history]), np.median([s[1] for s in self.size_history])
width_change, height_change = [abs(f - i) / i for f, i in zip([filtered_w, filtered_h], self.initial_size)]
else:
width_change, height_change = [abs(c - i) / i for c, i in zip(current_size, self.initial_size)]
return max(width_change, height_change) <= self.max_size_change
except Exception as e:
logger.error(f"Error in size consistency: {e}")
return True
class TemporalEvidenceAccumulator:
"""Accumulates temporal evidence for stable tracking"""
def __init__(self, evidence_threshold=3, position_tolerance_ratio=0.3, convergence_threshold=5,
iou_threshold=0.4, max_age=50, min_hits=2):
self.evidence_threshold = evidence_threshold
self.position_tolerance_ratio = position_tolerance_ratio
self.convergence_threshold = convergence_threshold
self.iou_threshold = iou_threshold
self.max_age = max_age
self.min_hits = min_hits
self.candidate_buffer = deque()
self.missed_count = 0
self.last_refined_bbox = None
self.hit_streak = 0
self.time_since_update = 0
def reset(self):
self.candidate_buffer.clear()
self.missed_count = 0
self.last_refined_bbox = None
self.hit_streak = 0
self.time_since_update = 0
def update(self, bbox, frame_index):
try:
if bbox is None:
self.missed_count += 1
self.time_since_update += 1
if self.time_since_update >= self.max_age:
self.reset()
return (None, False)
self.missed_count = 0
self.hit_streak += 1
self.time_since_update = 0
bbox = tuple(map(int, bbox))
if self.last_refined_bbox is not None and not (
bbox[0] >= self.last_refined_bbox[0] and
bbox[1] >= self.last_refined_bbox[1] and
bbox[0] + bbox[2] <= self.last_refined_bbox[0] + self.last_refined_bbox[2] and
bbox[1] + bbox[3] <= self.last_refined_bbox[1] + self.last_refined_bbox[3]
):
if self.candidate_buffer and iou(self.candidate_buffer[-1][1], bbox) < self.iou_threshold:
self.reset()
elif self.candidate_buffer and iou(self.candidate_buffer[-1][1], bbox) < self.iou_threshold:
self.reset()
self.candidate_buffer.append((frame_index, bbox))
if len(self.candidate_buffer) < self.evidence_threshold or self.hit_streak < self.min_hits:
return (None, False)
centers = np.array([[b[0] + b[2]/2.0, b[1] + b[3]/2.0] for _, b in self.candidate_buffer])
widths, heights = [b[2] for _, b in self.candidate_buffer], [b[3] for _, b in self.candidate_buffer]
if np.max(np.std(centers, axis=0)) > self.convergence_threshold:
return (None, False)
median_center, median_width, median_height = np.median(centers, axis=0), np.median(widths), np.median(heights)
min_width, min_height = min(widths), min(heights)
if (median_width <= min_width * (1 + self.position_tolerance_ratio) and
median_height <= min_height * (1 + self.position_tolerance_ratio)):
refined_bbox = (
int(median_center[0] - median_width / 2.0),
int(median_center[1] - median_height / 2.0),
int(median_width),
int(median_height)
)
self.last_refined_bbox = refined_bbox
self.candidate_buffer.clear()
return (refined_bbox, True)
return (None, False)
except Exception as e:
logger.error(f"Error in temporal update: {e}")
return (None, False)
class TrackerManager:
"""Main tracking manager integrating all components"""
def __init__(self, video_path=None):
self.video_path = video_path if video_path else DEFAULT_VIDEO_PATH
self.global_vars = GlobalVars()
self.model = self._load_model()
self.cap = self._initialize_video()
self.tracker = None
self.prev_frame = None
self.prev_time = time.time()
self.original_template = None
def _load_model(self):
try:
cfg.merge_from_file(CONFIG_PATH)
cfg.CUDA = torch.cuda.is_available()
device = torch.device('cuda' if cfg.CUDA else 'cpu')
model = ModelBuilder()
model.load_state_dict(torch.load(SNAPSHOT_PATH, map_location=lambda storage, loc: storage.cpu(), weights_only=True))
return model.eval().to(device)
except Exception as e:
logger.error(f"Failed to load model: {e}")
return None
def _initialize_video(self):
try:
cap = cv2.VideoCapture(self.video_path if self.video_path else 0)
if not cap.isOpened():
logger.warning(f"Failed to open video {self.video_path}, switching to webcam")
cap = cv2.VideoCapture(0)
if not cap.isOpened():
logger.error("Failed to open webcam")
return None
self.global_vars.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if self.video_path else 0
orig_fps = cap.get(cv2.CAP_PROP_FPS) or 25
self.global_vars.new_fps = min(120, 4 * orig_fps)
return cap
except Exception as e:
logger.error(f"Error initializing video: {e}")
return None
def process_frame(self, frame):
"""Process a single frame and return tracking results with FPS"""
try:
if not self.model or not self.cap or frame is None:
return None, None, 0.0
current_time = time.time()
fps = 1.0 / (current_time - self.prev_time) if (current_time - self.prev_time) > 0 else 0.0
self.prev_time = current_time
self.global_vars.frame_idx += 1
self.global_vars.frame = frame.copy()
if self.global_vars.mode == "selection":
return frame, None, fps
if self.global_vars.tracked_object:
tracker = self.global_vars.tracked_object['tracker']
outputs = tracker.track(frame)
bbox = outputs['bbox']
accumulator = self.global_vars.tracked_object['accumulator']
refined_bbox, confirmed = accumulator.update(bbox, self.global_vars.frame_idx)
if confirmed:
bbox = refined_bbox
if self.prev_frame is not None:
dx, dy = optical_flow_update(self.prev_frame, frame, self.global_vars.tracked_object['kalman_box'])
x, y, w, h = self.global_vars.tracked_object['kalman_box']
self.global_vars.tracked_object['kalman_box'] = (int(x + dx), int(y + dy), w, h)
self.global_vars.tracked_object['prev_center'] = (int(x + dx + w/2), int(y + dy + h/2))
tracker_confidence = outputs.get('score', 0.8)
kalman_bbox = self.global_vars.tracked_object['kalman'].update(bbox, tracker_confidence)
tracker_weight = max(0.3, min(0.7, tracker_confidence))
size_weight = 0.7 if min(bbox[2]/kalman_bbox[2] if kalman_bbox[2] else 1,
kalman_bbox[2]/bbox[2] if bbox[2] else 1) * \
min(bbox[3]/kalman_bbox[3] if kalman_bbox[3] else 1,
kalman_bbox[3]/bbox[3] if bbox[3] else 1) < 0.7 else 0.8
fused_box = (
int(tracker_weight * bbox[0] + (1-tracker_weight) * kalman_bbox[0]),
int(tracker_weight * bbox[1] + (1-tracker_weight) * kalman_bbox[1]),
int(size_weight * bbox[2] + (1-size_weight) * kalman_bbox[2]),
int(size_weight * bbox[3] + (1-size_weight) * kalman_bbox[3])
)
new_center = (fused_box[0] + fused_box[2]//2, fused_box[1] + fused_box[3]//2)
jump_distance = np.hypot(new_center[0] - self.global_vars.tracked_object['prev_center'][0],
new_center[1] - self.global_vars.tracked_object['prev_center'][1])
if jump_distance > JUMP_THRESHOLD_BASE * max(fused_box[2], fused_box[3]) / 100.0:
jump_factor = JUMP_THRESHOLD_BASE * max(fused_box[2], fused_box[3]) / 100.0 / (jump_distance + 1e-6)
new_center_x = self.global_vars.tracked_object['prev_center'][0] + (new_center[0] - self.global_vars.tracked_object['prev_center'][0]) * jump_factor
new_center_y = self.global_vars.tracked_object['prev_center'][1] + (new_center[1] - self.global_vars.tracked_object['prev_center'][1]) * jump_factor
self.global_vars.tracked_object['kalman_box'] = (
int(new_center_x - fused_box[2]/2), int(new_center_y - fused_box[3]/2), fused_box[2], fused_box[3]
)
self.global_vars.tracked_object['prev_center'] = (new_center_x, new_center_y)
else:
self.global_vars.tracked_object['kalman_box'] = fused_box
self.global_vars.tracked_object['prev_center'] = new_center
conf, lost = self.global_vars.tracked_object['confidence_tracker'].update(
frame, self.global_vars.tracked_object['kalman_box'], tracker_confidence, kalman_bbox
)
if conf < 0.4:
redetected_box = universal_redetection(frame, self.global_vars.tracked_object['kalman_box'],
self.global_vars.tracked_object['prev_frame'],
self.original_template)
if iou(redetected_box, self.global_vars.tracked_object['kalman_box']) > 0.6:
blend = max(0.3, min(0.7, conf))
self.global_vars.tracked_object['kalman_box'] = tuple(
int(blend * self.global_vars.tracked_object['kalman_box'][i] + (1-blend) * redetected_box[i])
for i in range(4)
)
if lost or self.global_vars.tracked_object['confidence_tracker'].time_since_update > 20:
self.original_template = self.global_vars.tracked_object['confidence_tracker'].appearance_model.original_template
self.global_vars.tracked_object = None
self.global_vars.mode = "selection"
return frame, None, fps
self.global_vars.tracked_object['prev_frame'] = frame.copy()
return frame, self.global_vars.tracked_object['kalman_box'], fps
return frame, None, fps
except Exception as e:
logger.error(f"Error processing frame: {e}")
return frame, None, fps
def start_tracking(self, box):
"""Initialize tracking with given bounding box"""
try:
if self.global_vars.frame is None or box is None or min(box[2], box[3]) <= 0:
return False
tracker = build_tracker(self.model)
tracker.init(self.global_vars.frame, box)
self.global_vars.tracked_object = {
'tracker': tracker,
'init_rect': box,
'kalman_box': box,
'confidence_tracker': ConfidenceTracker(),
'color': (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)),
'prev_center': (box[0] + box[2]//2, box[1] + box[3]//2),
'accumulator': TemporalEvidenceAccumulator(),
'kalman': EnhancedKalmanBox(box),
'prev_frame': None
}
self.global_vars.mode = "tracking"
self.global_vars.init_rect = None
self.global_vars.tracked_object['confidence_tracker'].appearance_model.original_template = cv2.cvtColor(
self.global_vars.frame[box[1]:box[1]+box[3], box[0]:box[0]+box[2]], cv2.COLOR_BGR2GRAY)
self.original_template = self.global_vars.tracked_object['confidence_tracker'].appearance_model.original_template
return True
except Exception as e:
logger.error(f"Error starting tracking: {e}")
return False
def reset_tracking(self):
"""Reset tracking state"""
self.global_vars.tracked_object = None
self.global_vars.mode = "selection"
self.global_vars.init_rect = None
self.global_vars.click_point = None
self.original_template = None
def get_frame(self):
"""Read next frame from video source"""
try:
ret, frame = self.cap.read()
if not ret and self.video_path:
return None
elif not ret:
self.cap = cv2.VideoCapture(0)
ret, frame = self.cap.read()
if not ret:
return None
return frame
except Exception as e:
logger.error(f"Error getting frame: {e}")
return None
def release(self):
"""Clean up resources"""
if self.cap:
self.cap.release()
class GlobalVars:
"""Global state management"""
def __init__(self):
self.init_rect = None
self.frame = None
self.tracking_started = False
self.paused = False
self.click_point = None
self.prev_gray = None
self.kalman_initialized = False
self.kalman_box = None
self.mode = "selection"
self.frame_idx = 0
self.total_frames = 0
self.new_fps = 0
self.cursor_pos = None
self.last_mouse_move = 0
self.tracked_object = None
self.dragging = False
self.drag_start = None
self.drag_end = None
self.message_display = ""
def iou(boxA, boxB):
"""Calculate Intersection over Union"""
try:
xA, yA = max(boxA[0], boxB[0]), max(boxA[1], boxB[1])
xB, yB = min(boxA[0] + boxA[2], boxB[0] + boxB[2]), min(boxA[1] + boxA[3], boxB[1] + boxB[3])
interArea = max(0, xB - xA) * max(0, yB - yA)
boxAArea, boxBArea = boxA[2] * boxA[3], boxB[2] * boxB[3]
return interArea / float(boxAArea + boxBArea - interArea) if boxAArea + boxBArea - interArea > 0 else 0.0
except Exception as e:
logger.error(f"Error in IoU calculation: {e}")
return 0.0
def get_adaptive_bbox(frame, x, y):
"""Get adaptive bounding box around point"""
try:
temp_model = AppearanceModel()
window_size = 240
h, w = frame.shape[:2]
x1, y1 = max(0, x - window_size // 2), max(0, y - window_size // 2)
x2, y2 = min(w, x + window_size // 2), min(h, y + window_size // 2)
return temp_model.get_precise_object_boundaries(frame, (x1, y1, x2 - x1, y2 - y1))
except Exception as e:
logger.error(f"Error in get_adaptive_bbox: {e}")
return (x-50, y-50, 100, 100)
def optical_flow_update(prev_frame, current_frame, bbox):
"""Calculate optical flow displacement"""
try:
x, y, w, h = map(int, bbox)
if x < 0 or y < 0 or w <= 0 or h <= 0 or x + w > prev_frame.shape[1] or y + h > prev_frame.shape[0]:
return (0.0, 0.0)
prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY) if len(prev_frame.shape) == 3 else prev_frame.copy()
curr_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY) if len(current_frame.shape) == 3 else current_frame.copy()
roi = prev_gray[y:y+h, x:x+w]
max_corners = max(15, min(500, int(10 * math.log2(1 + (w * h) / 100))))
p0 = cv2.goodFeaturesToTrack(roi, maxCorners=max_corners, qualityLevel=0.005, minDistance=5, blockSize=7,
useHarrisDetector=True, k=0.04)
if p0 is None:
return (0.0, 0.0)
p0 = np.array(p0, dtype=np.float32).reshape(-1, 1, 2)
p0[:, 0, 0] += x; p0[:, 0, 1] += y
p1, st, _ = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, p0, None, winSize=(51, 51), maxLevel=7,
criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 0.01))
if p1 is None or len(p1[st.flatten() == 1]) == 0:
return (0.0, 0.0)
displacement = np.mean(p1[st.flatten() == 1].reshape(-1, 2) - p0[st.flatten() == 1].reshape(-1, 2), axis=0)
shift_magnitude = np.sqrt(displacement[0]**2 + displacement[1]**2)
max_shift = np.sqrt(w*w + h*h) * (0.3 + 0.5 / (1.0 + 0.02 * min(w, h)))
if shift_magnitude > max_shift:
dampening = max_shift / shift_magnitude
return (displacement[0] * dampening, displacement[1] * dampening)
return (float(displacement[0]), float(displacement[1]))
except Exception as e:
logger.error(f"Error in optical flow update: {e}")
return (0.0, 0.0)
def universal_redetection(frame, last_bbox, last_frame=None, original_template=None):
"""Redetect object when tracking is uncertain"""
try:
x, y, w, h = map(int, last_bbox)
if w <= 0 or h <= 0 or last_frame is None:
return last_bbox
search_w, search_h = min(frame.shape[1], w * 4), min(frame.shape[0], h * 4)
search_x, search_y = max(0, x - (search_w - w)//2), max(0, y - (search_h - h)//2)
template = original_template if original_template is not None else last_frame[y:y+h, x:x+w]
search_area = frame[search_y:min(search_y+search_h, frame.shape[0]),
search_x:min(search_x+search_w, frame.shape[1])]
if search_area.size > 0 and template.size > 0:
search_gray = cv2.cvtColor(search_area, cv2.COLOR_BGR2GRAY) if len(search_area.shape) == 3 else search_area
template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY) if len(template.shape) == 3 else template
if search_gray.shape[0] < template_gray.shape[0] or search_gray.shape[1] < template_gray.shape[1]:
return last_bbox
res = cv2.matchTemplate(search_gray, template_gray, cv2.TM_CCOEFF_NORMED)
_, max_val, _, max_loc = cv2.minMaxLoc(res)
if max_val > 0.6:
return (search_x + max_loc[0], search_y + max_loc[1], w, h)
return last_bbox
except Exception as e:
logger.error(f"Error in universal redetection: {e}")
return last_bbox
def mouse_callback(event, x, y, flags, param):
"""Handle mouse events for object selection"""
g = param
try:
if event == cv2.EVENT_LBUTTONDOWN:
g.dragging = True
g.drag_start = g.drag_end = (x, y)
elif event == cv2.EVENT_MOUSEMOVE and g.dragging and (flags & cv2.EVENT_FLAG_LBUTTON):
g.drag_end = (x, y)
x1, y1 = min(g.drag_start[0], g.drag_end[0]), min(g.drag_start[1], g.drag_end[1])
g.init_rect = (x1, y1, max(g.drag_start[0], g.drag_end[0]) - x1, max(g.drag_start[1], g.drag_end[1]) - y1)
elif event == cv2.EVENT_LBUTTONUP and g.dragging:
g.dragging = False
g.drag_end = (x, y)
x1, y1 = min(g.drag_start[0], g.drag_end[0]), min(g.drag_start[1], g.drag_end[1])
g.init_rect = (x1, y1, max(g.drag_start[0], g.drag_end[0]) - x1, max(g.drag_start[1], g.drag_end[1]) - y1)
g.click_point = ((x1 + max(g.drag_start[0], g.drag_end[0])) // 2, (y1 + max(g.drag_start[1], g.drag_end[1])) // 2)
elif event == cv2.EVENT_LBUTTONDBLCLK and g.frame is not None:
g.tracking_started = False
g.kalman_initialized = False
g.mode = "selection"
g.init_rect = get_adaptive_bbox(g.frame, x, y)
g.click_point = (x, y)
except Exception as e:
logger.error(f"Error in mouse callback: {e}")
if __name__ == "__main__":
tracker = TrackerManager()
if tracker.model is None:
logger.error("Exiting due to model loading failure.")
exit(1)
cv2.namedWindow("Tracking")
cv2.setMouseCallback("Tracking", mouse_callback, tracker.global_vars)
while True:
try:
frame = tracker.get_frame()
if frame is None:
logger.error("Failed to read frame. Exiting.")
break
display_frame, bbox, fps = tracker.process_frame(frame)
if display_frame is None:
logger.error("Failed to process frame. Exiting.")
break
if bbox:
cv2.rectangle(display_frame, (bbox[0], bbox[1]),
(bbox[0] + bbox[2], bbox[1] + bbox[3]),
tracker.global_vars.tracked_object['color'], 2)
if tracker.global_vars.init_rect:
x, y, w, h = tracker.global_vars.init_rect
cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
cv2.putText(display_frame, f"FPS: {fps:.2f}", (10, 30),
cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
if tracker.global_vars.mode == "selection" and tracker.original_template is not None:
cv2.putText(display_frame, "Object lost - Reselect to resume", (10, 60),
cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
cv2.imshow("Tracking", display_frame)
key = cv2.waitKey(1) & 0xFF
if key == ord('q'):
break
elif key == ord('c') and tracker.global_vars.init_rect:
tracker.start_tracking(tracker.global_vars.init_rect)
elif key == ord('r'):
tracker.reset_tracking()
except Exception as e:
logger.error(f"Error in main loop: {e}")
break
tracker.release()
cv2.destroyAllWindows()
from __future__ import absolute_import, division, print_function, unicode_literals
import cv2
import torch
import numpy as np
from pysot.core.config import cfg
from pysot.models.model_builder import ModelBuilder
from pysot.tracker.tracker_builder import build_tracker
import logging
from collections import deque
import random
import math
import time
# Hardcoded configuration paths
CONFIG_PATH = r"D:\TestingPysot\experiments\siamrpn_mobilev2_l234_dwxcorr\config.yaml"
SNAPSHOT_PATH = r"D:\TestingPysot\experiments\siamrpn_mobilev2_l234_dwxcorr\model.pth"
DEFAULT_VIDEO_PATH = r"D:\TestingPysot\demo\M16.avi"
# DEFAULT_VIDEO_PATH= 0
# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
JUMP_THRESHOLD_BASE = 500
torch.backends.cudnn.benchmark = True
class AppearanceModel:
"""Handles feature extraction and object appearance tracking"""
def __init__(self, feature_memory_size=5, spectrum_type=None):
self.feature_memory = deque(maxlen=feature_memory_size)
self.color_memory = deque(maxlen=feature_memory_size)
self.min_match_count = 5
self.original_template = None
self.template = None
self.template_size = None
self.alpha = 0.95
self.spectrum_type = spectrum_type
self._initialize_feature_extractor(spectrum_type)
def _initialize_feature_extractor(self, spectrum_type=None):
params = {'nfeatures': 2000, 'scaleFactor': 1.15, 'nlevels': 10,
'edgeThreshold': 35, 'patchSize': 37, 'fastThreshold': 15}
if spectrum_type == "MWIR":
params.update({'fastThreshold': 12, 'edgeThreshold': 31, 'nlevels': 12})
elif spectrum_type == "SWIR":
params.update({'fastThreshold': 17, 'scaleFactor': 1.2, 'patchSize': 33})
elif spectrum_type == "LWIR":
params.update({'fastThreshold': 10, 'nfeatures': 2500, 'edgeThreshold': 41, 'patchSize': 41})
elif spectrum_type == "FLIR":
params.update({'fastThreshold': 13, 'edgeThreshold': 33, 'nfeatures': 2200})
elif spectrum_type == "VIS":
params.update({'fastThreshold': 20, 'edgeThreshold': 31, 'patchSize': 31, 'nfeatures': 1500})
self.feature_extractor = cv2.ORB_create(**params)
self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
def extract_adaptive_features(self, frame, bbox=None, object_type=None):
try:
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame.copy()
if gray.size == 0:
return [], None
mean_val, std_val = np.mean(gray), np.std(gray)
if std_val < 30:
clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
gray = clahe.apply(gray)
sigma_color = 20 if std_val < 40 else 10
sigma_space = 10 if mean_val < 100 else 7
gray = cv2.bilateralFilter(gray, 9, sigma_color, sigma_space)
original_nfeatures = self.feature_extractor.getMaxFeatures()
if object_type:
if object_type == 'large':
self.feature_extractor.setMaxFeatures(int(original_nfeatures * 1.5))
elif object_type == 'small':
self.feature_extractor.setMaxFeatures(int(original_nfeatures * 0.8))
patch_size = self.feature_extractor.getPatchSize()
self.feature_extractor.setPatchSize(max(15, patch_size - 10))
elif object_type == 'flat':
self.feature_extractor.setMaxFeatures(int(original_nfeatures * 1.2))
fast_thresh = self.feature_extractor.getFastThreshold()
self.feature_extractor.setFastThreshold(fast_thresh + 5)
if bbox:
x, y, w, h = map(int, bbox)
margin = int(min(w, h) * 0.2)
roi = gray[max(0, y-margin):min(gray.shape[0], y+h+margin),
max(0, x-margin):min(gray.shape[1], x+w+margin)]
if roi.size == 0:
return [], None
keypoints = self.feature_extractor.detect(roi, None)
for kp in keypoints:
kp.pt = (kp.pt[0] + max(0, x-margin), kp.pt[1] + max(0, y-margin))
else:
keypoints = self.feature_extractor.detect(gray, None)
keypoints, descriptors = self.feature_extractor.compute(gray, keypoints)
if object_type:
self.feature_extractor.setMaxFeatures(original_nfeatures)
if object_type == 'small':
self.feature_extractor.setPatchSize(patch_size)
elif object_type == 'flat':
self.feature_extractor.setFastThreshold(fast_thresh)
return keypoints, descriptors
except Exception as e:
logger.error(f"Error in feature extraction: {e}")
return [], None
def get_precise_object_boundaries(self, frame, initial_bbox, spectrum_type=None):
try:
x, y, w, h = map(int, initial_bbox)
search_margin = max(int(min(w, h) * 0.3), 30)
roi = frame[max(0, y-search_margin):min(frame.shape[0], y+h+search_margin),
max(0, x-search_margin):min(frame.shape[1], x+w+search_margin)]
if roi.size == 0:
return initial_bbox
gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) == 3 else roi.copy()
mean_val, std_val = np.mean(gray_roi), np.std(gray_roi)
low_thresh_base = max(10, int(mean_val * 0.2))
high_thresh_base = max(30, int(mean_val * 0.5))
if spectrum_type in ["LWIR", "MWIR"]:
low_thresh_base *= 0.7
high_thresh_base *= 0.7
elif spectrum_type == "SWIR":
low_thresh_base *= 1.1
high_thresh_base *= 1.1
edges_combined = np.zeros_like(gray_roi)
for scale in [0.8, 1.0, 1.3]:
edges = cv2.Canny(gray_roi, int(low_thresh_base * scale), int(high_thresh_base * scale))
edges_combined = cv2.bitwise_or(edges_combined, edges)
edges_combined = cv2.morphologyEx(edges_combined, cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8), iterations=1)
object_type = 'large' if min(w, h) > 200 else 'small' if min(w, h) < 50 else None
keypoints, _ = self.extract_adaptive_features(roi, object_type=object_type)
kp_mask = np.zeros_like(edges_combined)
for kp in keypoints:
x_kp, y_kp = map(int, kp.pt)
if 0 <= x_kp < kp_mask.shape[1] and 0 <= y_kp < kp_mask.shape[0]:
cv2.circle(kp_mask, (x_kp, y_kp), 3, 255, -1)
combined_map = cv2.addWeighted(edges_combined, 0.7, kp_mask, 0.3, 0)
contours, _ = cv2.findContours(combined_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
if not contours:
return initial_bbox
best_contour, best_score = None, float('-inf')
min_area, max_area = max(100, w * h * 0.2), w * h * 3.0
for cnt in contours:
area = cv2.contourArea(cnt)
if not (min_area <= area <= max_area):
continue
M = cv2.moments(cnt)
if M['m00'] == 0:
continue
cx, cy = int(M['m10'] / M['m00']), int(M['m01'] / M['m00'])
dist_to_center = np.sqrt((cx - w//2)**2 + (cy - h//2)**2)
x_cnt, y_cnt, w_cnt, h_cnt = cv2.boundingRect(cnt)
score = (
min(area/(w*h), (w*h)/area) * 0.4 +
min(w/h if h else 1, w_cnt/h_cnt if h_cnt else 1) * 0.3 +
max(0, 1 - dist_to_center/max(1, min(w, h))) * 0.3
)
if score > best_score:
best_score = score
best_contour = cnt
if best_contour is not None and best_score > 0.3:
x_cnt, y_cnt, w_cnt, h_cnt = cv2.boundingRect(best_contour)
return (x + x_cnt - search_margin, y + y_cnt - search_margin, w_cnt, h_cnt)
if keypoints:
pts = np.array([kp.pt for kp in keypoints])
min_x, min_y = np.percentile(pts[:, 0], 5), np.percentile(pts[:, 1], 5)
max_x, max_y = np.percentile(pts[:, 0], 95), np.percentile(pts[:, 1], 95)
return (
int(x + 0.3 * ((min_x + max_x)/2 - w/2)),
int(y + 0.3 * ((min_y + max_y)/2 - h/2)),
int(0.7 * w + 0.3 * (max_x - min_x)),
int(0.7 * h + 0.3 * (max_y - min_y))
)
return initial_bbox
except Exception as e:
logger.error(f"Error in get_precise_object_boundaries: {e}")
return initial_bbox
def update_features(self, frame, bbox):
try:
x, y, w, h = map(int, bbox)
x, y = max(0, min(x, frame.shape[1] - 1)), max(0, min(y, frame.shape[0] - 1))
w, h = min(w, frame.shape[1] - x), min(h, frame.shape[0] - y)
if w <= 0 or h <= 0:
return False
roi = frame[y:y+h, x:x+w]
object_type = 'large' if max(w, h) > 200 else 'small' if max(w, h) < 50 else None
keypoints, descriptors = self.extract_adaptive_features(roi, object_type=object_type)
if descriptors is not None and len(descriptors) > 0 and object_type != 'small':
self.feature_memory.append(descriptors)
else:
return False
if self.original_template is None:
self.original_template = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) == 3 else roi.copy()
self.template = self.original_template.copy()
self.template_size = (w, h)
elif max(w, h) > 200:
self.template = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) == 3 else roi.copy()
self.template_size = (w, h)
hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV) if len(roi.shape) == 3 else roi
hist = cv2.calcHist([hsv_roi], [0, 1] if len(roi.shape) == 3 else [0], None,
[50, 60] if len(roi.shape) == 3 else [50],
[0, 180, 0, 256] if len(roi.shape) == 3 else [0, 256])
cv2.normalize(hist, hist)
if object_type != 'small':
self.color_memory.append(hist)
return True
except Exception as e:
logger.error(f"Error in update_features: {e}")
return False
def compute_similarity(self, frame, bbox):
try:
if not self.feature_memory and self.original_template is None:
return 0.0
x, y, w, h = map(int, bbox)
roi = frame[y:y+h, x:x+w]
if roi.size == 0:
return 0.0
object_type = 'large' if max(w, h) > 200 else 'small' if max(w, h) < 50 else None
keypoints, descriptors = self.extract_adaptive_features(roi, object_type=object_type)
orb_similarity = 0.0
if descriptors is not None and len(descriptors) > 0:
if object_type == 'small' and self.original_template is not None:
gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) == 3 else roi.copy()
resized_template = cv2.resize(self.original_template, (w, h))
result = cv2.matchTemplate(gray_roi, resized_template, cv2.TM_CCOEFF_NORMED)
orb_similarity = np.max(result) if result.size > 0 else 0.0
else:
for stored_descriptors in self.feature_memory:
matches = self.matcher.match(descriptors, stored_descriptors)
if len(matches) > self.min_match_count:
avg_distance = sum(m.distance for m in matches) / len(matches)
orb_similarity = max(orb_similarity, 1.0 - (avg_distance / 100.0))
if self.template is not None and max(w, h) > 200:
gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) == 3 else roi.copy()
resized_template = cv2.resize(self.template, (w, h))
result = cv2.matchTemplate(gray_roi, resized_template, cv2.TM_CCOEFF_NORMED)
orb_similarity = 0.7 * orb_similarity + 0.3 * np.max(result) if result.size > 0 else orb_similarity
hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV) if len(roi.shape) == 3 else roi
curr_hist = cv2.calcHist([hsv_roi], [0, 1] if len(roi.shape) == 3 else [0], None,
[50, 60] if len(roi.shape) == 3 else [50],
[0, 180, 0, 256] if len(roi.shape) == 3 else [0, 256])
cv2.normalize(curr_hist, curr_hist)
color_similarity = max(0.0, max(cv2.compareHist(curr_hist, hist, cv2.HISTCMP_CORREL)
for hist in self.color_memory)) if self.color_memory and object_type != 'small' else 0.0
uniqueness_score = 0.0
if object_type == 'small' and keypoints and self.original_template is not None:
keypoint_ratio = len(keypoints) / (w * h / 1000)
uniqueness_score = min(1.0, keypoint_ratio * 0.5 + np.std([kp.response for kp in keypoints]) / 100.0 * 0.5)
return (0.6 * orb_similarity + 0.4 * color_similarity) * (1.0 + uniqueness_score) if object_type == 'small' else (0.6 * orb_similarity + 0.4 * color_similarity)
except Exception as e:
logger.error(f"Error in compute_similarity: {e}")
return 0.0
class MotionModel:
"""Tracks object motion and predicts positions"""
def __init__(self, history_size=5):
self.position_history = deque(maxlen=history_size)
self.velocity_history = deque(maxlen=history_size-1)
self.max_acceleration_base = 10000
self.max_velocity_base = 7500
self.object_size = 1.0
self.covariance = np.eye(2) * 1e-2
def set_object_size(self, bbox):
w, h = bbox[2], bbox[3]
self.object_size = max(w, h) / 100.0
self.max_acceleration = self.max_acceleration_base * self.object_size * (0.5 if max(w, h) < 50 else 1.0)
self.max_velocity = self.max_velocity_base * self.object_size * (0.5 if max(w, h) < 50 else 1.0)
def update(self, bbox):
center_x, center_y = bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2
current_pos = np.array([center_x, center_y])
self.position_history.append(current_pos)
if len(self.position_history) >= 2:
velocity = self.position_history[-1] - self.position_history[-2]
self.velocity_history.append(velocity)
self.covariance = np.cov(np.array(self.position_history).T) + np.eye(2) * 1e-4
def check_motion_consistency(self, bbox):
if len(self.velocity_history) < 2:
return True
center_x, center_y = bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2
predicted_pos = self.position_history[-1] + self.velocity_history[-1]
current_velocity = np.array([center_x, center_y]) - self.position_history[-1]
acceleration = current_velocity - self.velocity_history[-1]
return not (np.linalg.norm(acceleration) > self.max_acceleration * 2.0 or
np.linalg.norm(current_velocity) > self.max_velocity * 2.0)
def mahalanobis_distance(self, bbox):
center_x, center_y = bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2
actual_pos = np.array([center_x, center_y])
predicted_pos = (self.position_history[-1] + self.velocity_history[-1]
if self.velocity_history else actual_pos)
diff = actual_pos - predicted_pos
inv_cov = np.linalg.inv(self.covariance + np.eye(2) * 1e-6)
return np.sqrt(diff.T @ inv_cov @ diff)
def predict_next_position(self):
return (self.position_history[-1] + self.velocity_history[-1] * 2.0
if self.velocity_history else self.position_history[-1]
if self.position_history else None)
class EnhancedKalmanBox:
"""Kalman filter for bounding box tracking"""
def __init__(self, bbox):
self.kalman = cv2.KalmanFilter(10, 4)
dt = 1.0
self.kalman.transitionMatrix = np.array([
[1, 0, 0, 0, dt, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, dt, 0, 0, 0, 0],
[0, 0, 1, 0, 0, 0, dt, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, dt, 0, 0],
[0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
[0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
], np.float32)
self.kalman.measurementMatrix = np.array([
[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
], np.float32)
process_noise = np.eye(10, dtype=np.float32) * 1e-3
process_noise[0:2, 0:2] *= 50; process_noise[4:6, 4:6] *= 20; process_noise[8, 8] *= 5
self.kalman.processNoiseCov = process_noise
self.kalman.measurementNoiseCov = np.eye(4, dtype=np.float32) * 1e-1
self.kalman.errorCovPost = np.eye(10, dtype=np.float32)
x, y, w, h = bbox
self.kalman.statePre = np.array([[x], [y], [w], [h], [0], [0], [0], [0], [0], [w/h if h != 0 else 1.0]], np.float32)
self.initial_aspect_ratio = w/h if h != 0 else 1.0
self.max_aspect_ratio_change = 0.6
self.size_history = deque(maxlen=5)
self.initial_size = (w, h)
def predict(self):
return tuple(self.kalman.predict()[:4].flatten())
def update(self, bbox, confidence=1.0):
try:
x, y, w, h = bbox
current_aspect_ratio = w/h if h != 0 else 1.0
aspect_change = abs(current_aspect_ratio - self.initial_aspect_ratio) / self.initial_aspect_ratio
if aspect_change > self.max_aspect_ratio_change:
prediction = self.predict()
alpha = 0.7
x, y, w, h = [alpha * pred + (1 - alpha) * val for pred, val in zip(prediction, [x, y, w, h])]
self.size_history.append((w, h))
if len(self.size_history) > 3:
filtered_w, filtered_h = np.median([s[0] for s in self.size_history]), np.median([s[1] for s in self.size_history])
w_change, h_change = abs(w - self.initial_size[0]) / self.initial_size[0], abs(h - self.initial_size[1]) / self.initial_size[1]
if max(w_change, h_change) > 0.9:
w, h = filtered_w, filtered_h
self.kalman.correct(np.array([[np.float32(x)], [np.float32(y)], [np.float32(w)], [np.float32(h)]]))
return self.predict()
except Exception as e:
logger.error(f"Error in Kalman update: {e}")
return bbox
class ConfidenceTracker:
"""Manages tracking confidence and object persistence"""
def __init__(self, confidence_threshold=0.5, lost_frames_threshold=60, det_thresh=0.2):
self.confidence_threshold = confidence_threshold
self.lost_frames_threshold = lost_frames_threshold
self.det_thresh = det_thresh
self.lost_frames_count = 0
self.appearance_model = AppearanceModel()
self.motion_model = MotionModel()
self.initial_size = None
self.max_size_change = 0.9
self.hit_streak = 0
self.time_since_update = 0
self.size_history = deque(maxlen=5)
self.initial_context = None
def _dlo_confidence_boost(self, bbox, tracker_confidence, kalman_bbox):
iou_score = iou(bbox, kalman_bbox)
mahalanobis_dist = self.motion_model.mahalanobis_distance(bbox)
shape_similarity = min(bbox[2] / (kalman_bbox[2] + 1e-6), kalman_bbox[2] / (bbox[2] + 1e-6)) * \
min(bbox[3] / (kalman_bbox[3] + 1e-6), kalman_bbox[3] / (bbox[3] + 1e-6))
if iou_score > 0.3 and mahalanobis_dist < 5.0 and shape_similarity > 0.5:
return min(1.0, tracker_confidence + 0.2 * (iou_score + shape_similarity))
return tracker_confidence
def _duo_confidence_boost(self, bbox, tracker_confidence, kalman_bbox):
mahalanobis_dist = self.motion_model.mahalanobis_distance(bbox)
iou_score = iou(bbox, kalman_bbox)
if mahalanobis_dist > 7.0 or iou_score < 0.2:
return max(0.0, tracker_confidence - 0.2)
return tracker_confidence
def _check_context_consistency(self, frame, bbox):
try:
if self.initial_context is None or max(bbox[2], bbox[3]) > 200:
x, y, w, h = map(int, bbox)
margin = int(min(w, h) * 0.5)
context_roi = frame[max(0, y-margin):min(frame.shape[0], y+h+margin),
max(0, x-margin):min(frame.shape[1], x+w+margin)]
if context_roi.size > 0:
self.initial_context = cv2.calcHist([cv2.cvtColor(context_roi, cv2.COLOR_BGR2HSV) if len(context_roi.shape) == 3 else context_roi],
[0, 1] if len(context_roi.shape) == 3 else [0], None,
[50, 60] if len(context_roi.shape) == 3 else [50],
[0, 180, 0, 256] if len(context_roi.shape) == 3 else [0, 256])
cv2.normalize(self.initial_context, self.initial_context)
return True
x, y, w, h = map(int, bbox)
margin = int(min(w, h) * 0.5)
context_roi = frame[max(0, y-margin):min(frame.shape[0], y+h+margin),
max(0, x-margin):min(frame.shape[1], x+w+margin)]
if context_roi.size == 0:
return False
curr_context = cv2.calcHist([cv2.cvtColor(context_roi, cv2.COLOR_BGR2HSV) if len(context_roi.shape) == 3 else context_roi],
[0, 1] if len(context_roi.shape) == 3 else [0], None,
[50, 60] if len(context_roi.shape) == 3 else [50],
[0, 180, 0, 256] if len(context_roi.shape) == 3 else [0, 256])
cv2.normalize(curr_context, curr_context)
context_similarity = cv2.compareHist(self.initial_context, curr_context, cv2.HISTCMP_CORREL)
return context_similarity > 0.5
except Exception as e:
logger.error(f"Error in context consistency: {e}")
return False
def update(self, frame, bbox, tracker_confidence, kalman_bbox):
try:
if self.initial_size is None:
self.initial_size = (bbox[2], bbox[3])
self.motion_model.set_object_size(bbox)
self._check_context_consistency(frame, bbox)
self.motion_model.update(bbox)
predicted_pos = self.motion_model.predict_next_position()
if predicted_pos is not None:
center_x, center_y = bbox[0] + bbox[2]/2, bbox[1] + bbox[3]/2
pred_center_x, pred_center_y = predicted_pos
w, h = bbox[2], bbox[3]
motion_dist = np.hypot(pred_center_x - center_x, pred_center_y - center_y)
expansion = max(20, min(100, int(motion_dist * 0.5)))
adjusted_bbox = (
max(0, min(int(pred_center_x - w/2 - expansion), frame.shape[1] - w - 2*expansion)),
max(0, min(int(pred_center_y - h/2 - expansion), frame.shape[0] - h - 2*expansion)),
min(w + 2 * expansion, frame.shape[1]),
min(h + 2 * expansion, frame.shape[0])
)
else:
adjusted_bbox = bbox
success = self.appearance_model.update_features(frame, adjusted_bbox)
if not success:
expanded_bbox = (
max(0, min(bbox[0] - 50, frame.shape[1] - bbox[2] - 100)),
max(0, min(bbox[1] - 50, frame.shape[0] - bbox[3] - 100)),
min(bbox[2] + 100, frame.shape[1]),
min(bbox[3] + 100, frame.shape[0])
)
success = self.appearance_model.update_features(frame, expanded_bbox)
if not success:
self.time_since_update += 1
self.lost_frames_count += 1
return 0.0, self.lost_frames_count >= self.lost_frames_threshold
appearance_score = self.appearance_model.compute_similarity(frame, bbox)
motion_consistent = self.motion_model.check_motion_consistency(bbox)
size_consistent = self._check_size_consistency(bbox)
context_consistent = self._check_context_consistency(frame, bbox)
boosted_confidence = self._dlo_confidence_boost(bbox, tracker_confidence, kalman_bbox)
final_confidence = self._duo_confidence_boost(bbox, boosted_confidence, kalman_bbox)
combined_confidence = (
0.3 * final_confidence +
0.25 * appearance_score +
0.2 * iou(bbox, kalman_bbox) +
0.15 * (1.0 - min(self.motion_model.mahalanobis_distance(bbox) / 5.0, 1.0)) +
0.1 * min(bbox[2] / (kalman_bbox[2] + 1e-6), kalman_bbox[2] / (bbox[2] + 1e-6)) *
min(bbox[3] / (kalman_bbox[3] + 1e-6), kalman_bbox[3] / (bbox[3] + 1e-6))
) * (0.7 if not context_consistent else 1.0)
if combined_confidence >= self.confidence_threshold:
self.hit_streak += 1
self.time_since_update = 0
self.lost_frames_count = max(0, self.lost_frames_count - 1)
else:
self.time_since_update += 1
self.lost_frames_count += 1
self.hit_streak = max(0, self.hit_streak - 1)
if self.initial_size and max(bbox[2], bbox[3]) < 50:
initial_center = (self.initial_size[0] / 2, self.initial_size[1] / 2)
current_center = (bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2)
drift_distance = np.hypot(current_center[0] - initial_center[0], current_center[1] - initial_center[1])
if drift_distance > 2.0 * min(self.initial_size):
self.lost_frames_count += 3
return combined_confidence, self.lost_frames_count >= self.lost_frames_threshold
except Exception as e:
logger.error(f"Error in confidence update: {e}")
return 0.0, True
def _check_size_consistency(self, bbox):
try:
if self.initial_size is None:
return True
current_size = (bbox[2], bbox[3])
if self.initial_size[0] == 0 or self.initial_size[1] == 0:
self.initial_size = current_size
return True
self.size_history.append(current_size)
if len(self.size_history) > 3:
filtered_w, filtered_h = np.median([s[0] for s in self.size_history]), np.median([s[1] for s in self.size_history])
width_change, height_change = [abs(f - i) / i for f, i in zip([filtered_w, filtered_h], self.initial_size)]
else:
width_change, height_change = [abs(c - i) / i for c, i in zip(current_size, self.initial_size)]
return max(width_change, height_change) <= self.max_size_change
except Exception as e:
logger.error(f"Error in size consistency: {e}")
return True
class TemporalEvidenceAccumulator:
"""Accumulates temporal evidence for stable tracking"""
def __init__(self, evidence_threshold=3, position_tolerance_ratio=0.3, convergence_threshold=5,
iou_threshold=0.4, max_age=50, min_hits=2):
self.evidence_threshold = evidence_threshold
self.position_tolerance_ratio = position_tolerance_ratio
self.convergence_threshold = convergence_threshold
self.iou_threshold = iou_threshold
self.max_age = max_age
self.min_hits = min_hits
self.candidate_buffer = deque()
self.missed_count = 0
self.last_refined_bbox = None
self.hit_streak = 0
self.time_since_update = 0
def reset(self):
self.candidate_buffer.clear()
self.missed_count = 0
self.last_refined_bbox = None
self.hit_streak = 0
self.time_since_update = 0
def update(self, bbox, frame_index):
try:
if bbox is None:
self.missed_count += 1
self.time_since_update += 1
if self.time_since_update >= self.max_age:
self.reset()
return (None, False)
self.missed_count = 0
self.hit_streak += 1
self.time_since_update = 0
bbox = tuple(map(int, bbox))
if self.last_refined_bbox is not None and not (
bbox[0] >= self.last_refined_bbox[0] and
bbox[1] >= self.last_refined_bbox[1] and
bbox[0] + bbox[2] <= self.last_refined_bbox[0] + self.last_refined_bbox[2] and
bbox[1] + bbox[3] <= self.last_refined_bbox[1] + self.last_refined_bbox[3]
):
if self.candidate_buffer and iou(self.candidate_buffer[-1][1], bbox) < self.iou_threshold:
self.reset()
elif self.candidate_buffer and iou(self.candidate_buffer[-1][1], bbox) < self.iou_threshold:
self.reset()
self.candidate_buffer.append((frame_index, bbox))
if len(self.candidate_buffer) < self.evidence_threshold or self.hit_streak < self.min_hits:
return (None, False)
centers = np.array([[b[0] + b[2]/2.0, b[1] + b[3]/2.0] for _, b in self.candidate_buffer])
widths, heights = [b[2] for _, b in self.candidate_buffer], [b[3] for _, b in self.candidate_buffer]
if np.max(np.std(centers, axis=0)) > self.convergence_threshold:
return (None, False)
median_center, median_width, median_height = np.median(centers, axis=0), np.median(widths), np.median(heights)
min_width, min_height = min(widths), min(heights)
if (median_width <= min_width * (1 + self.position_tolerance_ratio) and
median_height <= min_height * (1 + self.position_tolerance_ratio)):
refined_bbox = (
int(median_center[0] - median_width / 2.0),
int(median_center[1] - median_height / 2.0),
int(median_width),
int(median_height)
)
self.last_refined_bbox = refined_bbox
self.candidate_buffer.clear()
return (refined_bbox, True)
return (None, False)
except Exception as e:
logger.error(f"Error in temporal update: {e}")
return (None, False)
class TrackerManager:
"""Main tracking manager integrating all components"""
def __init__(self, video_path=None):
self.video_path = video_path if video_path else DEFAULT_VIDEO_PATH
self.global_vars = GlobalVars()
self.model = self._load_model()
self.cap = self._initialize_video()
self.tracker = None
self.prev_frame = None
self.prev_time = time.time()
self.original_template = None
def _load_model(self):
try:
cfg.merge_from_file(CONFIG_PATH)
cfg.CUDA = torch.cuda.is_available()
device = torch.device('cuda' if cfg.CUDA else 'cpu')
model = ModelBuilder()
model.load_state_dict(torch.load(SNAPSHOT_PATH, map_location=lambda storage, loc: storage.cpu(), weights_only=True))
return model.eval().to(device)
except Exception as e:
logger.error(f"Failed to load model: {e}")
return None
def _initialize_video(self):
try:
cap = cv2.VideoCapture(self.video_path if self.video_path else 0)
if not cap.isOpened():
logger.warning(f"Failed to open video {self.video_path}, switching to webcam")
cap = cv2.VideoCapture(0)
if not cap.isOpened():
logger.error("Failed to open webcam")
return None
self.global_vars.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if self.video_path else 0
orig_fps = cap.get(cv2.CAP_PROP_FPS) or 25
self.global_vars.new_fps = min(120, 4 * orig_fps)
return cap
except Exception as e:
logger.error(f"Error initializing video: {e}")
return None
def process_frame(self, frame):
"""Process a single frame and return tracking results with FPS"""
try:
if not self.model or not self.cap or frame is None:
return None, None, 0.0
current_time = time.time()
fps = 1.0 / (current_time - self.prev_time) if (current_time - self.prev_time) > 0 else 0.0
self.prev_time = current_time
self.global_vars.frame_idx += 1
self.global_vars.frame = frame.copy()
if self.global_vars.mode == "selection":
return frame, None, fps
if self.global_vars.tracked_object:
tracker = self.global_vars.tracked_object['tracker']
outputs = tracker.track(frame)
bbox = outputs['bbox']
accumulator = self.global_vars.tracked_object['accumulator']
refined_bbox, confirmed = accumulator.update(bbox, self.global_vars.frame_idx)
if confirmed:
bbox = refined_bbox
if self.prev_frame is not None:
dx, dy = optical_flow_update(self.prev_frame, frame, self.global_vars.tracked_object['kalman_box'])
x, y, w, h = self.global_vars.tracked_object['kalman_box']
self.global_vars.tracked_object['kalman_box'] = (int(x + dx), int(y + dy), w, h)
self.global_vars.tracked_object['prev_center'] = (int(x + dx + w/2), int(y + dy + h/2))
tracker_confidence = outputs.get('score', 0.8)
kalman_bbox = self.global_vars.tracked_object['kalman'].update(bbox, tracker_confidence)
tracker_weight = max(0.3, min(0.7, tracker_confidence))
size_weight = 0.7 if min(bbox[2]/kalman_bbox[2] if kalman_bbox[2] else 1,
kalman_bbox[2]/bbox[2] if bbox[2] else 1) * \
min(bbox[3]/kalman_bbox[3] if kalman_bbox[3] else 1,
kalman_bbox[3]/bbox[3] if bbox[3] else 1) < 0.7 else 0.8
fused_box = (
int(tracker_weight * bbox[0] + (1-tracker_weight) * kalman_bbox[0]),
int(tracker_weight * bbox[1] + (1-tracker_weight) * kalman_bbox[1]),
int(size_weight * bbox[2] + (1-size_weight) * kalman_bbox[2]),
int(size_weight * bbox[3] + (1-size_weight) * kalman_bbox[3])
)
new_center = (fused_box[0] + fused_box[2]//2, fused_box[1] + fused_box[3]//2)
jump_distance = np.hypot(new_center[0] - self.global_vars.tracked_object['prev_center'][0],
new_center[1] - self.global_vars.tracked_object['prev_center'][1])
if jump_distance > JUMP_THRESHOLD_BASE * max(fused_box[2], fused_box[3]) / 100.0:
jump_factor = JUMP_THRESHOLD_BASE * max(fused_box[2], fused_box[3]) / 100.0 / (jump_distance + 1e-6)
new_center_x = self.global_vars.tracked_object['prev_center'][0] + (new_center[0] - self.global_vars.tracked_object['prev_center'][0]) * jump_factor
new_center_y = self.global_vars.tracked_object['prev_center'][1] + (new_center[1] - self.global_vars.tracked_object['prev_center'][1]) * jump_factor
self.global_vars.tracked_object['kalman_box'] = (
int(new_center_x - fused_box[2]/2), int(new_center_y - fused_box[3]/2), fused_box[2], fused_box[3]
)
self.global_vars.tracked_object['prev_center'] = (new_center_x, new_center_y)
else:
self.global_vars.tracked_object['kalman_box'] = fused_box
self.global_vars.tracked_object['prev_center'] = new_center
conf, lost = self.global_vars.tracked_object['confidence_tracker'].update(
frame, self.global_vars.tracked_object['kalman_box'], tracker_confidence, kalman_bbox
)
if conf < 0.4:
redetected_box = universal_redetection(frame, self.global_vars.tracked_object['kalman_box'],
self.global_vars.tracked_object['prev_frame'],
self.original_template)
if iou(redetected_box, self.global_vars.tracked_object['kalman_box']) > 0.6:
blend = max(0.3, min(0.7, conf))
self.global_vars.tracked_object['kalman_box'] = tuple(
int(blend * self.global_vars.tracked_object['kalman_box'][i] + (1-blend) * redetected_box[i])
for i in range(4)
)
if lost or self.global_vars.tracked_object['confidence_tracker'].time_since_update > 20:
self.original_template = self.global_vars.tracked_object['confidence_tracker'].appearance_model.original_template
self.global_vars.tracked_object = None
self.global_vars.mode = "selection"
return frame, None, fps
self.global_vars.tracked_object['prev_frame'] = frame.copy()
return frame, self.global_vars.tracked_object['kalman_box'], fps
return frame, None, fps
except Exception as e:
logger.error(f"Error processing frame: {e}")
return frame, None, fps
def start_tracking(self, box):
"""Initialize tracking with given bounding box"""
try:
if self.global_vars.frame is None or box is None or min(box[2], box[3]) <= 0:
return False
tracker = build_tracker(self.model)
tracker.init(self.global_vars.frame, box)
self.global_vars.tracked_object = {
'tracker': tracker,
'init_rect': box,
'kalman_box': box,
'confidence_tracker': ConfidenceTracker(),
'color': (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)),
'prev_center': (box[0] + box[2]//2, box[1] + box[3]//2),
'accumulator': TemporalEvidenceAccumulator(),
'kalman': EnhancedKalmanBox(box),
'prev_frame': None
}
self.global_vars.mode = "tracking"
self.global_vars.init_rect = None
self.global_vars.tracked_object['confidence_tracker'].appearance_model.original_template = cv2.cvtColor(
self.global_vars.frame[box[1]:box[1]+box[3], box[0]:box[0]+box[2]], cv2.COLOR_BGR2GRAY)
self.original_template = self.global_vars.tracked_object['confidence_tracker'].appearance_model.original_template
return True
except Exception as e:
logger.error(f"Error starting tracking: {e}")
return False
def reset_tracking(self):
"""Reset tracking state"""
self.global_vars.tracked_object = None
self.global_vars.mode = "selection"
self.global_vars.init_rect = None
self.global_vars.click_point = None
self.original_template = None
def get_frame(self):
"""Read next frame from video source"""
try:
ret, frame = self.cap.read()
if not ret and self.video_path:
return None
elif not ret:
self.cap = cv2.VideoCapture(0)
ret, frame = self.cap.read()
if not ret:
return None
return frame
except Exception as e:
logger.error(f"Error getting frame: {e}")
return None
def release(self):
"""Clean up resources"""
if self.cap:
self.cap.release()
class GlobalVars:
"""Global state management"""
def __init__(self):
self.init_rect = None
self.frame = None
self.tracking_started = False
self.paused = False
self.click_point = None
self.prev_gray = None
self.kalman_initialized = False
self.kalman_box = None
self.mode = "selection"
self.frame_idx = 0
self.total_frames = 0
self.new_fps = 0
self.cursor_pos = None
self.last_mouse_move = 0
self.tracked_object = None
self.dragging = False
self.drag_start = None
self.drag_end = None
self.message_display = ""
def iou(boxA, boxB):
"""Calculate Intersection over Union"""
try:
xA, yA = max(boxA[0], boxB[0]), max(boxA[1], boxB[1])
xB, yB = min(boxA[0] + boxA[2], boxB[0] + boxB[2]), min(boxA[1] + boxA[3], boxB[1] + boxB[3])
interArea = max(0, xB - xA) * max(0, yB - yA)
boxAArea, boxBArea = boxA[2] * boxA[3], boxB[2] * boxB[3]
return interArea / float(boxAArea + boxBArea - interArea) if boxAArea + boxBArea - interArea > 0 else 0.0
except Exception as e:
logger.error(f"Error in IoU calculation: {e}")
return 0.0
def get_adaptive_bbox(frame, x, y):
"""Get adaptive bounding box around point"""
try:
temp_model = AppearanceModel()
window_size = 240
h, w = frame.shape[:2]
x1, y1 = max(0, x - window_size // 2), max(0, y - window_size // 2)
x2, y2 = min(w, x + window_size // 2), min(h, y + window_size // 2)
return temp_model.get_precise_object_boundaries(frame, (x1, y1, x2 - x1, y2 - y1))
except Exception as e:
logger.error(f"Error in get_adaptive_bbox: {e}")
return (x-50, y-50, 100, 100)
def optical_flow_update(prev_frame, current_frame, bbox):
"""Calculate optical flow displacement"""
try:
x, y, w, h = map(int, bbox)
if x < 0 or y < 0 or w <= 0 or h <= 0 or x + w > prev_frame.shape[1] or y + h > prev_frame.shape[0]:
return (0.0, 0.0)
prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY) if len(prev_frame.shape) == 3 else prev_frame.copy()
curr_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY) if len(current_frame.shape) == 3 else current_frame.copy()
roi = prev_gray[y:y+h, x:x+w]
max_corners = max(15, min(500, int(10 * math.log2(1 + (w * h) / 100))))
p0 = cv2.goodFeaturesToTrack(roi, maxCorners=max_corners, qualityLevel=0.005, minDistance=5, blockSize=7,
useHarrisDetector=True, k=0.04)
if p0 is None:
return (0.0, 0.0)
p0 = np.array(p0, dtype=np.float32).reshape(-1, 1, 2)
p0[:, 0, 0] += x; p0[:, 0, 1] += y
p1, st, _ = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, p0, None, winSize=(51, 51), maxLevel=7,
criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 0.01))
if p1 is None or len(p1[st.flatten() == 1]) == 0:
return (0.0, 0.0)
displacement = np.mean(p1[st.flatten() == 1].reshape(-1, 2) - p0[st.flatten() == 1].reshape(-1, 2), axis=0)
shift_magnitude = np.sqrt(displacement[0]**2 + displacement[1]**2)
max_shift = np.sqrt(w*w + h*h) * (0.3 + 0.5 / (1.0 + 0.02 * min(w, h)))
if shift_magnitude > max_shift:
dampening = max_shift / shift_magnitude
return (displacement[0] * dampening, displacement[1] * dampening)
return (float(displacement[0]), float(displacement[1]))
except Exception as e:
logger.error(f"Error in optical flow update: {e}")
return (0.0, 0.0)
def universal_redetection(frame, last_bbox, last_frame=None, original_template=None):
"""Redetect object when tracking is uncertain"""
try:
x, y, w, h = map(int, last_bbox)
if w <= 0 or h <= 0 or last_frame is None:
return last_bbox
search_w, search_h = min(frame.shape[1], w * 4), min(frame.shape[0], h * 4)
search_x, search_y = max(0, x - (search_w - w)//2), max(0, y - (search_h - h)//2)
template = original_template if original_template is not None else last_frame[y:y+h, x:x+w]
search_area = frame[search_y:min(search_y+search_h, frame.shape[0]),
search_x:min(search_x+search_w, frame.shape[1])]
if search_area.size > 0 and template.size > 0:
search_gray = cv2.cvtColor(search_area, cv2.COLOR_BGR2GRAY) if len(search_area.shape) == 3 else search_area
template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY) if len(template.shape) == 3 else template
if search_gray.shape[0] < template_gray.shape[0] or search_gray.shape[1] < template_gray.shape[1]:
return last_bbox
res = cv2.matchTemplate(search_gray, template_gray, cv2.TM_CCOEFF_NORMED)
_, max_val, _, max_loc = cv2.minMaxLoc(res)
if max_val > 0.6:
return (search_x + max_loc[0], search_y + max_loc[1], w, h)
return last_bbox
except Exception as e:
logger.error(f"Error in universal redetection: {e}")
return last_bbox
def mouse_callback(event, x, y, flags, param):
"""Handle mouse events for object selection"""
g = param
try:
if event == cv2.EVENT_LBUTTONDOWN:
g.dragging = True
g.drag_start = g.drag_end = (x, y)
elif event == cv2.EVENT_MOUSEMOVE and g.dragging and (flags & cv2.EVENT_FLAG_LBUTTON):
g.drag_end = (x, y)
x1, y1 = min(g.drag_start[0], g.drag_end[0]), min(g.drag_start[1], g.drag_end[1])
g.init_rect = (x1, y1, max(g.drag_start[0], g.drag_end[0]) - x1, max(g.drag_start[1], g.drag_end[1]) - y1)
elif event == cv2.EVENT_LBUTTONUP and g.dragging:
g.dragging = False
g.drag_end = (x, y)
x1, y1 = min(g.drag_start[0], g.drag_end[0]), min(g.drag_start[1], g.drag_end[1])
g.init_rect = (x1, y1, max(g.drag_start[0], g.drag_end[0]) - x1, max(g.drag_start[1], g.drag_end[1]) - y1)
g.click_point = ((x1 + max(g.drag_start[0], g.drag_end[0])) // 2, (y1 + max(g.drag_start[1], g.drag_end[1])) // 2)
elif event == cv2.EVENT_LBUTTONDBLCLK and g.frame is not None:
g.tracking_started = False
g.kalman_initialized = False
g.mode = "selection"
g.init_rect = get_adaptive_bbox(g.frame, x, y)
g.click_point = (x, y)
except Exception as e:
logger.error(f"Error in mouse callback: {e}")
if __name__ == "__main__":
tracker = TrackerManager()
if tracker.model is None:
logger.error("Exiting due to model loading failure.")
exit(1)
cv2.namedWindow("Tracking")
cv2.setMouseCallback("Tracking", mouse_callback, tracker.global_vars)
while True:
try:
frame = tracker.get_frame()
if frame is None:
logger.error("Failed to read frame. Exiting.")
break
display_frame, bbox, fps = tracker.process_frame(frame)
if display_frame is None:
logger.error("Failed to process frame. Exiting.")
break
if bbox:
cv2.rectangle(display_frame, (bbox[0], bbox[1]),
(bbox[0] + bbox[2], bbox[1] + bbox[3]),
tracker.global_vars.tracked_object['color'], 2)
if tracker.global_vars.init_rect:
x, y, w, h = tracker.global_vars.init_rect
cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
cv2.putText(display_frame, f"FPS: {fps:.2f}", (10, 30),
cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
if tracker.global_vars.mode == "selection" and tracker.original_template is not None:
cv2.putText(display_frame, "Object lost - Reselect to resume", (10, 60),
cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
cv2.imshow("Tracking", display_frame)
key = cv2.waitKey(1) & 0xFF
if key == ord('q'):
break
elif key == ord('c') and tracker.global_vars.init_rect:
tracker.start_tracking(tracker.global_vars.init_rect)
elif key == ord('r'):
tracker.reset_tracking()
except Exception as e:
logger.error(f"Error in main loop: {e}")
break
tracker.release()
cv2.destroyAllWindows()
import cv2
import numpy as np
from collections import deque
from ultralytics import YOLO
class LocalizationApp:
LOW_VISIBILITY_THRESHOLD = 80    # Threshold for low visibility scenario
IOU_FUSION_THRESHOLD = 0.3         # IoU threshold for fusing detections when both methods are used
def __init__(self, video_path, model_path):
self.force_refresh_flag = False
self.lock_on_flag = False  # Operator override flag if needed
self.video_path = video_path
self.model_path = model_path
self.model = YOLO(self.model_path)
self.cap = cv2.VideoCapture(self.video_path)
if not self.cap.isOpened():
raise Exception("Error: Could not open video.")
self.fusion_buffer = []
self.fusion_buffer_size = 5   # Number of frames for multi-frame fusion
# Create instance of ObjectLocalizer for frame preprocessing and localization
self.localizer = self.ObjectLocalizer()
class ObjectLocalizer:
"""
This inner class implements robust pre-processing and localization using multi-frame fusion,
adaptive thresholding, morphological operations and temporal smoothing.
"""
BUFFER_SIZE = 30                  # Frames used for background modeling
EMA_ALPHA = 0.1                   # Weight for exponential moving average of background
BASE_THRESH_OFFSET = 10           # Base offset for thresholding (adjusted by noise)
MIN_AREA = 120                    # Minimum area threshold for a valid candidate
MORPH_KERNEL_SIZE = 3             # Kernel size for morphological operations
ROI_STD_THRESH = 12               # Minimum standard deviation in ROI (to avoid low contrast regions)
MIN_ASPECT_RATIO = 0.5            # Expected minimum aspect ratio for a valid object
MAX_ASPECT_RATIO = 3.0            # Expected maximum aspect ratio for a valid object
NMS_OVERLAP_THRESH = 0.3          # Overlap threshold for non-maximum suppression
SMOOTHING_DIST_THRESH = 30        # Maximum allowed distance for associating detections across frames
MIN_PERSISTENCE = 5               # Minimum consecutive frame detections for stability
UPDATE_AREA_FACTOR = 1.2          # When a larger candidate is detected, update the box if area increases by this factor
AGGREGATE_FRAMES = 5              # Frames to aggregate for weak signals
def __init__(self):
self.frame_buffer = deque(maxlen=self.BUFFER_SIZE)
self.prev_detections = []     # Each detection: (x, y, w, h, cx, cy, count)
self.last_detection = None    # Best candidate detection so far
self.prev_bg_model = None     # Background model computed via EMA
self.aggregate_detection = None
self.aggregate_count = 0
def compute_background_model_ema(self, gray_frame):
if self.prev_bg_model is None:
self.prev_bg_model = gray_frame.copy().astype(np.float32)
else:
cv2.accumulateWeighted(gray_frame, self.prev_bg_model, self.EMA_ALPHA)
return cv2.convertScaleAbs(self.prev_bg_model)
def preprocess_frame(self, frame):
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
gray = cv2.medianBlur(gray, 3)
clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))
enhanced = clahe.apply(gray)
return gray, enhanced
def adaptive_threshold(self, enhanced, bg_model):
diff = cv2.absdiff(enhanced, bg_model)
diff = cv2.GaussianBlur(diff, (5, 5), 0)
noise_level = np.std(diff)
adaptive_offset = self.BASE_THRESH_OFFSET * (noise_level / 10.0)
ret, _ = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
thresh_val = min(255, ret + adaptive_offset)
_, thresh_frame = cv2.threshold(diff, thresh_val, 255, cv2.THRESH_BINARY)
adapt_thresh = cv2.adaptiveThreshold(diff, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
cv2.THRESH_BINARY, 15, -2)
combined_thresh = cv2.bitwise_and(thresh_frame, adapt_thresh)
kernel = np.ones((self.MORPH_KERNEL_SIZE, self.MORPH_KERNEL_SIZE), np.uint8)
opened = cv2.morphologyEx(combined_thresh, cv2.MORPH_OPEN, kernel, iterations=1)
closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel, iterations=1)
return closed
def filter_candidate(self, cnt, enhanced):
area = cv2.contourArea(cnt)
if area < self.MIN_AREA:
return False, None, None
x, y, w, h = cv2.boundingRect(cnt)
aspect_ratio = w / float(h) if h != 0 else 0
if aspect_ratio < self.MIN_ASPECT_RATIO or aspect_ratio > self.MAX_ASPECT_RATIO:
return False, None, None
roi = enhanced[y:y+h, x:x+w]
if roi.size == 0 or np.std(roi) < self.ROI_STD_THRESH:
return False, None, None
M = cv2.moments(cnt)
if M["m00"] != 0:
cx = int(M["m10"] / M["m00"])
cy = int(M["m01"] / M["m00"])
else:
cx, cy = x + w // 2, y + h // 2
return True, (x, y, w, h), (cx, cy)
def non_max_suppression(self, boxes, overlapThresh):
if len(boxes) == 0:
return []
boxes_np = np.array(boxes, dtype=float)
pick = []
x1 = boxes_np[:, 0]
y1 = boxes_np[:, 1]
x2 = boxes_np[:, 0] + boxes_np[:, 2]
y2 = boxes_np[:, 1] + boxes_np[:, 3]
area = (x2 - x1 + 1) * (y2 - y1 + 1)
idxs = np.argsort(y2)
while len(idxs) > 0:
last = idxs[-1]
pick.append(last)
suppress = [len(idxs) - 1]
for pos in range(len(idxs) - 1):
i = idxs[pos]
xx1 = max(x1[last], x1[i])
yy1 = max(y1[last], y1[i])
xx2 = min(x2[last], x2[i])
yy2 = min(y2[last], y2[i])
w = max(0, xx2 - xx1 + 1)
h = max(0, yy2 - yy1 + 1)
overlap = float(w * h) / area[i]
if overlap > overlapThresh:
suppress.append(pos)
idxs = np.delete(idxs, suppress)
return boxes_np[pick].astype(int).tolist()
def get_objects(self, thresh_frame, enhanced):
contours, _ = cv2.findContours(thresh_frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
valid_boxes = []
for cnt in contours:
valid, bbox, _ = self.filter_candidate(cnt, enhanced)
if valid:
valid_boxes.append((bbox[0], bbox[1], bbox[2], bbox[3]))
final_boxes = self.non_max_suppression(valid_boxes, self.NMS_OVERLAP_THRESH)
detections = []
for box in final_boxes:
x, y, w, h = box
cx, cy = x + w // 2, y + h // 2
detections.append((x, y, w, h, cx, cy))
return detections
def temporal_smoothing(self, current_detections):
new_detections = []
for curr in current_detections:
x, y, w, h, cx, cy = curr
best_match = None
min_dist = float('inf')
for prev in self.prev_detections:
px, py, pw, ph, pcx, pcy, count = prev
dist = np.sqrt((cx - pcx)**2 + (cy - pcy)**2)
if dist < self.SMOOTHING_DIST_THRESH and dist < min_dist:
min_dist = dist
best_match = prev
if best_match:
px, py, pw, ph, pcx, pcy, count = best_match
new_x = int((x + px) / 2)
new_y = int((y + py) / 2)
new_w = int((w + pw) / 2)
new_h = int((h + ph) / 2)
new_cx = int((cx + pcx) / 2)
new_cy = int((cy + pcy) / 2)
new_detections.append((new_x, new_y, new_w, new_h, new_cx, new_cy, count + 1))
else:
new_detections.append((x, y, w, h, cx, cy, 1))
self.prev_detections = new_detections.copy()
if new_detections:
best = max(new_detections, key=lambda d: d[2] * d[3])
if self.last_detection is not None:
prev_area = self.last_detection[2] * self.last_detection[3]
new_area = best[2] * best[3]
if new_area > prev_area * self.UPDATE_AREA_FACTOR:
self.last_detection = best
else:
self.last_detection = best
if best[2] * best[3] < self.MIN_AREA * 4:
self.aggregate_count += 1
if self.aggregate_detection is None:
self.aggregate_detection = best
else:
ax, ay, aw, ah, acx, acy, _ = self.aggregate_detection
bx, by, bw, bh, bcx, bcy = best[:6]
agg_x = int((ax + bx) / 2)
agg_y = int((ay + by) / 2)
agg_w = int((aw + bw) / 2)
agg_h = int((ah + bh) / 2)
agg_cx = int((acx + bcx) / 2)
agg_cy = int((acy + bcy) / 2)
self.aggregate_detection = (agg_x, agg_y, agg_w, agg_h, agg_cx, agg_cy, 1)
if self.aggregate_count >= self.AGGREGATE_FRAMES:
self.last_detection = self.aggregate_detection
self.aggregate_detection = None
self.aggregate_count = 0
return [self.last_detection] if self.last_detection is not None else []
def fallback_trigger(self, frame):
gray, enhanced = self.preprocess_frame(frame)
bg_model = self.compute_background_model_ema(gray)
thresh_frame = self.adaptive_threshold(enhanced, bg_model)
objects = self.get_objects(thresh_frame, enhanced)
return objects, thresh_frame
def process_frame(self, frame, force_refresh=False, lock_on_override=False):
gray, enhanced = self.preprocess_frame(frame)
self.frame_buffer.append(gray)
bg_model_ema = self.compute_background_model_ema(gray)
if force_refresh:
objects, thresh_frame = self.fallback_trigger(frame)
else:
thresh_frame = self.adaptive_threshold(enhanced, bg_model_ema)
objects = self.get_objects(thresh_frame, enhanced)
detections = self.temporal_smoothing(objects)
if lock_on_override and not detections and self.last_detection is not None:
detections = [self.last_detection]
return detections, thresh_frame
@staticmethod
def compute_iou(boxA, boxB):
# boxA and boxB are in the format (x, y, w, h)
xA = max(boxA[0], boxB[0])
yA = max(boxA[1], boxB[1])
xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])
interW = max(0, xB - xA)
interH = max(0, yB - yA)
interArea = interW * interH
areaA = boxA[2] * boxA[3]
areaB = boxB[2] * boxB[3]
unionArea = areaA + areaB - interArea
if unionArea == 0:
return 0
return interArea / unionArea
@staticmethod
def fuse_detections(det1, det2):
# Fuse candidate boxes by averaging respective coordinates and centers.
x = int((det1[0] + det2[0]) / 2)
y = int((det1[1] + det2[1]) / 2)
w = int((det1[2] + det2[2]) / 2)
h = int((det1[3] + det2[3]) / 2)
cx = int((det1[4] + det2[4]) / 2)
cy = int((det1[5] + det2[5]) / 2)
return (x, y, w, h, cx, cy)
def focused_yolo_refinement(self, detection, frame):
x, y, w, h, cx, cy = detection[:6]
CONF_THRESHOLD = 0.3
exp_scale = 4  # Expand ROI for context
roi_x = max(0, x - (exp_scale - 1) * w // 2)
roi_y = max(0, y - (exp_scale - 1) * h // 2)
roi_w = min(frame.shape[1] - roi_x, w * exp_scale)
roi_h = min(frame.shape[0] - roi_y, h * exp_scale)
roi = frame[roi_y:roi_y+roi_h, roi_x:roi_x+roi_w]
yolo_results = self.model(roi)
for result in yolo_results:
for box in result.boxes:
if box.conf[0] >= CONF_THRESHOLD:
rx1, ry1, rx2, ry2 = map(int, box.xyxy[0])
full_x1 = roi_x + rx1
full_y1 = roi_y + ry1
full_w = rx2 - rx1
full_h = ry2 - ry1
return (full_x1, full_y1, full_w, full_h,
full_x1 + full_w // 2, full_y1 + full_h // 2)
return detection
def full_frame_yolo_scan(self, frame):
yolo_results = self.model(frame)
detections = []
CONF_THRESHOLD = 0.3
for result in yolo_results:
for box in result.boxes:
if box.conf[0] >= CONF_THRESHOLD:
x1, y1, x2, y2 = map(int, box.xyxy[0])
detections.append((x1, y1, x2 - x1, y2 - y1,
x1 + (x2 - x1) // 2, y1 + (y2 - y1) // 2))
if detections:
return max(detections, key=lambda d: d[2] * d[3])
else:
return None
def mouse_callback(self, event, x, y, flags, param):
if event == cv2.EVENT_LBUTTONDBLCLK:
self.force_refresh_flag = True
def run(self):
cv2.namedWindow("Localized Output")
cv2.setMouseCallback("Localized Output", self.mouse_callback)
while self.cap.isOpened():
ret, frame = self.cap.read()
if not ret:
break
# Stage 1: Preprocessing / Fusion for enhanced visibility
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
enhanced_gray = clahe.apply(gray)
self.fusion_buffer.append(enhanced_gray)
if len(self.fusion_buffer) > self.fusion_buffer_size:
self.fusion_buffer.pop(0)
fused_frame = np.mean(self.fusion_buffer, axis=0).astype(np.uint8)
heatmap = cv2.applyColorMap(fused_frame, cv2.COLORMAP_JET)
# Check for low visibility: fallback if average intensity is below threshold
avg_intensity = np.mean(fused_frame)
low_visibility = avg_intensity < self.LOW_VISIBILITY_THRESHOLD
# Stage 2: Localization preprocessing
detections, thresh_frame = self.localizer.process_frame(
frame, force_refresh=self.force_refresh_flag, lock_on_override=self.lock_on_flag)
self.force_refresh_flag = False
loc_candidate = None
if detections and not low_visibility:
# Use the first detection from preprocessing as candidate
loc_candidate = detections[0]
# Optionally refine with focused YOLO
detection_localizer = None
if loc_candidate is not None:
detection_localizer = self.focused_yolo_refinement(loc_candidate, frame)
# Run full-frame YOLO scan to compare detections
detection_yolo = self.full_frame_yolo_scan(frame)
# Fuse detections if both are available based on IoU
detection_to_draw = None
if detection_localizer is not None and detection_yolo is not None:
iou = self.compute_iou(detection_localizer[:4], detection_yolo[:4])
if iou > self.IOU_FUSION_THRESHOLD:
detection_to_draw = self.fuse_detections(detection_localizer, detection_yolo)
else:
detection_to_draw = detection_localizer
elif detection_localizer is not None:
detection_to_draw = detection_localizer
elif detection_yolo is not None:
detection_to_draw = detection_yolo
output_frame = frame.copy()
if detection_to_draw is not None:
x, y, w, h, cx, cy = detection_to_draw[:6]
cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
cv2.circle(output_frame, (cx, cy), 3, (0, 255, 0), -1)
cv2.putText(output_frame, f"({cx},{cy})", (x, y - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
cv2.imshow("Heatmap", heatmap)
cv2.imshow("Adaptive Threshold", thresh_frame)
cv2.imshow("Localized Output", output_frame)
if cv2.waitKey(1) & 0xFF == ord('q'):
break
self.cap.release()
cv2.destroyAllWindows()
if __name__ == "__main__":
video_path = r"D:\TX\Tracking\demo\M5.avi"
model_path = r"D:\TX\Tracking\experiments\yolov8n.pt"
app = LocalizationApp(video_path, model_path)
app.run()
from __future__ import absolute_import, division, print_function, unicode_literals
import argparse
import cv2
import torch
import numpy as np
from pysot.core.config import cfg
from pysot.models.model_builder import ModelBuilder
from pysot.tracker.tracker_builder import build_tracker
import logging
import time
from ultralytics import YOLO
import threading
import queue
import math
from collections import deque
import random
# Enable CUDA benchmark
torch.backends.cudnn.benchmark = True
# Configure logging
logging.basicConfig(level=logging.INFO,
format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
JUMP_THRESHOLD = 50  # Maximum allowed jump (in pixels) between consecutive bounding box centers
# Appearance model for keeping the target fingerprint
class AppearanceModel:
def __init__(self, feature_memory_size=5):
self.feature_extractor = cv2.SIFT_create()
self.feature_memory = deque(maxlen=feature_memory_size)
self.color_memory = deque(maxlen=feature_memory_size)
self.matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
self.min_match_count = 10
def update_features(self, frame, bbox):
x, y, w, h = map(int, bbox)
roi = frame[y:y+h, x:x+w]
if roi.size == 0:
return False
gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
keypoints, descriptors = self.feature_extractor.detectAndCompute(gray_roi, None)
if descriptors is not None:
self.feature_memory.append(descriptors)
else:
return False
hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
hist = cv2.calcHist([hsv_roi], channels=[0, 1], mask=None,
histSize=[50, 60], ranges=[0, 180, 0, 256])
cv2.normalize(hist, hist)
self.color_memory.append(hist)
return True
def compute_similarity(self, frame, bbox):
if not self.feature_memory:
return 0.0
x, y, w, h = map(int, bbox)
roi = frame[y:y+h, x:x+w]
if roi.size == 0:
return 0.0
gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
keypoints, descriptors = self.feature_extractor.detectAndCompute(gray_roi, None)
if descriptors is None:
sift_similarity = 0.0
else:
sift_similarity = 0.0
for stored_descriptors in self.feature_memory:
matches = self.matcher.match(descriptors, stored_descriptors)
if len(matches) > self.min_match_count:
distances = [m.distance for m in matches]
similarity = 1.0 - (sum(distances) / len(distances)) / 500.0
sift_similarity = max(sift_similarity, similarity)
hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
curr_hist = cv2.calcHist([hsv_roi], channels=[0, 1], mask=None,
histSize=[50, 60], ranges=[0, 180, 0, 256])
cv2.normalize(curr_hist, curr_hist)
color_similarity = 0.0
if self.color_memory:
for stored_hist in self.color_memory:
sim = cv2.compareHist(curr_hist, stored_hist, cv2.HISTCMP_CORREL)
color_similarity = max(color_similarity, max(sim, 0.0))
combined_similarity = 0.5 * sift_similarity + 0.5 * color_similarity
return combined_similarity
# Motion model to verify consistency of the target
class MotionModel:
def __init__(self, history_size=5):
self.position_history = deque(maxlen=history_size)
self.velocity_history = deque(maxlen=history_size-1)
self.max_acceleration = 50
self.max_velocity = 100
def update(self, bbox):
center_x = bbox[0] + bbox[2] / 2
center_y = bbox[1] + bbox[3] / 2
current_pos = np.array([center_x, center_y])
self.position_history.append(current_pos)
if len(self.position_history) >= 2:
velocity = self.position_history[-1] - self.position_history[-2]
self.velocity_history.append(velocity)
def check_motion_consistency(self, bbox):
if len(self.velocity_history) < 2:
return True
center_x = bbox[0] + bbox[2] / 2
center_y = bbox[1] + bbox[3] / 2
predicted_pos = self.position_history[-1] + self.velocity_history[-1]
actual_pos = np.array([center_x, center_y])
current_velocity = actual_pos - self.position_history[-1]
acceleration = current_velocity - self.velocity_history[-1]
if np.linalg.norm(acceleration) > self.max_acceleration:
return False
if np.linalg.norm(current_velocity) > self.max_velocity:
return False
return True
# Kalman filter extension for box tracking
class EnhancedKalmanBox:
def __init__(self, bbox):
self.kalman = cv2.KalmanFilter(8, 4)
dt = 1.0
self.kalman.transitionMatrix = np.array([
[1, 0, 0, 0, dt, 0,  0,  0],
[0, 1, 0, 0, 0,  dt, 0,  0],
[0, 0, 1, 0, 0, 0,  dt, 0],
[0, 0, 0, 1, 0, 0, 0,  dt],
[0, 0, 0, 0, 1, 0, 0,  0],
[0, 0, 0, 0, 0, 1, 0,  0],
[0, 0, 0, 0, 0, 0, 1,  0],
[0, 0, 0, 0, 0, 0, 0,  1]
], np.float32)
self.kalman.measurementMatrix = np.array([
[1, 0, 0, 0, 0, 0, 0, 0],
[0, 1, 0, 0, 0, 0, 0, 0],
[0, 0, 1, 0, 0, 0, 0, 0],
[0, 0, 0, 1, 0, 0, 0, 0]
], np.float32)
self.kalman.processNoiseCov = np.eye(8, dtype=np.float32) * 1e-3
self.kalman.measurementNoiseCov = np.eye(4, dtype=np.float32) * 1e-1
self.kalman.errorCovPost = np.eye(8, dtype=np.float32)
x, y, w, h = bbox
self.kalman.statePre = np.array([[x], [y], [w], [h], [0], [0], [0], [0]], np.float32)
self.initial_aspect_ratio = w / h if h != 0 else 1.0
self.max_aspect_ratio_change = 0.3
def predict(self):
prediction = self.kalman.predict()
return tuple(prediction[:4].flatten())
def update(self, bbox):
x, y, w, h = bbox
current_aspect_ratio = w / h if h != 0 else 1.0
aspect_ratio_change = abs(current_aspect_ratio - self.initial_aspect_ratio) / self.initial_aspect_ratio
if aspect_ratio_change > self.max_aspect_ratio_change:
prediction = self.predict()
alpha = 0.7
x = alpha * prediction[0] + (1 - alpha) * x
y = alpha * prediction[1] + (1 - alpha) * y
w = alpha * prediction[2] + (1 - alpha) * w
h = alpha * prediction[3] + (1 - alpha) * h
measurement = np.array([[np.float32(x)], [np.float32(y)],
[np.float32(w)], [np.float32(h)]])
self.kalman.correct(measurement)
return self.predict()
# Confidence tracker combines motion, appearance, and detection confidence
class ConfidenceTracker:
def __init__(self, confidence_threshold=0.6, lost_frames_threshold=30):
self.confidence_threshold = confidence_threshold
self.lost_frames_threshold = lost_frames_threshold
self.lost_frames_count = 0
self.appearance_model = AppearanceModel()
self.motion_model = MotionModel()
self.initial_size = None
self.max_size_change = 0.5
def update(self, frame, bbox, tracker_confidence):
if self.initial_size is None:
self.initial_size = (bbox[2], bbox[3])
self.motion_model.update(bbox)
self.appearance_model.update_features(frame, bbox)
appearance_score = self.appearance_model.compute_similarity(frame, bbox)
motion_consistent = self.motion_model.check_motion_consistency(bbox)
size_consistent = self.check_size_consistency(bbox)
combined_confidence = (
0.4 * tracker_confidence +
0.3 * appearance_score +
0.2 * float(motion_consistent) +
0.1 * float(size_consistent)
)
if combined_confidence < self.confidence_threshold:
self.lost_frames_count += 1
else:
self.lost_frames_count = max(0, self.lost_frames_count - 1)
return combined_confidence, self.lost_frames_count >= self.lost_frames_threshold
def check_size_consistency(self, bbox):
if self.initial_size is None:
return True
current_size = (bbox[2], bbox[3])
if self.initial_size[0] == 0 or self.initial_size[1] == 0:
self.initial_size = current_size
return True
width_change = abs(current_size[0] - self.initial_size[0]) / self.initial_size[0]
height_change = abs(current_size[1] - self.initial_size[1]) / self.initial_size[1]
return max(width_change, height_change) <= self.max_size_change
# Global variables to maintain state
class GlobalVars:
def __init__(self):
self.init_rect = None
self.frame = None
self.tracking_started = False
self.paused = False
self.click_point = None
self.prev_gray = None
self.kalman_initialized = False
self.kalman_box = None
self.lock = threading.Lock()
self.mode = "selection"
self.stop_async_thread = False
self.frame_queue = None
self.frame_idx = 0
self.total_frames = 0
self.new_fps = 0
self.cursor_pos = None
self.last_mouse_move = 0
self.use_bytetrack = False
self.tracked_objects = []  # For ByteTrack mode
self.single_trackers = []  # For non-ByteTrack multi-object tracking
self.dragging = False
self.drag_start = None
self.drag_end = None
self.message_display = ""  # For GUI messages
g = GlobalVars()
import numpy as np
from collections import deque
def iou(boxA, boxB):
"""Compute the Intersection over Union (IoU) between two bounding boxes in (x, y, w, h) format."""
xA = max(boxA[0], boxB[0])
yA = max(boxA[1], boxB[1])
xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])
interArea = max(0, xB - xA) * max(0, yB - yA)
boxAArea = boxA[2] * boxA[3]
boxBArea = boxB[2] * boxB[3]
denom = boxAArea + boxBArea - interArea
if denom == 0:
return 0.0
return interArea / float(denom)
class TemporalEvidenceAccumulator:
def __init__(self, evidence_threshold=5,
position_tolerance_ratio=0.2,
convergence_threshold=3,   # maximum standard deviation in pixels for candidate centers
iou_threshold=0.3,         # minimum required IoU between successive candidates
max_missed_frames=3):
self.evidence_threshold = evidence_threshold
self.position_tolerance_ratio = position_tolerance_ratio
self.convergence_threshold = convergence_threshold
self.iou_threshold = iou_threshold
self.max_missed_frames = max_missed_frames
self.candidate_buffer = deque()
self.missed_count = 0
self.last_refined_bbox = None  # Store previously confirmed refined bbox
def reset(self):
"""Reset the candidate buffer and missed frame count."""
self.candidate_buffer.clear()
self.missed_count = 0
self.last_refined_bbox = None
def update(self, bbox, frame_index):
# If no detection in the current frame
if bbox is None:
self.missed_count += 1
if self.missed_count >= self.max_missed_frames:
print("Missed candidate for too many frames; resetting buffer.")
self.reset()
return (None, False)
else:
self.missed_count = 0
bbox = tuple(map(int, bbox))
# If we have a previously confirmed refined bbox, check if new candidate is inside it.
if self.last_refined_bbox is not None:
lx, ly, lw, lh = self.last_refined_bbox
if (bbox[0] >= lx and bbox[1] >= ly and
bbox[0] + bbox[2] <= lx + lw and
bbox[1] + bbox[3] <= ly + lh):
# Candidate is inside the refined bounding box. Update candidate buffer.
print(f"Candidate at frame {frame_index} is inside refined bbox; updating buffer.")
self.candidate_buffer.append((frame_index, bbox))
else:
# Check IoU with latest candidate in buffer if exists.
if self.candidate_buffer:
_, last_bbox = self.candidate_buffer[-1]
current_iou = iou(last_bbox, bbox)
if current_iou < self.iou_threshold:
print(f"Inconsistent candidate (IoU {current_iou:.2f}) at frame {frame_index}; resetting buffer.")
self.reset()
self.candidate_buffer.append((frame_index, bbox))
else:
self.candidate_buffer.append((frame_index, bbox))
else:
# No previous confirmed bbox, so check consistency using IoU.
if self.candidate_buffer:
_, last_bbox = self.candidate_buffer[-1]
current_iou = iou(last_bbox, bbox)
if current_iou < self.iou_threshold:
print(f"Inconsistent candidate (IoU {current_iou:.2f}) at frame {frame_index}; resetting buffer.")
self.reset()
self.candidate_buffer.append((frame_index, bbox))
else:
self.candidate_buffer.append((frame_index, bbox))
# If we haven't collected enough evidence, return unconfirmed.
if len(self.candidate_buffer) < self.evidence_threshold:
return (None, False)
# Extract candidate center and size from the buffer.
centers = []
widths = []
heights = []
for _, b in self.candidate_buffer:
cx = b[0] + b[2] / 2.0
cy = b[1] + b[3] / 2.0
centers.append([cx, cy])
widths.append(b[2])
heights.append(b[3])
centers = np.array(centers)
median_center = np.median(centers, axis=0)
median_width = np.median(widths)
median_height = np.median(heights)
std_center = np.std(centers, axis=0)
if np.max(std_center) > self.convergence_threshold:
print(f"Candidates have not converged (std: {std_center}); waiting for more evidence.")
return (None, False)
# Use the smallest candidate as the anchor.
min_width = min(widths)
min_height = min(heights)
if (median_width <= min_width * (1 + self.position_tolerance_ratio) and
median_height <= min_height * (1 + self.position_tolerance_ratio)):
refined_x = int(median_center[0] - median_width / 2.0)
refined_y = int(median_center[1] - median_height / 2.0)
refined_bbox = (refined_x, refined_y, int(median_width), int(median_height))
print(f"Detection confirmed at frame {frame_index}. Refined bbox: {refined_bbox}")
self.last_refined_bbox = refined_bbox
self.reset()
return (refined_bbox, True)
return (None, False)
# Mouse callback for drag-and-drop and double-click bounding box selection
def mouse_callback(event, x, y, flags, param):
with g.lock:
if event == cv2.EVENT_LBUTTONDOWN:
g.dragging = True
g.drag_start = (x, y)
g.drag_end = (x, y)
print("Left button down at:", (x, y))
elif event == cv2.EVENT_MOUSEMOVE:
if g.dragging and (flags & cv2.EVENT_FLAG_LBUTTON):
g.drag_end = (x, y)
# Update selection rectangle during dragging.
x1 = min(g.drag_start[0], g.drag_end[0])
y1 = min(g.drag_start[1], g.drag_end[1])
x2 = max(g.drag_start[0], g.drag_end[0])
y2 = max(g.drag_start[1], g.drag_end[1])
g.init_rect = (x1, y1, x2 - x1, y2 - y1)
else:
g.cursor_pos = (x, y)
g.last_mouse_move = time.time()
elif event == cv2.EVENT_LBUTTONUP:
if g.dragging:
g.dragging = False
g.drag_end = (x, y)
x1 = min(g.drag_start[0], g.drag_end[0])
y1 = min(g.drag_start[1], g.drag_end[1])
x2 = max(g.drag_start[0], g.drag_end[0])
y2 = max(g.drag_start[1], g.drag_end[1])
g.init_rect = (x1, y1, x2 - x1, y2 - y1)
g.click_point = ((x1 + x2) // 2, (y1 + y2) // 2)
g.message_display = "Object Locked via Drag and Drop - Object ready for tracking."
print("Drag selection finished:", g.init_rect)
elif event == cv2.EVENT_LBUTTONDBLCLK:
if g.frame is None:
return
try:
g.tracking_started = False
g.kalman_initialized = False
g.mode = "selection"
height, width = g.frame.shape[:2]
box_size = 100
x1 = max(0, x - box_size // 2)
y1 = max(0, y - box_size // 2)
x2 = min(width, x + box_size // 2)
y2 = min(height, y + box_size // 2)
g.init_rect = (x1, y1, x2 - x1, y2 - y1)
g.click_point = (x, y)
g.message_display = "Object Locked - Object ready for tracking."
print("Double-click selection:", g.init_rect)
except Exception as e:
logger.error("Error in mouse callback: %s", e)
g.init_rect = None
g.click_point = None
def optical_flow_update(prev_frame, current_frame, bbox):
x, y, w, h = map(int, bbox)
if x < 0 or y < 0 or w <= 0 or h <= 0 or x + w > prev_frame.shape[1] or y + h > prev_frame.shape[0]:
return (0.0, 0.0)
try:
prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY) if len(prev_frame.shape) == 3 else prev_frame.copy()
curr_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY) if len(current_frame.shape) == 3 else current_frame.copy()
roi = prev_gray[y:y+h, x:x+w]
p0 = cv2.goodFeaturesToTrack(
roi,
maxCorners=15,
qualityLevel=0.01,
minDistance=7,
blockSize=7,
useHarrisDetector=True,
k=0.04
)
if p0 is None:
return (0.0, 0.0)
p0 = np.array(p0, dtype=np.float32).reshape(-1, 1, 2)
p0[:, 0, 0] += x
p0[:, 0, 1] += y
p1, st, err = cv2.calcOpticalFlowPyrLK(
prev_gray,
curr_gray,
p0,
None,
winSize=(21, 21),
maxLevel=3,
criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
)
if p1 is None:
return (0.0, 0.0)
good_new = p1[st.flatten() == 1]
good_old = p0[st.flatten() == 1]
if len(good_new) == 0 or len(good_old) == 0:
return (0.0, 0.0)
good_new = good_new.reshape(-1, 2)
good_old = good_old.reshape(-1, 2)
displacement = np.mean(good_new - good_old, axis=0)
if displacement.size != 2:
return (0.0, 0.0)
dx = float(displacement[0])
dy = float(displacement[1])
if np.sqrt(dx*dx + dy*dy) > 20:
logger.warning("Optical flow shift too high, skipping update")
return (0.0, 0.0)
return (dx, dy)
except Exception as e:
logger.error(f"Error in optical flow calculation: {e}")
return (0.0, 0.0)
def compute_byt_detection(frame, center_point, yolo_model, max_distance=300):
results = yolo_model(frame)
selected_det = None
min_area = float('inf')
if center_point is None:
return None
click_x, click_y = center_point
if results and len(results[0].boxes) > 0:
logger.info(f"Number of detections: {len(results[0].boxes)}")
for box in results[0].boxes:
conf = box.conf.item() if hasattr(box, 'conf') else 0.0
if conf < 0.25:
continue
bbox_arr = box.xyxy[0].cpu().numpy().astype(int)
x1, y1, x2, y2 = bbox_arr
w, h = x2 - x1, y2 - y1
area = w * h
if (x1 <= click_x <= x2) and (y1 <= click_y <= y2):
aspect_ratio = w / h if h > 0 else 0
if 0.2 <= aspect_ratio <= 5.0:
if area < min_area:
min_area = area
selected_det = (x1, y1, x2-x1, y2-y1)
return selected_det
def frame_reader(cap, frame_queue):
while cap.isOpened() and not g.stop_async_thread:
ret, frame = cap.read()
if not ret:
break
with g.lock:
g.frame_idx = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
try:
while not frame_queue.empty():
frame_queue.get_nowait()
except Exception:
pass
frame_queue.put(frame)
time.sleep(1.0 / g.new_fps)
cap.release()
def main():
parser = argparse.ArgumentParser(description='Enhanced Ship Tracking System')
parser.add_argument('--config', type=str, required=True, help='Config file for SiamRPN')
parser.add_argument('--snapshot', type=str, required=True, help='Model snapshot for SiamRPN')
parser.add_argument('--video_name', type=str, required=True, help='Video file path')
parser.add_argument('--yolo_model', type=str, required=True, help='YOLO model path')
parser.add_argument('--use_bytetrack', action='store_true', help='Use ByteTrack for multi-object tracking')
args = parser.parse_args()
g.use_bytetrack = args.use_bytetrack
cfg.merge_from_file(args.config)
cfg.CUDA = torch.cuda.is_available()
device = torch.device('cuda' if cfg.CUDA else 'cpu')
try:
model = ModelBuilder()
model.load_state_dict(torch.load(args.snapshot, map_location=lambda storage, loc: storage.cpu()))
model.eval().to(device)
tracker = build_tracker(model)
yolo = YOLO(args.yolo_model)
yolo.conf = 0.25
yolo.iou = 0.45
if cfg.CUDA:
yolo.model.to(device)
logger.info("Models initialized successfully")
except Exception as e:
logger.error("Error initializing models: %s", e)
return
cap = cv2.VideoCapture(args.video_name)
if not cap.isOpened():
logger.error("Error: Could not open video")
return
g.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
orig_fps = cap.get(cv2.CAP_PROP_FPS) or 25
g.new_fps = 2 * orig_fps
cv2.namedWindow("Tracking")
cv2.setMouseCallback("Tracking", mouse_callback)
logger.info("\nControls:")
logger.info("Drag and Drop or Double Click: Select tracking object")
logger.info("'c' key: Confirm selection as new object (new selection replaces previous one)")
logger.info("Space: Pause/Resume")
logger.info("'r' key: Reset tracking")
logger.info("q: Quit")
g.frame_queue = queue.Queue(maxsize=1)
prev_time = time.time()
while True:
current_time = time.time()
fps = 1 / (current_time - prev_time) if (current_time - prev_time) > 0 else 0
prev_time = current_time
with g.lock:
current_mode = g.mode
paused = g.paused
if current_mode in ("selection", "selection_during_tracking"):
delay_ms = 20
ret, frame = cap.read()
if not ret:
logger.info("End of video reached.")
break
with g.lock:
g.frame_idx += 1
g.frame = frame.copy()
progress = (g.frame_idx / g.total_frames) * 100 if g.total_frames > 0 else 0
display_frame = frame.copy()
cv2.putText(display_frame, f"Frame: {g.frame_idx}/{g.total_frames} ({progress:.2f}%)",
(10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
cv2.putText(display_frame, f"FPS: {fps:.2f}", (10, 60),
cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
if g.init_rect is not None:
x, y, w, h = g.init_rect
cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
if g.click_point:
cv2.circle(display_frame, g.click_point, 5, (0, 255, 0), -1)
if g.dragging and g.drag_start and g.drag_end:
x1 = min(g.drag_start[0], g.drag_end[0])
y1 = min(g.drag_start[1], g.drag_end[1])
x2 = max(g.drag_start[0], g.drag_end[0])
y2 = max(g.drag_start[1], g.drag_end[1])
cv2.rectangle(display_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)
if g.message_display:
cv2.putText(display_frame, g.message_display, (10, 90),
cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
cv2.imshow("Tracking", display_frame)
key = cv2.waitKey(delay_ms)
if key & 0xFF == ord('q'):
break
elif key & 0xFF == ord(' '):
with g.lock:
g.paused = not g.paused
elif key & 0xFF == ord('r'):
with g.lock:
if g.use_bytetrack:
g.tracked_objects = []
else:
g.single_trackers = []
g.tracking_started = False
g.init_rect = None
g.click_point = None
g.kalman_initialized = False
g.message_display = ""
elif key & 0xFF == ord('c'):
with g.lock:
if g.use_bytetrack:
g.tracked_objects = []
if g.init_rect is not None:
box = g.init_rect
elif g.click_point is not None:
x_center, y_center = g.click_point
box_size = 100
frame_h, frame_w = frame.shape[:2]
x1 = max(0, x_center - box_size // 2)
y1 = max(0, y_center - box_size // 2)
x2 = min(frame_w, x_center + box_size // 2)
y2 = min(frame_h, y_center + box_size // 2)
box = (x1, y1, x2 - x1, y2 - y1)
else:
box = None
if box is not None:
new_obj = {}
new_obj['init_rect'] = box
x, y, w, h = box
new_obj['click_point'] = (x + w // 2, y + h // 2)
new_obj['kalman_filter'] = EnhancedKalmanBox(box)
new_obj['kalman_box'] = box
new_obj['confidence_tracker'] = ConfidenceTracker()
new_obj['color'] = (random.randint(0, 255),
random.randint(0, 255),
random.randint(0, 255))
new_obj['prev_center'] = (x + w // 2, y + h // 2)
new_obj['accumulator'] = TemporalEvidenceAccumulator()
g.tracked_objects.append(new_obj)
g.mode = "tracking"
g.init_rect = None
g.click_point = None
g.message_display = "Object tracking."
else:
logger.warning("No selection made. Please select an object before pressing 'c'.")
else:
g.single_trackers = []
if g.init_rect is not None:
tracker.init(frame, g.init_rect)
new_tracker = {}
new_tracker['tracker'] = tracker
new_tracker['kalman_box'] = g.init_rect
new_tracker['confidence_tracker'] = ConfidenceTracker()
new_tracker['kalman_initialized'] = False
new_tracker['accumulator'] = TemporalEvidenceAccumulator()
g.single_trackers.append(new_tracker)
g.tracking_started = True
g.init_rect = None
g.message_display = "Object tracking."
else:
logger.warning("No selection made. Please select an object before pressing 'c'.")
elif current_mode == "tracking":
if paused:
cv2.waitKey(1)
continue
if g.use_bytetrack:
ret, frame = cap.read()
if not ret:
break
with g.lock:
g.frame_idx += 1
g.frame = frame.copy()
display_frame = frame.copy()
for obj in list(g.tracked_objects):
prev_box = obj['kalman_box']
prev_center = obj.get('prev_center', (prev_box[0] + prev_box[2] // 2,
prev_box[1] + prev_box[3] // 2))
dx, dy = optical_flow_update(g.frame, frame, prev_box)
x, y, w, h = prev_box
new_box = (int(x+dx), int(y+dy), w, h)
new_center = (new_box[0] + new_box[2] // 2, new_box[1] + new_box[3] // 2)
jump_distance = math.hypot(new_center[0] - prev_center[0],
new_center[1] - prev_center[1])
if jump_distance > JUMP_THRESHOLD:
cv2.putText(display_frame, "Tracked Object lost due to jump.", (50, 50),
cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
logger.info("Jump distance exceeded threshold; removing tracking object")
g.tracked_objects.remove(obj)
continue
obj['kalman_box'] = new_box
detection = compute_byt_detection(frame, new_center, yolo)
if detection is not None:
x1, y1, w1, h1 = new_box
x2, y2, w2, h2 = detection
fused_box = (
int(0.6 * x1 + 0.4 * x2),
int(0.6 * y1 + 0.4 * y2),
int(0.6 * w1 + 0.4 * w2),
int(0.6 * h1 + 0.4 * h2)
)
fused_center = (fused_box[0] + fused_box[2] // 2,
fused_box[1] + fused_box[3] // 2)
jump_distance_fused = math.hypot(fused_center[0] - prev_center[0],
fused_center[1] - prev_center[1])
if jump_distance_fused > JUMP_THRESHOLD:
cv2.putText(display_frame, "Tracked Object lost due to fused jump.", (50, 50),
cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
logger.info("Fused jump distance exceeded threshold; removing tracking object")
g.tracked_objects.remove(obj)
continue
else:
obj['kalman_box'] = fused_box
new_center = fused_center
obj['prev_center'] = new_center
conf, lost = obj['confidence_tracker'].update(frame, obj['kalman_box'], 0.8)
if lost:
logger.info("Lost track of one object, removing from tracking list")
g.tracked_objects.remove(obj)
continue
x, y, w, h = obj['kalman_box']
cv2.rectangle(display_frame, (x, y), (x+w, y+h), obj['color'], 2)
cv2.putText(display_frame, f"FPS: {fps:.2f}", (10, 60),
cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
cv2.imshow("Tracking", display_frame)
key = cv2.waitKey(1)
if key & 0xFF == ord('q'):
break
elif key & 0xFF == ord('r'):
with g.lock:
g.tracked_objects = []
else:
try:
frame = g.frame_queue.get(timeout=0.01)
except queue.Empty:
with g.lock:
frame = g.frame
if frame is None:
continue
display_frame = frame.copy()
for tracker_instance in list(g.single_trackers):
outputs = tracker_instance['tracker'].track(frame)
bbox = outputs['bbox']
accumulator = tracker_instance['accumulator']
refined_bbox, confirmed = accumulator.update(bbox, g.frame_idx)
if confirmed:
bbox = refined_bbox
if not tracker_instance['kalman_initialized']:
tracker_instance['kalman_box'] = bbox
tracker_instance['kalman_initialized'] = True
else:
dx, dy = optical_flow_update(g.frame, frame, tracker_instance['kalman_box'])
x, y, w, h = tracker_instance['kalman_box']
tracker_instance['kalman_box'] = (int(x + dx), int(y + dy), w, h)
conf, lost = tracker_instance['confidence_tracker'].update(frame,
tracker_instance['kalman_box'],
outputs.get('score', 0.8))
if lost:
logger.info("Tracking lost on one instance, removing from tracking list")
g.single_trackers.remove(tracker_instance)
continue
x, y, w, h = tracker_instance['kalman_box']
cv2.rectangle(display_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
cv2.putText(display_frame, f"FPS: {fps:.2f}", (10, 60),
cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
cv2.imshow("Tracking", display_frame)
key = cv2.waitKey(1)
if key & 0xFF == ord('q'):
break
elif key & 0xFF == ord('r'):
with g.lock:
g.single_trackers = []
g.tracking_started = False
g.mode = "selection"
g.kalman_initialized = False
cap.release()
cv2.destroyAllWindows()
if __name__ == "__main__":
main()